Iteration 1, loss = 1.62458172
Iteration 2, loss = 1.52095880
Iteration 3, loss = 1.45062006
Iteration 4, loss = 1.39407654
Iteration 5, loss = 1.34394211
Iteration 6, loss = 1.29699519
Iteration 7, loss = 1.24987621
Iteration 8, loss = 1.20238042
Iteration 9, loss = 1.15532577
Iteration 10, loss = 1.10986445
Iteration 11, loss = 1.06619007
Iteration 12, loss = 1.02462810
Iteration 13, loss = 0.98482862
Iteration 14, loss = 0.94725362
Iteration 15, loss = 0.91119250
Iteration 16, loss = 0.87694710
Iteration 17, loss = 0.84427056
Iteration 18, loss = 0.81311065
Iteration 19, loss = 0.78366132
Iteration 20, loss = 0.75570853
Iteration 21, loss = 0.72931451
Iteration 22, loss = 0.70433294
Iteration 23, loss = 0.68070534
Iteration 24, loss = 0.65825883
Iteration 25, loss = 0.63708896
Iteration 26, loss = 0.61687626
Iteration 27, loss = 0.59774161
Iteration 28, loss = 0.57949803
Iteration 29, loss = 0.56242793
Iteration 30, loss = 0.54611157
Iteration 31, loss = 0.53030822
Iteration 32, loss = 0.51539704
Iteration 33, loss = 0.50114589
Iteration 34, loss = 0.48764358
Iteration 35, loss = 0.47466090
Iteration 36, loss = 0.46228065
Iteration 37, loss = 0.45046269
Iteration 38, loss = 0.43925229
Iteration 39, loss = 0.42824585
Iteration 40, loss = 0.41789345
Iteration 41, loss = 0.40797111
Iteration 42, loss = 0.39838232
Iteration 43, loss = 0.38919653
Iteration 44, loss = 0.38030974
Iteration 45, loss = 0.37189368
Iteration 46, loss = 0.36352114
Iteration 47, loss = 0.35573537
Iteration 48, loss = 0.34801860
Iteration 49, loss = 0.34081207
Iteration 50, loss = 0.33365018
Iteration 51, loss = 0.32698236
Iteration 52, loss = 0.32035191
Iteration 53, loss = 0.31396389
Iteration 54, loss = 0.30780354
Iteration 55, loss = 0.30192350
Iteration 56, loss = 0.29606381
Iteration 57, loss = 0.29043661
Iteration 58, loss = 0.28501286
Iteration 59, loss = 0.27979129
Iteration 60, loss = 0.27472178
Iteration 61, loss = 0.26985960
Iteration 62, loss = 0.26491967
Iteration 63, loss = 0.26028614
Iteration 64, loss = 0.25575353
Iteration 65, loss = 0.25141785
Iteration 66, loss = 0.24708438
Iteration 67, loss = 0.24300241
Iteration 68, loss = 0.23896258
Iteration 69, loss = 0.23517044
Iteration 70, loss = 0.23116416
Iteration 71, loss = 0.22743136
Iteration 72, loss = 0.22380366
Iteration 73, loss = 0.22028564
Iteration 74, loss = 0.21680662
Iteration 75, loss = 0.21366969
Iteration 76, loss = 0.21023661
Iteration 77, loss = 0.20710097
Iteration 78, loss = 0.20393475
Iteration 79, loss = 0.20093620
Iteration 80, loss = 0.19795254
Iteration 81, loss = 0.19504251
Iteration 82, loss = 0.19226259
Iteration 83, loss = 0.18939448
Iteration 84, loss = 0.18668035
Iteration 85, loss = 0.18422661
Iteration 86, loss = 0.18174321
Iteration 87, loss = 0.17901562
Iteration 88, loss = 0.17647492
Iteration 89, loss = 0.17409847
Iteration 90, loss = 0.17162458
Iteration 91, loss = 0.16930176
Iteration 92, loss = 0.16710082
Iteration 93, loss = 0.16483946
Iteration 94, loss = 0.16267974
Iteration 95, loss = 0.16058977
Iteration 96, loss = 0.15845155
Iteration 97, loss = 0.15623707
Iteration 98, loss = 0.15423831
Iteration 99, loss = 0.15223611
Iteration 100, loss = 0.15030846
Iteration 101, loss = 0.14841047
Iteration 102, loss = 0.14653907
Iteration 103, loss = 0.14488404
Iteration 104, loss = 0.14273743
Iteration 105, loss = 0.14103224
Iteration 106, loss = 0.13927314
Iteration 107, loss = 0.13767159
Iteration 108, loss = 0.13582412
Iteration 109, loss = 0.13417360
Iteration 110, loss = 0.13238292
Iteration 111, loss = 0.13081228
Iteration 112, loss = 0.12922330
Iteration 113, loss = 0.12770579
Iteration 114, loss = 0.12609798
Iteration 115, loss = 0.12447698
Iteration 116, loss = 0.12300303
Iteration 117, loss = 0.12168687
Iteration 118, loss = 0.12026952
Iteration 119, loss = 0.11874582
Iteration 120, loss = 0.11721012
Iteration 121, loss = 0.11576680
Iteration 122, loss = 0.11450763
Iteration 123, loss = 0.11316648
Iteration 124, loss = 0.11161730
Iteration 125, loss = 0.11056935
Iteration 126, loss = 0.10912903
Iteration 127, loss = 0.10773975
Iteration 128, loss = 0.10643438
Iteration 129, loss = 0.10530761
Iteration 130, loss = 0.10419074
Iteration 131, loss = 0.10288925
Iteration 132, loss = 0.10164049
Iteration 133, loss = 0.10046115
Iteration 134, loss = 0.09937135
Iteration 135, loss = 0.09815794
Iteration 136, loss = 0.09704196
Iteration 137, loss = 0.09599282
Iteration 138, loss = 0.09485870
Iteration 139, loss = 0.09389535
Iteration 140, loss = 0.09279872
Iteration 141, loss = 0.09176533
Iteration 142, loss = 0.09079877
Iteration 143, loss = 0.08977046
Iteration 144, loss = 0.08857542
Iteration 145, loss = 0.08775379
Iteration 146, loss = 0.08678431
Iteration 147, loss = 0.08564182
Iteration 148, loss = 0.08476180
Iteration 149, loss = 0.08391264
Iteration 150, loss = 0.08290018
Iteration 151, loss = 0.08198185
Iteration 152, loss = 0.08101862
Iteration 153, loss = 0.08011222
Iteration 154, loss = 0.07929393
Iteration 155, loss = 0.07845016
Iteration 156, loss = 0.07756987
Iteration 157, loss = 0.07675916
Iteration 158, loss = 0.07591650
Iteration 159, loss = 0.07509505
Iteration 160, loss = 0.07448057
Iteration 161, loss = 0.07361154
Iteration 162, loss = 0.07279340
Iteration 163, loss = 0.07217670
Iteration 164, loss = 0.07142373
Iteration 165, loss = 0.07060359
Iteration 166, loss = 0.06987921
Iteration 167, loss = 0.06923039
Iteration 168, loss = 0.06844412
Iteration 169, loss = 0.06775559
Iteration 170, loss = 0.06704240
Iteration 171, loss = 0.06653944
Iteration 172, loss = 0.06585220
Iteration 173, loss = 0.06515331
Iteration 174, loss = 0.06439701
Iteration 175, loss = 0.06399095
Iteration 176, loss = 0.06324277
Iteration 177, loss = 0.06264963
Iteration 178, loss = 0.06207205
Iteration 179, loss = 0.06155771
Iteration 180, loss = 0.06081052
Iteration 181, loss = 0.06027467
Iteration 182, loss = 0.05954159
Iteration 183, loss = 0.05904400
Iteration 184, loss = 0.05843481
Iteration 185, loss = 0.05780559
Iteration 186, loss = 0.05724855
Iteration 187, loss = 0.05673739
Iteration 188, loss = 0.05616073
Iteration 189, loss = 0.05565513
Iteration 190, loss = 0.05516708
Iteration 191, loss = 0.05456112
Iteration 192, loss = 0.05405933
Iteration 193, loss = 0.05365057
Iteration 194, loss = 0.05316111
Iteration 195, loss = 0.05268426
Iteration 196, loss = 0.05220857
Iteration 197, loss = 0.05181248
Iteration 198, loss = 0.05134770
Iteration 199, loss = 0.05091451
Iteration 200, loss = 0.05053585
Number of features extracted:   5
Error rate: 12.88% ( 322/2500)
 - Class 1:  48, Class 2:  21, Class 3: 126, Class 4:  63, Class 5:  64
Iteration 1, loss = 1.48431454
Iteration 2, loss = 1.28346996
Iteration 3, loss = 1.14646641
Iteration 4, loss = 1.03815162
Iteration 5, loss = 0.94443987
Iteration 6, loss = 0.86033741
Iteration 7, loss = 0.78526900
Iteration 8, loss = 0.71834259
Iteration 9, loss = 0.65955685
Iteration 10, loss = 0.60737062
Iteration 11, loss = 0.56143100
Iteration 12, loss = 0.52097213
Iteration 13, loss = 0.48550159
Iteration 14, loss = 0.45338399
Iteration 15, loss = 0.42500419
Iteration 16, loss = 0.39967622
Iteration 17, loss = 0.37692716
Iteration 18, loss = 0.35624134
Iteration 19, loss = 0.33741506
Iteration 20, loss = 0.32076035
Iteration 21, loss = 0.30528112
Iteration 22, loss = 0.29142768
Iteration 23, loss = 0.27867643
Iteration 24, loss = 0.26698890
Iteration 25, loss = 0.25624549
Iteration 26, loss = 0.24629977
Iteration 27, loss = 0.23738035
Iteration 28, loss = 0.22867767
Iteration 29, loss = 0.22081977
Iteration 30, loss = 0.21352050
Iteration 31, loss = 0.20687240
Iteration 32, loss = 0.20046265
Iteration 33, loss = 0.19438472
Iteration 34, loss = 0.18872774
Iteration 35, loss = 0.18341495
Iteration 36, loss = 0.17840660
Iteration 37, loss = 0.17372296
Iteration 38, loss = 0.16939185
Iteration 39, loss = 0.16517364
Iteration 40, loss = 0.16108614
Iteration 41, loss = 0.15725673
Iteration 42, loss = 0.15373155
Iteration 43, loss = 0.15005020
Iteration 44, loss = 0.14669250
Iteration 45, loss = 0.14364444
Iteration 46, loss = 0.14049314
Iteration 47, loss = 0.13752849
Iteration 48, loss = 0.13475650
Iteration 49, loss = 0.13196314
Iteration 50, loss = 0.12936586
Iteration 51, loss = 0.12675337
Iteration 52, loss = 0.12456495
Iteration 53, loss = 0.12196460
Iteration 54, loss = 0.11992689
Iteration 55, loss = 0.11748091
Iteration 56, loss = 0.11537310
Iteration 57, loss = 0.11330395
Iteration 58, loss = 0.11121517
Iteration 59, loss = 0.10957910
Iteration 60, loss = 0.10751278
Iteration 61, loss = 0.10552153
Iteration 62, loss = 0.10394936
Iteration 63, loss = 0.10202065
Iteration 64, loss = 0.10025823
Iteration 65, loss = 0.09866489
Iteration 66, loss = 0.09723177
Iteration 67, loss = 0.09553891
Iteration 68, loss = 0.09423404
Iteration 69, loss = 0.09274452
Iteration 70, loss = 0.09099157
Iteration 71, loss = 0.08952672
Iteration 72, loss = 0.08819664
Iteration 73, loss = 0.08675017
Iteration 74, loss = 0.08537427
Iteration 75, loss = 0.08412183
Iteration 76, loss = 0.08306764
Iteration 77, loss = 0.08161302
Iteration 78, loss = 0.08042963
Iteration 79, loss = 0.07930933
Iteration 80, loss = 0.07806807
Iteration 81, loss = 0.07696465
Iteration 82, loss = 0.07591335
Iteration 83, loss = 0.07469822
Iteration 84, loss = 0.07370180
Iteration 85, loss = 0.07270368
Iteration 86, loss = 0.07157156
Iteration 87, loss = 0.07053224
Iteration 88, loss = 0.06952030
Iteration 89, loss = 0.06864741
Iteration 90, loss = 0.06773657
Iteration 91, loss = 0.06671255
Iteration 92, loss = 0.06589082
Iteration 93, loss = 0.06495421
Iteration 94, loss = 0.06417271
Iteration 95, loss = 0.06311062
Iteration 96, loss = 0.06236029
Iteration 97, loss = 0.06142565
Iteration 98, loss = 0.06062641
Iteration 99, loss = 0.05979727
Iteration 100, loss = 0.05910035
Iteration 101, loss = 0.05824578
Iteration 102, loss = 0.05761324
Iteration 103, loss = 0.05692241
Iteration 104, loss = 0.05620980
Iteration 105, loss = 0.05537700
Iteration 106, loss = 0.05471550
Iteration 107, loss = 0.05393247
Iteration 108, loss = 0.05340668
Iteration 109, loss = 0.05272062
Iteration 110, loss = 0.05203787
Iteration 111, loss = 0.05140563
Iteration 112, loss = 0.05084139
Iteration 113, loss = 0.05018368
Iteration 114, loss = 0.04958362
Iteration 115, loss = 0.04892743
Iteration 116, loss = 0.04841575
Iteration 117, loss = 0.04786580
Iteration 118, loss = 0.04733897
Iteration 119, loss = 0.04678120
Iteration 120, loss = 0.04619871
Iteration 121, loss = 0.04564493
Iteration 122, loss = 0.04513286
Iteration 123, loss = 0.04470412
Iteration 124, loss = 0.04407084
Iteration 125, loss = 0.04366073
Iteration 126, loss = 0.04308921
Iteration 127, loss = 0.04261139
Iteration 128, loss = 0.04211691
Iteration 129, loss = 0.04166961
Iteration 130, loss = 0.04140594
Iteration 131, loss = 0.04071686
Iteration 132, loss = 0.04024233
Iteration 133, loss = 0.03990903
Iteration 134, loss = 0.03945962
Iteration 135, loss = 0.03908550
Iteration 136, loss = 0.03863045
Iteration 137, loss = 0.03835742
Iteration 138, loss = 0.03768036
Iteration 139, loss = 0.03732743
Iteration 140, loss = 0.03694037
Iteration 141, loss = 0.03646387
Iteration 142, loss = 0.03604481
Iteration 143, loss = 0.03567222
Iteration 144, loss = 0.03526733
Iteration 145, loss = 0.03494821
Iteration 146, loss = 0.03450375
Iteration 147, loss = 0.03411906
Iteration 148, loss = 0.03371391
Iteration 149, loss = 0.03334985
Iteration 150, loss = 0.03300855
Iteration 151, loss = 0.03264968
Iteration 152, loss = 0.03222484
Iteration 153, loss = 0.03191108
Iteration 154, loss = 0.03160970
Iteration 155, loss = 0.03127832
Iteration 156, loss = 0.03088257
Iteration 157, loss = 0.03062156
Iteration 158, loss = 0.03038414
Iteration 159, loss = 0.02995318
Iteration 160, loss = 0.02959044
Iteration 161, loss = 0.02930241
Iteration 162, loss = 0.02910133
Iteration 163, loss = 0.02876292
Iteration 164, loss = 0.02845747
Iteration 165, loss = 0.02808490
Iteration 166, loss = 0.02779909
Iteration 167, loss = 0.02747337
Iteration 168, loss = 0.02726211
Iteration 169, loss = 0.02684490
Iteration 170, loss = 0.02655494
Iteration 171, loss = 0.02637568
Iteration 172, loss = 0.02605713
Iteration 173, loss = 0.02579358
Iteration 174, loss = 0.02554440
Iteration 175, loss = 0.02532676
Iteration 176, loss = 0.02503935
Iteration 177, loss = 0.02481962
Iteration 178, loss = 0.02464693
Iteration 179, loss = 0.02432206
Iteration 180, loss = 0.02403467
Iteration 181, loss = 0.02384937
Iteration 182, loss = 0.02360979
Iteration 183, loss = 0.02337693
Iteration 184, loss = 0.02314771
Iteration 185, loss = 0.02295561
Iteration 186, loss = 0.02273489
Iteration 187, loss = 0.02250786
Iteration 188, loss = 0.02237827
Iteration 189, loss = 0.02205417
Iteration 190, loss = 0.02180773
Iteration 191, loss = 0.02164493
Iteration 192, loss = 0.02151124
Iteration 193, loss = 0.02127914
Iteration 194, loss = 0.02101858
Iteration 195, loss = 0.02079831
Iteration 196, loss = 0.02064407
Iteration 197, loss = 0.02044294
Iteration 198, loss = 0.02024453
Iteration 199, loss = 0.02003902
Iteration 200, loss = 0.01989762
Number of features extracted:  10
Error rate: 23.56% ( 589/2500)
 - Class 1:  22, Class 2:  16, Class 3: 498, Class 4:  36, Class 5:  17
Iteration 1, loss = 1.49887405
Iteration 2, loss = 1.24177542
Iteration 3, loss = 1.06365115
Iteration 4, loss = 0.92557507
Iteration 5, loss = 0.80994418
Iteration 6, loss = 0.71100368
Iteration 7, loss = 0.62717732
Iteration 8, loss = 0.55713650
Iteration 9, loss = 0.49895957
Iteration 10, loss = 0.45008231
Iteration 11, loss = 0.40941661
Iteration 12, loss = 0.37465463
Iteration 13, loss = 0.34507073
Iteration 14, loss = 0.31973585
Iteration 15, loss = 0.29800015
Iteration 16, loss = 0.27893240
Iteration 17, loss = 0.26216206
Iteration 18, loss = 0.24731889
Iteration 19, loss = 0.23412432
Iteration 20, loss = 0.22223596
Iteration 21, loss = 0.21149687
Iteration 22, loss = 0.20198881
Iteration 23, loss = 0.19315786
Iteration 24, loss = 0.18522505
Iteration 25, loss = 0.17791315
Iteration 26, loss = 0.17112287
Iteration 27, loss = 0.16485285
Iteration 28, loss = 0.15901644
Iteration 29, loss = 0.15376351
Iteration 30, loss = 0.14863070
Iteration 31, loss = 0.14402341
Iteration 32, loss = 0.13968883
Iteration 33, loss = 0.13552854
Iteration 34, loss = 0.13168914
Iteration 35, loss = 0.12778578
Iteration 36, loss = 0.12431499
Iteration 37, loss = 0.12092076
Iteration 38, loss = 0.11783268
Iteration 39, loss = 0.11479536
Iteration 40, loss = 0.11165435
Iteration 41, loss = 0.10887343
Iteration 42, loss = 0.10623205
Iteration 43, loss = 0.10383107
Iteration 44, loss = 0.10139698
Iteration 45, loss = 0.09885122
Iteration 46, loss = 0.09676357
Iteration 47, loss = 0.09432972
Iteration 48, loss = 0.09217598
Iteration 49, loss = 0.09014410
Iteration 50, loss = 0.08822730
Iteration 51, loss = 0.08654492
Iteration 52, loss = 0.08459984
Iteration 53, loss = 0.08274216
Iteration 54, loss = 0.08113396
Iteration 55, loss = 0.07943690
Iteration 56, loss = 0.07779967
Iteration 57, loss = 0.07630131
Iteration 58, loss = 0.07472938
Iteration 59, loss = 0.07335836
Iteration 60, loss = 0.07222250
Iteration 61, loss = 0.07067858
Iteration 62, loss = 0.06937482
Iteration 63, loss = 0.06797742
Iteration 64, loss = 0.06677467
Iteration 65, loss = 0.06555194
Iteration 66, loss = 0.06438616
Iteration 67, loss = 0.06323809
Iteration 68, loss = 0.06205552
Iteration 69, loss = 0.06102107
Iteration 70, loss = 0.05988642
Iteration 71, loss = 0.05885080
Iteration 72, loss = 0.05797753
Iteration 73, loss = 0.05692596
Iteration 74, loss = 0.05593072
Iteration 75, loss = 0.05500023
Iteration 76, loss = 0.05414826
Iteration 77, loss = 0.05318468
Iteration 78, loss = 0.05221007
Iteration 79, loss = 0.05155817
Iteration 80, loss = 0.05057928
Iteration 81, loss = 0.04973951
Iteration 82, loss = 0.04897477
Iteration 83, loss = 0.04840611
Iteration 84, loss = 0.04745717
Iteration 85, loss = 0.04660335
Iteration 86, loss = 0.04590248
Iteration 87, loss = 0.04513632
Iteration 88, loss = 0.04443742
Iteration 89, loss = 0.04377642
Iteration 90, loss = 0.04306752
Iteration 91, loss = 0.04239134
Iteration 92, loss = 0.04166830
Iteration 93, loss = 0.04112812
Iteration 94, loss = 0.04044223
Iteration 95, loss = 0.03986115
Iteration 96, loss = 0.03912805
Iteration 97, loss = 0.03857140
Iteration 98, loss = 0.03800564
Iteration 99, loss = 0.03739953
Iteration 100, loss = 0.03679896
Iteration 101, loss = 0.03632944
Iteration 102, loss = 0.03569462
Iteration 103, loss = 0.03532079
Iteration 104, loss = 0.03475613
Iteration 105, loss = 0.03425469
Iteration 106, loss = 0.03364919
Iteration 107, loss = 0.03322821
Iteration 108, loss = 0.03262539
Iteration 109, loss = 0.03214856
Iteration 110, loss = 0.03162391
Iteration 111, loss = 0.03122269
Iteration 112, loss = 0.03076863
Iteration 113, loss = 0.03032044
Iteration 114, loss = 0.02990238
Iteration 115, loss = 0.02942055
Iteration 116, loss = 0.02897357
Iteration 117, loss = 0.02856456
Iteration 118, loss = 0.02812027
Iteration 119, loss = 0.02777114
Iteration 120, loss = 0.02735387
Iteration 121, loss = 0.02700995
Iteration 122, loss = 0.02661036
Iteration 123, loss = 0.02630448
Iteration 124, loss = 0.02591303
Iteration 125, loss = 0.02549110
Iteration 126, loss = 0.02508663
Iteration 127, loss = 0.02475072
Iteration 128, loss = 0.02444080
Iteration 129, loss = 0.02406000
Iteration 130, loss = 0.02373473
Iteration 131, loss = 0.02339432
Iteration 132, loss = 0.02311059
Iteration 133, loss = 0.02279852
Iteration 134, loss = 0.02250485
Iteration 135, loss = 0.02217894
Iteration 136, loss = 0.02186486
Iteration 137, loss = 0.02158792
Iteration 138, loss = 0.02135763
Iteration 139, loss = 0.02096947
Iteration 140, loss = 0.02071513
Iteration 141, loss = 0.02046980
Iteration 142, loss = 0.02023404
Iteration 143, loss = 0.02002225
Iteration 144, loss = 0.01977313
Iteration 145, loss = 0.01943659
Iteration 146, loss = 0.01925569
Iteration 147, loss = 0.01891518
Iteration 148, loss = 0.01864124
Iteration 149, loss = 0.01845361
Iteration 150, loss = 0.01822098
Iteration 151, loss = 0.01798108
Iteration 152, loss = 0.01780175
Iteration 153, loss = 0.01748313
Iteration 154, loss = 0.01728166
Iteration 155, loss = 0.01709413
Iteration 156, loss = 0.01681297
Iteration 157, loss = 0.01664685
Iteration 158, loss = 0.01643009
Iteration 159, loss = 0.01619966
Iteration 160, loss = 0.01597630
Iteration 161, loss = 0.01579808
Iteration 162, loss = 0.01562766
Iteration 163, loss = 0.01541148
Iteration 164, loss = 0.01521979
Iteration 165, loss = 0.01506722
Iteration 166, loss = 0.01484780
Iteration 167, loss = 0.01464014
Iteration 168, loss = 0.01453863
Iteration 169, loss = 0.01429025
Iteration 170, loss = 0.01416604
Iteration 171, loss = 0.01395711
Iteration 172, loss = 0.01379146
Iteration 173, loss = 0.01363344
Iteration 174, loss = 0.01347355
Iteration 175, loss = 0.01328171
Iteration 176, loss = 0.01315532
Iteration 177, loss = 0.01298140
Iteration 178, loss = 0.01287020
Iteration 179, loss = 0.01269645
Iteration 180, loss = 0.01252039
Iteration 181, loss = 0.01236461
Iteration 182, loss = 0.01226151
Iteration 183, loss = 0.01211033
Iteration 184, loss = 0.01196338
Iteration 185, loss = 0.01185713
Iteration 186, loss = 0.01170476
Iteration 187, loss = 0.01157477
Iteration 188, loss = 0.01146787
Iteration 189, loss = 0.01131400
Iteration 190, loss = 0.01121344
Iteration 191, loss = 0.01109993
Iteration 192, loss = 0.01097131
Iteration 193, loss = 0.01087585
Iteration 194, loss = 0.01075033
Iteration 195, loss = 0.01064980
Iteration 196, loss = 0.01059330
Iteration 197, loss = 0.01042960
Iteration 198, loss = 0.01032116
Iteration 199, loss = 0.01021326
Iteration 200, loss = 0.01009040
Number of features extracted:  15
Error rate: 11.16% ( 279/2500)
 - Class 1:  23, Class 2:  23, Class 3:  93, Class 4:  79, Class 5:  61
Iteration 1, loss = 1.47971263
Iteration 2, loss = 1.18485296
Iteration 3, loss = 0.98617683
Iteration 4, loss = 0.83392959
Iteration 5, loss = 0.71008962
Iteration 6, loss = 0.60895543
Iteration 7, loss = 0.52713029
Iteration 8, loss = 0.46146319
Iteration 9, loss = 0.40827681
Iteration 10, loss = 0.36551441
Iteration 11, loss = 0.33027641
Iteration 12, loss = 0.30086935
Iteration 13, loss = 0.27643437
Iteration 14, loss = 0.25567257
Iteration 15, loss = 0.23777435
Iteration 16, loss = 0.22230803
Iteration 17, loss = 0.20892593
Iteration 18, loss = 0.19714669
Iteration 19, loss = 0.18667458
Iteration 20, loss = 0.17742029
Iteration 21, loss = 0.16902359
Iteration 22, loss = 0.16125445
Iteration 23, loss = 0.15443310
Iteration 24, loss = 0.14816406
Iteration 25, loss = 0.14253691
Iteration 26, loss = 0.13710628
Iteration 27, loss = 0.13200789
Iteration 28, loss = 0.12755042
Iteration 29, loss = 0.12348219
Iteration 30, loss = 0.11952863
Iteration 31, loss = 0.11540691
Iteration 32, loss = 0.11209098
Iteration 33, loss = 0.10868789
Iteration 34, loss = 0.10555042
Iteration 35, loss = 0.10240013
Iteration 36, loss = 0.09971911
Iteration 37, loss = 0.09691643
Iteration 38, loss = 0.09430115
Iteration 39, loss = 0.09180707
Iteration 40, loss = 0.08948688
Iteration 41, loss = 0.08731255
Iteration 42, loss = 0.08526246
Iteration 43, loss = 0.08312539
Iteration 44, loss = 0.08116481
Iteration 45, loss = 0.07916801
Iteration 46, loss = 0.07727875
Iteration 47, loss = 0.07557271
Iteration 48, loss = 0.07381474
Iteration 49, loss = 0.07212649
Iteration 50, loss = 0.07051707
Iteration 51, loss = 0.06901395
Iteration 52, loss = 0.06745021
Iteration 53, loss = 0.06611570
Iteration 54, loss = 0.06470664
Iteration 55, loss = 0.06326263
Iteration 56, loss = 0.06184025
Iteration 57, loss = 0.06070141
Iteration 58, loss = 0.05962655
Iteration 59, loss = 0.05830291
Iteration 60, loss = 0.05705311
Iteration 61, loss = 0.05583113
Iteration 62, loss = 0.05483719
Iteration 63, loss = 0.05362383
Iteration 64, loss = 0.05260820
Iteration 65, loss = 0.05149068
Iteration 66, loss = 0.05078563
Iteration 67, loss = 0.04969376
Iteration 68, loss = 0.04900052
Iteration 69, loss = 0.04779874
Iteration 70, loss = 0.04688524
Iteration 71, loss = 0.04579554
Iteration 72, loss = 0.04507351
Iteration 73, loss = 0.04422043
Iteration 74, loss = 0.04344708
Iteration 75, loss = 0.04254894
Iteration 76, loss = 0.04166881
Iteration 77, loss = 0.04097892
Iteration 78, loss = 0.04037821
Iteration 79, loss = 0.03947813
Iteration 80, loss = 0.03888742
Iteration 81, loss = 0.03802550
Iteration 82, loss = 0.03745187
Iteration 83, loss = 0.03680265
Iteration 84, loss = 0.03615271
Iteration 85, loss = 0.03541108
Iteration 86, loss = 0.03473392
Iteration 87, loss = 0.03423069
Iteration 88, loss = 0.03354750
Iteration 89, loss = 0.03297991
Iteration 90, loss = 0.03239108
Iteration 91, loss = 0.03179638
Iteration 92, loss = 0.03128516
Iteration 93, loss = 0.03077403
Iteration 94, loss = 0.03027903
Iteration 95, loss = 0.02971861
Iteration 96, loss = 0.02925211
Iteration 97, loss = 0.02866208
Iteration 98, loss = 0.02823579
Iteration 99, loss = 0.02777115
Iteration 100, loss = 0.02729922
Iteration 101, loss = 0.02681544
Iteration 102, loss = 0.02646335
Iteration 103, loss = 0.02596042
Iteration 104, loss = 0.02550953
Iteration 105, loss = 0.02513887
Iteration 106, loss = 0.02477851
Iteration 107, loss = 0.02425045
Iteration 108, loss = 0.02405995
Iteration 109, loss = 0.02356341
Iteration 110, loss = 0.02315650
Iteration 111, loss = 0.02275533
Iteration 112, loss = 0.02242271
Iteration 113, loss = 0.02200372
Iteration 114, loss = 0.02177048
Iteration 115, loss = 0.02132808
Iteration 116, loss = 0.02095955
Iteration 117, loss = 0.02069280
Iteration 118, loss = 0.02031890
Iteration 119, loss = 0.02008460
Iteration 120, loss = 0.01967066
Iteration 121, loss = 0.01947255
Iteration 122, loss = 0.01906643
Iteration 123, loss = 0.01880443
Iteration 124, loss = 0.01855233
Iteration 125, loss = 0.01825606
Iteration 126, loss = 0.01795756
Iteration 127, loss = 0.01773092
Iteration 128, loss = 0.01744536
Iteration 129, loss = 0.01715619
Iteration 130, loss = 0.01690645
Iteration 131, loss = 0.01666214
Iteration 132, loss = 0.01647703
Iteration 133, loss = 0.01632712
Iteration 134, loss = 0.01593666
Iteration 135, loss = 0.01565787
Iteration 136, loss = 0.01546286
Iteration 137, loss = 0.01529440
Iteration 138, loss = 0.01502877
Iteration 139, loss = 0.01484320
Iteration 140, loss = 0.01466065
Iteration 141, loss = 0.01442166
Iteration 142, loss = 0.01424098
Iteration 143, loss = 0.01401547
Iteration 144, loss = 0.01381118
Iteration 145, loss = 0.01361822
Iteration 146, loss = 0.01344858
Iteration 147, loss = 0.01328756
Iteration 148, loss = 0.01307064
Iteration 149, loss = 0.01291383
Iteration 150, loss = 0.01276679
Iteration 151, loss = 0.01256971
Iteration 152, loss = 0.01242405
Iteration 153, loss = 0.01221064
Iteration 154, loss = 0.01216607
Iteration 155, loss = 0.01195886
Iteration 156, loss = 0.01176063
Iteration 157, loss = 0.01162422
Iteration 158, loss = 0.01144478
Iteration 159, loss = 0.01129179
Iteration 160, loss = 0.01114181
Iteration 161, loss = 0.01095347
Iteration 162, loss = 0.01082767
Iteration 163, loss = 0.01068560
Iteration 164, loss = 0.01055905
Iteration 165, loss = 0.01043308
Iteration 166, loss = 0.01025311
Iteration 167, loss = 0.01011375
Iteration 168, loss = 0.01000363
Iteration 169, loss = 0.00985576
Iteration 170, loss = 0.00973201
Iteration 171, loss = 0.00960149
Iteration 172, loss = 0.00945008
Iteration 173, loss = 0.00930915
Iteration 174, loss = 0.00916286
Iteration 175, loss = 0.00901178
Iteration 176, loss = 0.00879439
Iteration 177, loss = 0.00868049
Iteration 178, loss = 0.00853872
Iteration 179, loss = 0.00842953
Iteration 180, loss = 0.00826516
Iteration 181, loss = 0.00816118
Iteration 182, loss = 0.00806617
Iteration 183, loss = 0.00798346
Iteration 184, loss = 0.00782534
Iteration 185, loss = 0.00769989
Iteration 186, loss = 0.00759304
Iteration 187, loss = 0.00748738
Iteration 188, loss = 0.00736748
Iteration 189, loss = 0.00724446
Iteration 190, loss = 0.00713272
Iteration 191, loss = 0.00703674
Iteration 192, loss = 0.00693940
Iteration 193, loss = 0.00681711
Iteration 194, loss = 0.00669809
Iteration 195, loss = 0.00665278
Iteration 196, loss = 0.00650604
Iteration 197, loss = 0.00641193
Iteration 198, loss = 0.00631599
Iteration 199, loss = 0.00622939
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  20
Error rate: 24.00% ( 600/2500)
 - Class 1:  34, Class 2:  17, Class 3: 500, Class 4:  27, Class 5:  22
Iteration 1, loss = 1.44473675
Iteration 2, loss = 1.13503554
Iteration 3, loss = 0.91433635
Iteration 4, loss = 0.74485678
Iteration 5, loss = 0.61468123
Iteration 6, loss = 0.51455889
Iteration 7, loss = 0.43769931
Iteration 8, loss = 0.37880445
Iteration 9, loss = 0.33271856
Iteration 10, loss = 0.29630095
Iteration 11, loss = 0.26764924
Iteration 12, loss = 0.24408249
Iteration 13, loss = 0.22480427
Iteration 14, loss = 0.20828285
Iteration 15, loss = 0.19456249
Iteration 16, loss = 0.18290079
Iteration 17, loss = 0.17262667
Iteration 18, loss = 0.16357106
Iteration 19, loss = 0.15559824
Iteration 20, loss = 0.14860128
Iteration 21, loss = 0.14229316
Iteration 22, loss = 0.13647948
Iteration 23, loss = 0.13115800
Iteration 24, loss = 0.12632155
Iteration 25, loss = 0.12185304
Iteration 26, loss = 0.11756891
Iteration 27, loss = 0.11428397
Iteration 28, loss = 0.11021671
Iteration 29, loss = 0.10691690
Iteration 30, loss = 0.10356006
Iteration 31, loss = 0.10097981
Iteration 32, loss = 0.09803518
Iteration 33, loss = 0.09515420
Iteration 34, loss = 0.09259356
Iteration 35, loss = 0.08992946
Iteration 36, loss = 0.08760436
Iteration 37, loss = 0.08548217
Iteration 38, loss = 0.08336351
Iteration 39, loss = 0.08128223
Iteration 40, loss = 0.07920045
Iteration 41, loss = 0.07720846
Iteration 42, loss = 0.07538063
Iteration 43, loss = 0.07362786
Iteration 44, loss = 0.07192577
Iteration 45, loss = 0.07042053
Iteration 46, loss = 0.06862022
Iteration 47, loss = 0.06702799
Iteration 48, loss = 0.06561398
Iteration 49, loss = 0.06428132
Iteration 50, loss = 0.06300253
Iteration 51, loss = 0.06131884
Iteration 52, loss = 0.05997601
Iteration 53, loss = 0.05902443
Iteration 54, loss = 0.05753617
Iteration 55, loss = 0.05637422
Iteration 56, loss = 0.05518054
Iteration 57, loss = 0.05396917
Iteration 58, loss = 0.05293451
Iteration 59, loss = 0.05199885
Iteration 60, loss = 0.05078859
Iteration 61, loss = 0.04990728
Iteration 62, loss = 0.04885886
Iteration 63, loss = 0.04778431
Iteration 64, loss = 0.04680789
Iteration 65, loss = 0.04598314
Iteration 66, loss = 0.04500969
Iteration 67, loss = 0.04410735
Iteration 68, loss = 0.04313825
Iteration 69, loss = 0.04234019
Iteration 70, loss = 0.04154408
Iteration 71, loss = 0.04083209
Iteration 72, loss = 0.03984406
Iteration 73, loss = 0.03911360
Iteration 74, loss = 0.03853227
Iteration 75, loss = 0.03763184
Iteration 76, loss = 0.03693679
Iteration 77, loss = 0.03622983
Iteration 78, loss = 0.03549316
Iteration 79, loss = 0.03493874
Iteration 80, loss = 0.03428447
Iteration 81, loss = 0.03357722
Iteration 82, loss = 0.03302040
Iteration 83, loss = 0.03233794
Iteration 84, loss = 0.03171018
Iteration 85, loss = 0.03127073
Iteration 86, loss = 0.03053185
Iteration 87, loss = 0.03001212
Iteration 88, loss = 0.02944667
Iteration 89, loss = 0.02904386
Iteration 90, loss = 0.02841896
Iteration 91, loss = 0.02789968
Iteration 92, loss = 0.02728691
Iteration 93, loss = 0.02692590
Iteration 94, loss = 0.02640241
Iteration 95, loss = 0.02585248
Iteration 96, loss = 0.02533637
Iteration 97, loss = 0.02502811
Iteration 98, loss = 0.02454058
Iteration 99, loss = 0.02410982
Iteration 100, loss = 0.02363906
Iteration 101, loss = 0.02332118
Iteration 102, loss = 0.02298003
Iteration 103, loss = 0.02244567
Iteration 104, loss = 0.02199984
Iteration 105, loss = 0.02162956
Iteration 106, loss = 0.02131064
Iteration 107, loss = 0.02089338
Iteration 108, loss = 0.02050899
Iteration 109, loss = 0.02014275
Iteration 110, loss = 0.01985073
Iteration 111, loss = 0.01949891
Iteration 112, loss = 0.01914160
Iteration 113, loss = 0.01877366
Iteration 114, loss = 0.01847660
Iteration 115, loss = 0.01818219
Iteration 116, loss = 0.01788438
Iteration 117, loss = 0.01754258
Iteration 118, loss = 0.01724769
Iteration 119, loss = 0.01696595
Iteration 120, loss = 0.01663859
Iteration 121, loss = 0.01641095
Iteration 122, loss = 0.01611187
Iteration 123, loss = 0.01582120
Iteration 124, loss = 0.01564382
Iteration 125, loss = 0.01532932
Iteration 126, loss = 0.01507311
Iteration 127, loss = 0.01485924
Iteration 128, loss = 0.01462049
Iteration 129, loss = 0.01437664
Iteration 130, loss = 0.01421411
Iteration 131, loss = 0.01394485
Iteration 132, loss = 0.01368518
Iteration 133, loss = 0.01346400
Iteration 134, loss = 0.01325863
Iteration 135, loss = 0.01304069
Iteration 136, loss = 0.01288754
Iteration 137, loss = 0.01266045
Iteration 138, loss = 0.01245234
Iteration 139, loss = 0.01224203
Iteration 140, loss = 0.01205488
Iteration 141, loss = 0.01186373
Iteration 142, loss = 0.01172950
Iteration 143, loss = 0.01156359
Iteration 144, loss = 0.01136239
Iteration 145, loss = 0.01114217
Iteration 146, loss = 0.01098641
Iteration 147, loss = 0.01083419
Iteration 148, loss = 0.01070785
Iteration 149, loss = 0.01050971
Iteration 150, loss = 0.01036953
Iteration 151, loss = 0.01020649
Iteration 152, loss = 0.01005522
Iteration 153, loss = 0.00987385
Iteration 154, loss = 0.00975966
Iteration 155, loss = 0.00962120
Iteration 156, loss = 0.00944943
Iteration 157, loss = 0.00930441
Iteration 158, loss = 0.00914415
Iteration 159, loss = 0.00905747
Iteration 160, loss = 0.00893019
Iteration 161, loss = 0.00876118
Iteration 162, loss = 0.00862679
Iteration 163, loss = 0.00847822
Iteration 164, loss = 0.00837066
Iteration 165, loss = 0.00824234
Iteration 166, loss = 0.00812831
Iteration 167, loss = 0.00800720
Iteration 168, loss = 0.00788520
Iteration 169, loss = 0.00774361
Iteration 170, loss = 0.00764694
Iteration 171, loss = 0.00752930
Iteration 172, loss = 0.00743420
Iteration 173, loss = 0.00730384
Iteration 174, loss = 0.00721350
Iteration 175, loss = 0.00710350
Iteration 176, loss = 0.00700618
Iteration 177, loss = 0.00691031
Iteration 178, loss = 0.00680500
Iteration 179, loss = 0.00674607
Iteration 180, loss = 0.00659260
Iteration 181, loss = 0.00653722
Iteration 182, loss = 0.00641703
Iteration 183, loss = 0.00631325
Iteration 184, loss = 0.00624293
Iteration 185, loss = 0.00614720
Iteration 186, loss = 0.00605452
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  25
Error rate: 9.12% ( 228/2500)
 - Class 1:  24, Class 2:  15, Class 3: 104, Class 4:  54, Class 5:  31
Iteration 1, loss = 1.42351816
Iteration 2, loss = 1.07069490
Iteration 3, loss = 0.83581730
Iteration 4, loss = 0.66675183
Iteration 5, loss = 0.54214262
Iteration 6, loss = 0.44993129
Iteration 7, loss = 0.38074391
Iteration 8, loss = 0.32880993
Iteration 9, loss = 0.28945927
Iteration 10, loss = 0.25835825
Iteration 11, loss = 0.23412223
Iteration 12, loss = 0.21418414
Iteration 13, loss = 0.19769520
Iteration 14, loss = 0.18423804
Iteration 15, loss = 0.17236414
Iteration 16, loss = 0.16241442
Iteration 17, loss = 0.15356294
Iteration 18, loss = 0.14594370
Iteration 19, loss = 0.13923120
Iteration 20, loss = 0.13311946
Iteration 21, loss = 0.12739430
Iteration 22, loss = 0.12278614
Iteration 23, loss = 0.11789874
Iteration 24, loss = 0.11359064
Iteration 25, loss = 0.10971281
Iteration 26, loss = 0.10636738
Iteration 27, loss = 0.10262625
Iteration 28, loss = 0.09956889
Iteration 29, loss = 0.09636532
Iteration 30, loss = 0.09365652
Iteration 31, loss = 0.09105884
Iteration 32, loss = 0.08825895
Iteration 33, loss = 0.08583853
Iteration 34, loss = 0.08348689
Iteration 35, loss = 0.08125703
Iteration 36, loss = 0.07901535
Iteration 37, loss = 0.07697995
Iteration 38, loss = 0.07503784
Iteration 39, loss = 0.07309325
Iteration 40, loss = 0.07128043
Iteration 41, loss = 0.06962801
Iteration 42, loss = 0.06776704
Iteration 43, loss = 0.06642982
Iteration 44, loss = 0.06459119
Iteration 45, loss = 0.06301912
Iteration 46, loss = 0.06158420
Iteration 47, loss = 0.06018805
Iteration 48, loss = 0.05894257
Iteration 49, loss = 0.05754771
Iteration 50, loss = 0.05659355
Iteration 51, loss = 0.05519134
Iteration 52, loss = 0.05349343
Iteration 53, loss = 0.05260993
Iteration 54, loss = 0.05137833
Iteration 55, loss = 0.05011934
Iteration 56, loss = 0.04939995
Iteration 57, loss = 0.04800426
Iteration 58, loss = 0.04702895
Iteration 59, loss = 0.04612005
Iteration 60, loss = 0.04506595
Iteration 61, loss = 0.04423321
Iteration 62, loss = 0.04342077
Iteration 63, loss = 0.04263142
Iteration 64, loss = 0.04158484
Iteration 65, loss = 0.04062913
Iteration 66, loss = 0.03984489
Iteration 67, loss = 0.03916044
Iteration 68, loss = 0.03833015
Iteration 69, loss = 0.03761247
Iteration 70, loss = 0.03681499
Iteration 71, loss = 0.03625024
Iteration 72, loss = 0.03539930
Iteration 73, loss = 0.03470062
Iteration 74, loss = 0.03404054
Iteration 75, loss = 0.03344135
Iteration 76, loss = 0.03263235
Iteration 77, loss = 0.03212137
Iteration 78, loss = 0.03144264
Iteration 79, loss = 0.03091798
Iteration 80, loss = 0.03031192
Iteration 81, loss = 0.02972691
Iteration 82, loss = 0.02906839
Iteration 83, loss = 0.02858245
Iteration 84, loss = 0.02802735
Iteration 85, loss = 0.02755199
Iteration 86, loss = 0.02701219
Iteration 87, loss = 0.02655947
Iteration 88, loss = 0.02603411
Iteration 89, loss = 0.02551034
Iteration 90, loss = 0.02510888
Iteration 91, loss = 0.02459326
Iteration 92, loss = 0.02413320
Iteration 93, loss = 0.02366618
Iteration 94, loss = 0.02322375
Iteration 95, loss = 0.02279668
Iteration 96, loss = 0.02238345
Iteration 97, loss = 0.02203024
Iteration 98, loss = 0.02156707
Iteration 99, loss = 0.02130823
Iteration 100, loss = 0.02090060
Iteration 101, loss = 0.02038645
Iteration 102, loss = 0.02009486
Iteration 103, loss = 0.01974169
Iteration 104, loss = 0.01931504
Iteration 105, loss = 0.01896085
Iteration 106, loss = 0.01866877
Iteration 107, loss = 0.01832661
Iteration 108, loss = 0.01799451
Iteration 109, loss = 0.01761494
Iteration 110, loss = 0.01725460
Iteration 111, loss = 0.01688231
Iteration 112, loss = 0.01660270
Iteration 113, loss = 0.01627930
Iteration 114, loss = 0.01605232
Iteration 115, loss = 0.01570060
Iteration 116, loss = 0.01543319
Iteration 117, loss = 0.01516263
Iteration 118, loss = 0.01484068
Iteration 119, loss = 0.01464167
Iteration 120, loss = 0.01437428
Iteration 121, loss = 0.01406213
Iteration 122, loss = 0.01395901
Iteration 123, loss = 0.01363132
Iteration 124, loss = 0.01331710
Iteration 125, loss = 0.01312586
Iteration 126, loss = 0.01294102
Iteration 127, loss = 0.01265603
Iteration 128, loss = 0.01243668
Iteration 129, loss = 0.01218352
Iteration 130, loss = 0.01198073
Iteration 131, loss = 0.01182955
Iteration 132, loss = 0.01158142
Iteration 133, loss = 0.01142951
Iteration 134, loss = 0.01121121
Iteration 135, loss = 0.01104737
Iteration 136, loss = 0.01090623
Iteration 137, loss = 0.01066365
Iteration 138, loss = 0.01056285
Iteration 139, loss = 0.01043136
Iteration 140, loss = 0.01023418
Iteration 141, loss = 0.01000626
Iteration 142, loss = 0.00984682
Iteration 143, loss = 0.00966192
Iteration 144, loss = 0.00951199
Iteration 145, loss = 0.00935774
Iteration 146, loss = 0.00924603
Iteration 147, loss = 0.00908271
Iteration 148, loss = 0.00892848
Iteration 149, loss = 0.00878246
Iteration 150, loss = 0.00865941
Iteration 151, loss = 0.00855164
Iteration 152, loss = 0.00841180
Iteration 153, loss = 0.00825908
Iteration 154, loss = 0.00814684
Iteration 155, loss = 0.00800544
Iteration 156, loss = 0.00790661
Iteration 157, loss = 0.00778895
Iteration 158, loss = 0.00765974
Iteration 159, loss = 0.00753483
Iteration 160, loss = 0.00743985
Iteration 161, loss = 0.00728254
Iteration 162, loss = 0.00719075
Iteration 163, loss = 0.00706388
Iteration 164, loss = 0.00693381
Iteration 165, loss = 0.00685692
Iteration 166, loss = 0.00674710
Iteration 167, loss = 0.00665469
Iteration 168, loss = 0.00651848
Iteration 169, loss = 0.00640737
Iteration 170, loss = 0.00634397
Iteration 171, loss = 0.00625017
Iteration 172, loss = 0.00613869
Iteration 173, loss = 0.00603936
Iteration 174, loss = 0.00597349
Iteration 175, loss = 0.00587566
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  30
Error rate: 11.40% ( 285/2500)
 - Class 1:  53, Class 2:  20, Class 3: 132, Class 4:  38, Class 5:  42
Iteration 1, loss = 1.40186014
Iteration 2, loss = 1.02199239
Iteration 3, loss = 0.75844062
Iteration 4, loss = 0.57829694
Iteration 5, loss = 0.45797149
Iteration 6, loss = 0.37556123
Iteration 7, loss = 0.31760399
Iteration 8, loss = 0.27591760
Iteration 9, loss = 0.24451123
Iteration 10, loss = 0.22033737
Iteration 11, loss = 0.20122355
Iteration 12, loss = 0.18553004
Iteration 13, loss = 0.17257456
Iteration 14, loss = 0.16165845
Iteration 15, loss = 0.15243632
Iteration 16, loss = 0.14446122
Iteration 17, loss = 0.13699209
Iteration 18, loss = 0.13091316
Iteration 19, loss = 0.12515277
Iteration 20, loss = 0.12042967
Iteration 21, loss = 0.11543665
Iteration 22, loss = 0.11166653
Iteration 23, loss = 0.10748914
Iteration 24, loss = 0.10383802
Iteration 25, loss = 0.10006131
Iteration 26, loss = 0.09686137
Iteration 27, loss = 0.09382045
Iteration 28, loss = 0.09105075
Iteration 29, loss = 0.08791422
Iteration 30, loss = 0.08564776
Iteration 31, loss = 0.08300742
Iteration 32, loss = 0.08046890
Iteration 33, loss = 0.07806361
Iteration 34, loss = 0.07600102
Iteration 35, loss = 0.07434405
Iteration 36, loss = 0.07208496
Iteration 37, loss = 0.07016974
Iteration 38, loss = 0.06835658
Iteration 39, loss = 0.06643129
Iteration 40, loss = 0.06465748
Iteration 41, loss = 0.06316860
Iteration 42, loss = 0.06154604
Iteration 43, loss = 0.05983553
Iteration 44, loss = 0.05839890
Iteration 45, loss = 0.05689918
Iteration 46, loss = 0.05550718
Iteration 47, loss = 0.05414193
Iteration 48, loss = 0.05279263
Iteration 49, loss = 0.05157881
Iteration 50, loss = 0.05033719
Iteration 51, loss = 0.04943244
Iteration 52, loss = 0.04823773
Iteration 53, loss = 0.04700656
Iteration 54, loss = 0.04595948
Iteration 55, loss = 0.04487760
Iteration 56, loss = 0.04387473
Iteration 57, loss = 0.04275533
Iteration 58, loss = 0.04216362
Iteration 59, loss = 0.04099774
Iteration 60, loss = 0.04005744
Iteration 61, loss = 0.03932756
Iteration 62, loss = 0.03825453
Iteration 63, loss = 0.03755112
Iteration 64, loss = 0.03668991
Iteration 65, loss = 0.03598453
Iteration 66, loss = 0.03519360
Iteration 67, loss = 0.03459638
Iteration 68, loss = 0.03373653
Iteration 69, loss = 0.03315560
Iteration 70, loss = 0.03242786
Iteration 71, loss = 0.03165259
Iteration 72, loss = 0.03090466
Iteration 73, loss = 0.03016492
Iteration 74, loss = 0.02970454
Iteration 75, loss = 0.02903309
Iteration 76, loss = 0.02839937
Iteration 77, loss = 0.02775866
Iteration 78, loss = 0.02715345
Iteration 79, loss = 0.02670655
Iteration 80, loss = 0.02627593
Iteration 81, loss = 0.02549511
Iteration 82, loss = 0.02501063
Iteration 83, loss = 0.02456771
Iteration 84, loss = 0.02399482
Iteration 85, loss = 0.02368896
Iteration 86, loss = 0.02319071
Iteration 87, loss = 0.02245018
Iteration 88, loss = 0.02209471
Iteration 89, loss = 0.02163353
Iteration 90, loss = 0.02124048
Iteration 91, loss = 0.02078725
Iteration 92, loss = 0.02039208
Iteration 93, loss = 0.01997822
Iteration 94, loss = 0.01948470
Iteration 95, loss = 0.01915465
Iteration 96, loss = 0.01879865
Iteration 97, loss = 0.01851030
Iteration 98, loss = 0.01800535
Iteration 99, loss = 0.01770760
Iteration 100, loss = 0.01737840
Iteration 101, loss = 0.01697189
Iteration 102, loss = 0.01657173
Iteration 103, loss = 0.01626494
Iteration 104, loss = 0.01598708
Iteration 105, loss = 0.01564892
Iteration 106, loss = 0.01532605
Iteration 107, loss = 0.01503320
Iteration 108, loss = 0.01464650
Iteration 109, loss = 0.01443595
Iteration 110, loss = 0.01407241
Iteration 111, loss = 0.01384894
Iteration 112, loss = 0.01359262
Iteration 113, loss = 0.01331296
Iteration 114, loss = 0.01308549
Iteration 115, loss = 0.01272455
Iteration 116, loss = 0.01253552
Iteration 117, loss = 0.01233476
Iteration 118, loss = 0.01201773
Iteration 119, loss = 0.01177627
Iteration 120, loss = 0.01156067
Iteration 121, loss = 0.01132825
Iteration 122, loss = 0.01113942
Iteration 123, loss = 0.01088726
Iteration 124, loss = 0.01069580
Iteration 125, loss = 0.01049460
Iteration 126, loss = 0.01026875
Iteration 127, loss = 0.01004808
Iteration 128, loss = 0.00987362
Iteration 129, loss = 0.00974420
Iteration 130, loss = 0.00951180
Iteration 131, loss = 0.00941256
Iteration 132, loss = 0.00911198
Iteration 133, loss = 0.00898219
Iteration 134, loss = 0.00887721
Iteration 135, loss = 0.00867702
Iteration 136, loss = 0.00848474
Iteration 137, loss = 0.00833683
Iteration 138, loss = 0.00821094
Iteration 139, loss = 0.00807109
Iteration 140, loss = 0.00786096
Iteration 141, loss = 0.00774555
Iteration 142, loss = 0.00765456
Iteration 143, loss = 0.00747643
Iteration 144, loss = 0.00738881
Iteration 145, loss = 0.00720798
Iteration 146, loss = 0.00713137
Iteration 147, loss = 0.00699980
Iteration 148, loss = 0.00684430
Iteration 149, loss = 0.00674887
Iteration 150, loss = 0.00657890
Iteration 151, loss = 0.00649254
Iteration 152, loss = 0.00638216
Iteration 153, loss = 0.00626519
Iteration 154, loss = 0.00619484
Iteration 155, loss = 0.00608324
Iteration 156, loss = 0.00595946
Iteration 157, loss = 0.00583409
Iteration 158, loss = 0.00576900
Iteration 159, loss = 0.00566285
Iteration 160, loss = 0.00555860
Iteration 161, loss = 0.00549504
Iteration 162, loss = 0.00538450
Iteration 163, loss = 0.00531396
Iteration 164, loss = 0.00522180
Iteration 165, loss = 0.00513402
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  35
Error rate: 24.48% ( 612/2500)
 - Class 1:  35, Class 2:  19, Class 3: 483, Class 4:  31, Class 5:  44
Iteration 1, loss = 1.42565305
Iteration 2, loss = 1.03626500
Iteration 3, loss = 0.77536343
Iteration 4, loss = 0.59048637
Iteration 5, loss = 0.46318305
Iteration 6, loss = 0.37700765
Iteration 7, loss = 0.31612082
Iteration 8, loss = 0.27283178
Iteration 9, loss = 0.24056995
Iteration 10, loss = 0.21643716
Iteration 11, loss = 0.19669519
Iteration 12, loss = 0.18118980
Iteration 13, loss = 0.16838905
Iteration 14, loss = 0.15728276
Iteration 15, loss = 0.14835138
Iteration 16, loss = 0.14060281
Iteration 17, loss = 0.13321327
Iteration 18, loss = 0.12712080
Iteration 19, loss = 0.12123888
Iteration 20, loss = 0.11619246
Iteration 21, loss = 0.11179945
Iteration 22, loss = 0.10730733
Iteration 23, loss = 0.10335514
Iteration 24, loss = 0.09994727
Iteration 25, loss = 0.09658732
Iteration 26, loss = 0.09326967
Iteration 27, loss = 0.09030639
Iteration 28, loss = 0.08767521
Iteration 29, loss = 0.08481322
Iteration 30, loss = 0.08232685
Iteration 31, loss = 0.07994592
Iteration 32, loss = 0.07760682
Iteration 33, loss = 0.07560889
Iteration 34, loss = 0.07338141
Iteration 35, loss = 0.07139173
Iteration 36, loss = 0.06963044
Iteration 37, loss = 0.06758770
Iteration 38, loss = 0.06609596
Iteration 39, loss = 0.06419170
Iteration 40, loss = 0.06261466
Iteration 41, loss = 0.06092266
Iteration 42, loss = 0.05942632
Iteration 43, loss = 0.05801032
Iteration 44, loss = 0.05687858
Iteration 45, loss = 0.05535421
Iteration 46, loss = 0.05368560
Iteration 47, loss = 0.05262746
Iteration 48, loss = 0.05141237
Iteration 49, loss = 0.04992490
Iteration 50, loss = 0.04896763
Iteration 51, loss = 0.04781498
Iteration 52, loss = 0.04657582
Iteration 53, loss = 0.04566943
Iteration 54, loss = 0.04458614
Iteration 55, loss = 0.04336565
Iteration 56, loss = 0.04238837
Iteration 57, loss = 0.04156038
Iteration 58, loss = 0.04048194
Iteration 59, loss = 0.03949382
Iteration 60, loss = 0.03874167
Iteration 61, loss = 0.03782005
Iteration 62, loss = 0.03694075
Iteration 63, loss = 0.03615870
Iteration 64, loss = 0.03537694
Iteration 65, loss = 0.03446792
Iteration 66, loss = 0.03372637
Iteration 67, loss = 0.03301765
Iteration 68, loss = 0.03235953
Iteration 69, loss = 0.03141854
Iteration 70, loss = 0.03087105
Iteration 71, loss = 0.03015286
Iteration 72, loss = 0.02944526
Iteration 73, loss = 0.02899984
Iteration 74, loss = 0.02817515
Iteration 75, loss = 0.02758065
Iteration 76, loss = 0.02697594
Iteration 77, loss = 0.02653867
Iteration 78, loss = 0.02587270
Iteration 79, loss = 0.02519382
Iteration 80, loss = 0.02465853
Iteration 81, loss = 0.02411925
Iteration 82, loss = 0.02371142
Iteration 83, loss = 0.02310192
Iteration 84, loss = 0.02260034
Iteration 85, loss = 0.02201939
Iteration 86, loss = 0.02169627
Iteration 87, loss = 0.02125276
Iteration 88, loss = 0.02059131
Iteration 89, loss = 0.02021701
Iteration 90, loss = 0.01976925
Iteration 91, loss = 0.01934845
Iteration 92, loss = 0.01886214
Iteration 93, loss = 0.01843258
Iteration 94, loss = 0.01813720
Iteration 95, loss = 0.01768518
Iteration 96, loss = 0.01738025
Iteration 97, loss = 0.01691884
Iteration 98, loss = 0.01659487
Iteration 99, loss = 0.01625331
Iteration 100, loss = 0.01584572
Iteration 101, loss = 0.01558571
Iteration 102, loss = 0.01529234
Iteration 103, loss = 0.01491538
Iteration 104, loss = 0.01462447
Iteration 105, loss = 0.01422886
Iteration 106, loss = 0.01398142
Iteration 107, loss = 0.01365812
Iteration 108, loss = 0.01332842
Iteration 109, loss = 0.01313335
Iteration 110, loss = 0.01280347
Iteration 111, loss = 0.01253672
Iteration 112, loss = 0.01230455
Iteration 113, loss = 0.01198900
Iteration 114, loss = 0.01177136
Iteration 115, loss = 0.01147594
Iteration 116, loss = 0.01126058
Iteration 117, loss = 0.01104281
Iteration 118, loss = 0.01081620
Iteration 119, loss = 0.01062837
Iteration 120, loss = 0.01035580
Iteration 121, loss = 0.01022674
Iteration 122, loss = 0.00995962
Iteration 123, loss = 0.00978604
Iteration 124, loss = 0.00963451
Iteration 125, loss = 0.00935444
Iteration 126, loss = 0.00919039
Iteration 127, loss = 0.00904446
Iteration 128, loss = 0.00890596
Iteration 129, loss = 0.00875641
Iteration 130, loss = 0.00858990
Iteration 131, loss = 0.00832706
Iteration 132, loss = 0.00825107
Iteration 133, loss = 0.00811272
Iteration 134, loss = 0.00791000
Iteration 135, loss = 0.00772590
Iteration 136, loss = 0.00763643
Iteration 137, loss = 0.00747396
Iteration 138, loss = 0.00730785
Iteration 139, loss = 0.00716413
Iteration 140, loss = 0.00704367
Iteration 141, loss = 0.00691622
Iteration 142, loss = 0.00678352
Iteration 143, loss = 0.00665689
Iteration 144, loss = 0.00656745
Iteration 145, loss = 0.00643649
Iteration 146, loss = 0.00631097
Iteration 147, loss = 0.00619357
Iteration 148, loss = 0.00610905
Iteration 149, loss = 0.00599593
Iteration 150, loss = 0.00588486
Iteration 151, loss = 0.00579598
Iteration 152, loss = 0.00567466
Iteration 153, loss = 0.00558336
Iteration 154, loss = 0.00548496
Iteration 155, loss = 0.00540450
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  40
Error rate: 24.80% ( 620/2500)
 - Class 1:  49, Class 2:  16, Class 3: 500, Class 4:  26, Class 5:  29
Iteration 1, loss = 1.34776898
Iteration 2, loss = 0.92394046
Iteration 3, loss = 0.65237678
Iteration 4, loss = 0.48397790
Iteration 5, loss = 0.37839874
Iteration 6, loss = 0.31041284
Iteration 7, loss = 0.26394664
Iteration 8, loss = 0.23101092
Iteration 9, loss = 0.20598210
Iteration 10, loss = 0.18686141
Iteration 11, loss = 0.17198754
Iteration 12, loss = 0.15945341
Iteration 13, loss = 0.14905685
Iteration 14, loss = 0.14020375
Iteration 15, loss = 0.13253482
Iteration 16, loss = 0.12584989
Iteration 17, loss = 0.11982270
Iteration 18, loss = 0.11445288
Iteration 19, loss = 0.10972576
Iteration 20, loss = 0.10522171
Iteration 21, loss = 0.10093273
Iteration 22, loss = 0.09740500
Iteration 23, loss = 0.09374336
Iteration 24, loss = 0.09042812
Iteration 25, loss = 0.08730385
Iteration 26, loss = 0.08436245
Iteration 27, loss = 0.08169859
Iteration 28, loss = 0.07944027
Iteration 29, loss = 0.07691110
Iteration 30, loss = 0.07484824
Iteration 31, loss = 0.07187648
Iteration 32, loss = 0.06981544
Iteration 33, loss = 0.06773480
Iteration 34, loss = 0.06584682
Iteration 35, loss = 0.06382016
Iteration 36, loss = 0.06208590
Iteration 37, loss = 0.06036729
Iteration 38, loss = 0.05886137
Iteration 39, loss = 0.05715488
Iteration 40, loss = 0.05552756
Iteration 41, loss = 0.05398870
Iteration 42, loss = 0.05257082
Iteration 43, loss = 0.05114257
Iteration 44, loss = 0.04990817
Iteration 45, loss = 0.04859490
Iteration 46, loss = 0.04719126
Iteration 47, loss = 0.04600654
Iteration 48, loss = 0.04495605
Iteration 49, loss = 0.04365577
Iteration 50, loss = 0.04284559
Iteration 51, loss = 0.04155015
Iteration 52, loss = 0.04078431
Iteration 53, loss = 0.03962303
Iteration 54, loss = 0.03850999
Iteration 55, loss = 0.03756426
Iteration 56, loss = 0.03670006
Iteration 57, loss = 0.03586877
Iteration 58, loss = 0.03495019
Iteration 59, loss = 0.03411370
Iteration 60, loss = 0.03316112
Iteration 61, loss = 0.03251800
Iteration 62, loss = 0.03161620
Iteration 63, loss = 0.03090750
Iteration 64, loss = 0.03041011
Iteration 65, loss = 0.02964726
Iteration 66, loss = 0.02881501
Iteration 67, loss = 0.02820906
Iteration 68, loss = 0.02762995
Iteration 69, loss = 0.02690705
Iteration 70, loss = 0.02627216
Iteration 71, loss = 0.02555832
Iteration 72, loss = 0.02500111
Iteration 73, loss = 0.02444278
Iteration 74, loss = 0.02403592
Iteration 75, loss = 0.02349778
Iteration 76, loss = 0.02283719
Iteration 77, loss = 0.02237027
Iteration 78, loss = 0.02178845
Iteration 79, loss = 0.02126763
Iteration 80, loss = 0.02097399
Iteration 81, loss = 0.02030347
Iteration 82, loss = 0.01973675
Iteration 83, loss = 0.01934259
Iteration 84, loss = 0.01885733
Iteration 85, loss = 0.01842238
Iteration 86, loss = 0.01805591
Iteration 87, loss = 0.01757833
Iteration 88, loss = 0.01713365
Iteration 89, loss = 0.01685325
Iteration 90, loss = 0.01643050
Iteration 91, loss = 0.01602888
Iteration 92, loss = 0.01579989
Iteration 93, loss = 0.01531438
Iteration 94, loss = 0.01498524
Iteration 95, loss = 0.01462346
Iteration 96, loss = 0.01428225
Iteration 97, loss = 0.01391359
Iteration 98, loss = 0.01368467
Iteration 99, loss = 0.01336382
Iteration 100, loss = 0.01306563
Iteration 101, loss = 0.01274549
Iteration 102, loss = 0.01246455
Iteration 103, loss = 0.01216452
Iteration 104, loss = 0.01184504
Iteration 105, loss = 0.01156392
Iteration 106, loss = 0.01129683
Iteration 107, loss = 0.01107035
Iteration 108, loss = 0.01080142
Iteration 109, loss = 0.01056909
Iteration 110, loss = 0.01036652
Iteration 111, loss = 0.01006491
Iteration 112, loss = 0.00986695
Iteration 113, loss = 0.00962290
Iteration 114, loss = 0.00946021
Iteration 115, loss = 0.00923843
Iteration 116, loss = 0.00904081
Iteration 117, loss = 0.00884298
Iteration 118, loss = 0.00867633
Iteration 119, loss = 0.00848579
Iteration 120, loss = 0.00830943
Iteration 121, loss = 0.00811822
Iteration 122, loss = 0.00802177
Iteration 123, loss = 0.00776614
Iteration 124, loss = 0.00760403
Iteration 125, loss = 0.00747642
Iteration 126, loss = 0.00727852
Iteration 127, loss = 0.00719870
Iteration 128, loss = 0.00700490
Iteration 129, loss = 0.00691863
Iteration 130, loss = 0.00674500
Iteration 131, loss = 0.00661586
Iteration 132, loss = 0.00647112
Iteration 133, loss = 0.00639376
Iteration 134, loss = 0.00621214
Iteration 135, loss = 0.00613289
Iteration 136, loss = 0.00599050
Iteration 137, loss = 0.00589341
Iteration 138, loss = 0.00582000
Iteration 139, loss = 0.00569286
Iteration 140, loss = 0.00560060
Iteration 141, loss = 0.00547748
Iteration 142, loss = 0.00536846
Iteration 143, loss = 0.00528165
Iteration 144, loss = 0.00518195
Iteration 145, loss = 0.00511337
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  45
Error rate: 24.44% ( 611/2500)
 - Class 1:  48, Class 2:  17, Class 3: 485, Class 4:  44, Class 5:  17
Iteration 1, loss = 1.38937061
Iteration 2, loss = 0.95170962
Iteration 3, loss = 0.68096359
Iteration 4, loss = 0.50438273
Iteration 5, loss = 0.39164342
Iteration 6, loss = 0.31878730
Iteration 7, loss = 0.26929806
Iteration 8, loss = 0.23443537
Iteration 9, loss = 0.20835352
Iteration 10, loss = 0.18871450
Iteration 11, loss = 0.17304453
Iteration 12, loss = 0.16057412
Iteration 13, loss = 0.14955472
Iteration 14, loss = 0.14093648
Iteration 15, loss = 0.13283020
Iteration 16, loss = 0.12596660
Iteration 17, loss = 0.12019553
Iteration 18, loss = 0.11476596
Iteration 19, loss = 0.10951884
Iteration 20, loss = 0.10511093
Iteration 21, loss = 0.10117999
Iteration 22, loss = 0.09712251
Iteration 23, loss = 0.09368309
Iteration 24, loss = 0.09058936
Iteration 25, loss = 0.08707058
Iteration 26, loss = 0.08446402
Iteration 27, loss = 0.08140178
Iteration 28, loss = 0.07867123
Iteration 29, loss = 0.07652082
Iteration 30, loss = 0.07367617
Iteration 31, loss = 0.07148115
Iteration 32, loss = 0.06933243
Iteration 33, loss = 0.06732045
Iteration 34, loss = 0.06536522
Iteration 35, loss = 0.06351016
Iteration 36, loss = 0.06196141
Iteration 37, loss = 0.06012922
Iteration 38, loss = 0.05829460
Iteration 39, loss = 0.05689765
Iteration 40, loss = 0.05531842
Iteration 41, loss = 0.05381981
Iteration 42, loss = 0.05254621
Iteration 43, loss = 0.05077999
Iteration 44, loss = 0.04942547
Iteration 45, loss = 0.04832817
Iteration 46, loss = 0.04728456
Iteration 47, loss = 0.04583420
Iteration 48, loss = 0.04460453
Iteration 49, loss = 0.04371568
Iteration 50, loss = 0.04253822
Iteration 51, loss = 0.04126614
Iteration 52, loss = 0.04022806
Iteration 53, loss = 0.03938878
Iteration 54, loss = 0.03818193
Iteration 55, loss = 0.03732961
Iteration 56, loss = 0.03646106
Iteration 57, loss = 0.03550040
Iteration 58, loss = 0.03467969
Iteration 59, loss = 0.03410147
Iteration 60, loss = 0.03322283
Iteration 61, loss = 0.03238226
Iteration 62, loss = 0.03171447
Iteration 63, loss = 0.03121550
Iteration 64, loss = 0.03000368
Iteration 65, loss = 0.02926690
Iteration 66, loss = 0.02863062
Iteration 67, loss = 0.02792306
Iteration 68, loss = 0.02720627
Iteration 69, loss = 0.02660487
Iteration 70, loss = 0.02601266
Iteration 71, loss = 0.02531488
Iteration 72, loss = 0.02478549
Iteration 73, loss = 0.02410209
Iteration 74, loss = 0.02365289
Iteration 75, loss = 0.02307839
Iteration 76, loss = 0.02255716
Iteration 77, loss = 0.02205057
Iteration 78, loss = 0.02158074
Iteration 79, loss = 0.02105604
Iteration 80, loss = 0.02054665
Iteration 81, loss = 0.02028775
Iteration 82, loss = 0.01952465
Iteration 83, loss = 0.01909434
Iteration 84, loss = 0.01873171
Iteration 85, loss = 0.01819611
Iteration 86, loss = 0.01778860
Iteration 87, loss = 0.01735218
Iteration 88, loss = 0.01695305
Iteration 89, loss = 0.01656756
Iteration 90, loss = 0.01610188
Iteration 91, loss = 0.01590212
Iteration 92, loss = 0.01542954
Iteration 93, loss = 0.01502447
Iteration 94, loss = 0.01474730
Iteration 95, loss = 0.01447294
Iteration 96, loss = 0.01409978
Iteration 97, loss = 0.01373894
Iteration 98, loss = 0.01335220
Iteration 99, loss = 0.01303474
Iteration 100, loss = 0.01273527
Iteration 101, loss = 0.01243361
Iteration 102, loss = 0.01222089
Iteration 103, loss = 0.01183043
Iteration 104, loss = 0.01161694
Iteration 105, loss = 0.01135613
Iteration 106, loss = 0.01111326
Iteration 107, loss = 0.01081935
Iteration 108, loss = 0.01063121
Iteration 109, loss = 0.01037645
Iteration 110, loss = 0.01011815
Iteration 111, loss = 0.00994930
Iteration 112, loss = 0.00965147
Iteration 113, loss = 0.00943894
Iteration 114, loss = 0.00927006
Iteration 115, loss = 0.00904382
Iteration 116, loss = 0.00888954
Iteration 117, loss = 0.00870013
Iteration 118, loss = 0.00849389
Iteration 119, loss = 0.00828027
Iteration 120, loss = 0.00809603
Iteration 121, loss = 0.00792076
Iteration 122, loss = 0.00778565
Iteration 123, loss = 0.00757758
Iteration 124, loss = 0.00746332
Iteration 125, loss = 0.00730773
Iteration 126, loss = 0.00717775
Iteration 127, loss = 0.00697350
Iteration 128, loss = 0.00683993
Iteration 129, loss = 0.00670642
Iteration 130, loss = 0.00656602
Iteration 131, loss = 0.00645089
Iteration 132, loss = 0.00631915
Iteration 133, loss = 0.00622105
Iteration 134, loss = 0.00608155
Iteration 135, loss = 0.00595937
Iteration 136, loss = 0.00583729
Iteration 137, loss = 0.00573091
Iteration 138, loss = 0.00565642
Iteration 139, loss = 0.00555070
Iteration 140, loss = 0.00541129
Iteration 141, loss = 0.00533500
Iteration 142, loss = 0.00521460
Iteration 143, loss = 0.00510293
Iteration 144, loss = 0.00503615
Iteration 145, loss = 0.00495983
Iteration 146, loss = 0.00486029
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  50
Error rate: 23.96% ( 599/2500)
 - Class 1:  33, Class 2:  19, Class 3: 491, Class 4:  37, Class 5:  19
Iteration 1, loss = 1.34341166
Iteration 2, loss = 0.88483810
Iteration 3, loss = 0.61139257
Iteration 4, loss = 0.44681369
Iteration 5, loss = 0.34741968
Iteration 6, loss = 0.28413811
Iteration 7, loss = 0.24175997
Iteration 8, loss = 0.21211296
Iteration 9, loss = 0.19006040
Iteration 10, loss = 0.17245729
Iteration 11, loss = 0.15871637
Iteration 12, loss = 0.14779586
Iteration 13, loss = 0.13834859
Iteration 14, loss = 0.12995259
Iteration 15, loss = 0.12328189
Iteration 16, loss = 0.11662283
Iteration 17, loss = 0.11146270
Iteration 18, loss = 0.10655857
Iteration 19, loss = 0.10211140
Iteration 20, loss = 0.09788771
Iteration 21, loss = 0.09398475
Iteration 22, loss = 0.09047305
Iteration 23, loss = 0.08726996
Iteration 24, loss = 0.08404653
Iteration 25, loss = 0.08192843
Iteration 26, loss = 0.07836310
Iteration 27, loss = 0.07607202
Iteration 28, loss = 0.07333262
Iteration 29, loss = 0.07130788
Iteration 30, loss = 0.06876892
Iteration 31, loss = 0.06658280
Iteration 32, loss = 0.06447820
Iteration 33, loss = 0.06254676
Iteration 34, loss = 0.06094069
Iteration 35, loss = 0.05908983
Iteration 36, loss = 0.05718783
Iteration 37, loss = 0.05561982
Iteration 38, loss = 0.05439486
Iteration 39, loss = 0.05240955
Iteration 40, loss = 0.05111395
Iteration 41, loss = 0.04972178
Iteration 42, loss = 0.04814717
Iteration 43, loss = 0.04675278
Iteration 44, loss = 0.04578195
Iteration 45, loss = 0.04449940
Iteration 46, loss = 0.04329420
Iteration 47, loss = 0.04210470
Iteration 48, loss = 0.04108444
Iteration 49, loss = 0.03987868
Iteration 50, loss = 0.03897877
Iteration 51, loss = 0.03786807
Iteration 52, loss = 0.03685468
Iteration 53, loss = 0.03597973
Iteration 54, loss = 0.03509140
Iteration 55, loss = 0.03411995
Iteration 56, loss = 0.03312343
Iteration 57, loss = 0.03263013
Iteration 58, loss = 0.03165125
Iteration 59, loss = 0.03087362
Iteration 60, loss = 0.03040197
Iteration 61, loss = 0.02926199
Iteration 62, loss = 0.02855556
Iteration 63, loss = 0.02781596
Iteration 64, loss = 0.02723606
Iteration 65, loss = 0.02645810
Iteration 66, loss = 0.02589946
Iteration 67, loss = 0.02517925
Iteration 68, loss = 0.02459638
Iteration 69, loss = 0.02396639
Iteration 70, loss = 0.02324409
Iteration 71, loss = 0.02278621
Iteration 72, loss = 0.02209610
Iteration 73, loss = 0.02155453
Iteration 74, loss = 0.02101573
Iteration 75, loss = 0.02043914
Iteration 76, loss = 0.02007008
Iteration 77, loss = 0.01955290
Iteration 78, loss = 0.01895556
Iteration 79, loss = 0.01851590
Iteration 80, loss = 0.01803582
Iteration 81, loss = 0.01772474
Iteration 82, loss = 0.01720947
Iteration 83, loss = 0.01670893
Iteration 84, loss = 0.01632357
Iteration 85, loss = 0.01600890
Iteration 86, loss = 0.01553610
Iteration 87, loss = 0.01505579
Iteration 88, loss = 0.01472740
Iteration 89, loss = 0.01435895
Iteration 90, loss = 0.01392192
Iteration 91, loss = 0.01355659
Iteration 92, loss = 0.01320192
Iteration 93, loss = 0.01291397
Iteration 94, loss = 0.01261562
Iteration 95, loss = 0.01227922
Iteration 96, loss = 0.01199970
Iteration 97, loss = 0.01166935
Iteration 98, loss = 0.01141534
Iteration 99, loss = 0.01116800
Iteration 100, loss = 0.01085501
Iteration 101, loss = 0.01055483
Iteration 102, loss = 0.01037078
Iteration 103, loss = 0.01014580
Iteration 104, loss = 0.00997175
Iteration 105, loss = 0.00965830
Iteration 106, loss = 0.00941674
Iteration 107, loss = 0.00913563
Iteration 108, loss = 0.00895731
Iteration 109, loss = 0.00874816
Iteration 110, loss = 0.00854772
Iteration 111, loss = 0.00836801
Iteration 112, loss = 0.00811964
Iteration 113, loss = 0.00794323
Iteration 114, loss = 0.00775013
Iteration 115, loss = 0.00756725
Iteration 116, loss = 0.00740524
Iteration 117, loss = 0.00734043
Iteration 118, loss = 0.00715129
Iteration 119, loss = 0.00694057
Iteration 120, loss = 0.00679746
Iteration 121, loss = 0.00662213
Iteration 122, loss = 0.00650317
Iteration 123, loss = 0.00640283
Iteration 124, loss = 0.00622686
Iteration 125, loss = 0.00608138
Iteration 126, loss = 0.00596641
Iteration 127, loss = 0.00587036
Iteration 128, loss = 0.00571861
Iteration 129, loss = 0.00562224
Iteration 130, loss = 0.00547790
Iteration 131, loss = 0.00538148
Iteration 132, loss = 0.00527561
Iteration 133, loss = 0.00517044
Iteration 134, loss = 0.00510505
Iteration 135, loss = 0.00496340
Iteration 136, loss = 0.00487157
Iteration 137, loss = 0.00479538
Iteration 138, loss = 0.00468004
Iteration 139, loss = 0.00458999
Iteration 140, loss = 0.00452795
Iteration 141, loss = 0.00444494
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  55
Error rate: 11.44% ( 286/2500)
 - Class 1:  46, Class 2:  17, Class 3: 133, Class 4:  66, Class 5:  24
Iteration 1, loss = 1.35669065
Iteration 2, loss = 0.86795904
Iteration 3, loss = 0.58968257
Iteration 4, loss = 0.42903827
Iteration 5, loss = 0.33331175
Iteration 6, loss = 0.27359414
Iteration 7, loss = 0.23334693
Iteration 8, loss = 0.20491248
Iteration 9, loss = 0.18344345
Iteration 10, loss = 0.16731103
Iteration 11, loss = 0.15411331
Iteration 12, loss = 0.14338617
Iteration 13, loss = 0.13446822
Iteration 14, loss = 0.12682241
Iteration 15, loss = 0.12012806
Iteration 16, loss = 0.11389806
Iteration 17, loss = 0.10864379
Iteration 18, loss = 0.10360185
Iteration 19, loss = 0.09972508
Iteration 20, loss = 0.09522878
Iteration 21, loss = 0.09163549
Iteration 22, loss = 0.08851207
Iteration 23, loss = 0.08532152
Iteration 24, loss = 0.08234640
Iteration 25, loss = 0.07911035
Iteration 26, loss = 0.07688356
Iteration 27, loss = 0.07411464
Iteration 28, loss = 0.07137619
Iteration 29, loss = 0.06948615
Iteration 30, loss = 0.06716184
Iteration 31, loss = 0.06472883
Iteration 32, loss = 0.06309446
Iteration 33, loss = 0.06063573
Iteration 34, loss = 0.05895603
Iteration 35, loss = 0.05718679
Iteration 36, loss = 0.05598545
Iteration 37, loss = 0.05411297
Iteration 38, loss = 0.05252641
Iteration 39, loss = 0.05066629
Iteration 40, loss = 0.04953990
Iteration 41, loss = 0.04807492
Iteration 42, loss = 0.04698411
Iteration 43, loss = 0.04553398
Iteration 44, loss = 0.04433215
Iteration 45, loss = 0.04320273
Iteration 46, loss = 0.04173452
Iteration 47, loss = 0.04070799
Iteration 48, loss = 0.03936727
Iteration 49, loss = 0.03857024
Iteration 50, loss = 0.03762231
Iteration 51, loss = 0.03642008
Iteration 52, loss = 0.03559445
Iteration 53, loss = 0.03469183
Iteration 54, loss = 0.03384715
Iteration 55, loss = 0.03292235
Iteration 56, loss = 0.03183506
Iteration 57, loss = 0.03143588
Iteration 58, loss = 0.03038308
Iteration 59, loss = 0.02957871
Iteration 60, loss = 0.02886942
Iteration 61, loss = 0.02822968
Iteration 62, loss = 0.02743897
Iteration 63, loss = 0.02677899
Iteration 64, loss = 0.02614888
Iteration 65, loss = 0.02557020
Iteration 66, loss = 0.02494399
Iteration 67, loss = 0.02407759
Iteration 68, loss = 0.02365958
Iteration 69, loss = 0.02315701
Iteration 70, loss = 0.02244169
Iteration 71, loss = 0.02177990
Iteration 72, loss = 0.02136219
Iteration 73, loss = 0.02068416
Iteration 74, loss = 0.02024720
Iteration 75, loss = 0.01987648
Iteration 76, loss = 0.01918949
Iteration 77, loss = 0.01866856
Iteration 78, loss = 0.01814221
Iteration 79, loss = 0.01765172
Iteration 80, loss = 0.01721766
Iteration 81, loss = 0.01685535
Iteration 82, loss = 0.01630363
Iteration 83, loss = 0.01587018
Iteration 84, loss = 0.01550258
Iteration 85, loss = 0.01516089
Iteration 86, loss = 0.01472727
Iteration 87, loss = 0.01433425
Iteration 88, loss = 0.01394792
Iteration 89, loss = 0.01360799
Iteration 90, loss = 0.01326915
Iteration 91, loss = 0.01296078
Iteration 92, loss = 0.01263842
Iteration 93, loss = 0.01225951
Iteration 94, loss = 0.01205199
Iteration 95, loss = 0.01181144
Iteration 96, loss = 0.01134479
Iteration 97, loss = 0.01115204
Iteration 98, loss = 0.01092362
Iteration 99, loss = 0.01063450
Iteration 100, loss = 0.01025520
Iteration 101, loss = 0.01005917
Iteration 102, loss = 0.00996832
Iteration 103, loss = 0.00959383
Iteration 104, loss = 0.00939394
Iteration 105, loss = 0.00917255
Iteration 106, loss = 0.00895942
Iteration 107, loss = 0.00874249
Iteration 108, loss = 0.00858579
Iteration 109, loss = 0.00835185
Iteration 110, loss = 0.00813885
Iteration 111, loss = 0.00793969
Iteration 112, loss = 0.00778083
Iteration 113, loss = 0.00760650
Iteration 114, loss = 0.00744843
Iteration 115, loss = 0.00723032
Iteration 116, loss = 0.00710633
Iteration 117, loss = 0.00695932
Iteration 118, loss = 0.00678094
Iteration 119, loss = 0.00667967
Iteration 120, loss = 0.00651205
Iteration 121, loss = 0.00634963
Iteration 122, loss = 0.00623528
Iteration 123, loss = 0.00608487
Iteration 124, loss = 0.00593679
Iteration 125, loss = 0.00583594
Iteration 126, loss = 0.00572660
Iteration 127, loss = 0.00561759
Iteration 128, loss = 0.00547919
Iteration 129, loss = 0.00538530
Iteration 130, loss = 0.00527283
Iteration 131, loss = 0.00516356
Iteration 132, loss = 0.00508398
Iteration 133, loss = 0.00502129
Iteration 134, loss = 0.00486642
Iteration 135, loss = 0.00477980
Iteration 136, loss = 0.00470853
Iteration 137, loss = 0.00458864
Iteration 138, loss = 0.00451698
Iteration 139, loss = 0.00442024
Iteration 140, loss = 0.00435632
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  60
Error rate: 24.52% ( 613/2500)
 - Class 1:  21, Class 2:  18, Class 3: 485, Class 4:  32, Class 5:  57
Iteration 1, loss = 1.28172845
Iteration 2, loss = 0.80022807
Iteration 3, loss = 0.52580053
Iteration 4, loss = 0.37694183
Iteration 5, loss = 0.29248470
Iteration 6, loss = 0.24173222
Iteration 7, loss = 0.20747591
Iteration 8, loss = 0.18376469
Iteration 9, loss = 0.16587859
Iteration 10, loss = 0.15215561
Iteration 11, loss = 0.14081489
Iteration 12, loss = 0.13122684
Iteration 13, loss = 0.12346041
Iteration 14, loss = 0.11644795
Iteration 15, loss = 0.11064770
Iteration 16, loss = 0.10574462
Iteration 17, loss = 0.10065934
Iteration 18, loss = 0.09643056
Iteration 19, loss = 0.09224803
Iteration 20, loss = 0.08926270
Iteration 21, loss = 0.08588730
Iteration 22, loss = 0.08262597
Iteration 23, loss = 0.07943320
Iteration 24, loss = 0.07616703
Iteration 25, loss = 0.07408028
Iteration 26, loss = 0.07145397
Iteration 27, loss = 0.06902508
Iteration 28, loss = 0.06664375
Iteration 29, loss = 0.06442187
Iteration 30, loss = 0.06250908
Iteration 31, loss = 0.06064838
Iteration 32, loss = 0.05874590
Iteration 33, loss = 0.05640146
Iteration 34, loss = 0.05484276
Iteration 35, loss = 0.05301377
Iteration 36, loss = 0.05161885
Iteration 37, loss = 0.05003083
Iteration 38, loss = 0.04890564
Iteration 39, loss = 0.04717361
Iteration 40, loss = 0.04568934
Iteration 41, loss = 0.04464747
Iteration 42, loss = 0.04332894
Iteration 43, loss = 0.04187151
Iteration 44, loss = 0.04073229
Iteration 45, loss = 0.03961560
Iteration 46, loss = 0.03841770
Iteration 47, loss = 0.03737626
Iteration 48, loss = 0.03643397
Iteration 49, loss = 0.03526933
Iteration 50, loss = 0.03492856
Iteration 51, loss = 0.03342074
Iteration 52, loss = 0.03259616
Iteration 53, loss = 0.03194300
Iteration 54, loss = 0.03107763
Iteration 55, loss = 0.03004143
Iteration 56, loss = 0.02939669
Iteration 57, loss = 0.02836464
Iteration 58, loss = 0.02771456
Iteration 59, loss = 0.02700789
Iteration 60, loss = 0.02608698
Iteration 61, loss = 0.02552788
Iteration 62, loss = 0.02466013
Iteration 63, loss = 0.02399951
Iteration 64, loss = 0.02362781
Iteration 65, loss = 0.02302056
Iteration 66, loss = 0.02235563
Iteration 67, loss = 0.02159092
Iteration 68, loss = 0.02117164
Iteration 69, loss = 0.02047144
Iteration 70, loss = 0.01994830
Iteration 71, loss = 0.01943365
Iteration 72, loss = 0.01893962
Iteration 73, loss = 0.01837030
Iteration 74, loss = 0.01793112
Iteration 75, loss = 0.01743567
Iteration 76, loss = 0.01693597
Iteration 77, loss = 0.01661473
Iteration 78, loss = 0.01604622
Iteration 79, loss = 0.01572145
Iteration 80, loss = 0.01519650
Iteration 81, loss = 0.01485235
Iteration 82, loss = 0.01456417
Iteration 83, loss = 0.01404292
Iteration 84, loss = 0.01372082
Iteration 85, loss = 0.01330076
Iteration 86, loss = 0.01301129
Iteration 87, loss = 0.01275239
Iteration 88, loss = 0.01233596
Iteration 89, loss = 0.01198045
Iteration 90, loss = 0.01168186
Iteration 91, loss = 0.01141902
Iteration 92, loss = 0.01112700
Iteration 93, loss = 0.01081176
Iteration 94, loss = 0.01056404
Iteration 95, loss = 0.01032157
Iteration 96, loss = 0.01015651
Iteration 97, loss = 0.00985920
Iteration 98, loss = 0.00960171
Iteration 99, loss = 0.00940109
Iteration 100, loss = 0.00916762
Iteration 101, loss = 0.00894429
Iteration 102, loss = 0.00872250
Iteration 103, loss = 0.00851995
Iteration 104, loss = 0.00837333
Iteration 105, loss = 0.00814613
Iteration 106, loss = 0.00797296
Iteration 107, loss = 0.00783002
Iteration 108, loss = 0.00760986
Iteration 109, loss = 0.00740152
Iteration 110, loss = 0.00726417
Iteration 111, loss = 0.00707781
Iteration 112, loss = 0.00693652
Iteration 113, loss = 0.00679412
Iteration 114, loss = 0.00665725
Iteration 115, loss = 0.00647376
Iteration 116, loss = 0.00631095
Iteration 117, loss = 0.00619570
Iteration 118, loss = 0.00604592
Iteration 119, loss = 0.00597609
Iteration 120, loss = 0.00580826
Iteration 121, loss = 0.00570496
Iteration 122, loss = 0.00562603
Iteration 123, loss = 0.00548694
Iteration 124, loss = 0.00535311
Iteration 125, loss = 0.00527871
Iteration 126, loss = 0.00512984
Iteration 127, loss = 0.00501318
Iteration 128, loss = 0.00494269
Iteration 129, loss = 0.00486392
Iteration 130, loss = 0.00474723
Iteration 131, loss = 0.00466137
Iteration 132, loss = 0.00458555
Iteration 133, loss = 0.00446986
Iteration 134, loss = 0.00436336
Iteration 135, loss = 0.00428725
Iteration 136, loss = 0.00419491
Iteration 137, loss = 0.00413694
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  65
Error rate: 25.08% ( 627/2500)
 - Class 1:  27, Class 2:  15, Class 3: 500, Class 4:  34, Class 5:  51
Iteration 1, loss = 1.27462247
Iteration 2, loss = 0.77432274
Iteration 3, loss = 0.51036344
Iteration 4, loss = 0.36792223
Iteration 5, loss = 0.28775188
Iteration 6, loss = 0.23815113
Iteration 7, loss = 0.20518317
Iteration 8, loss = 0.18138359
Iteration 9, loss = 0.16389665
Iteration 10, loss = 0.15020945
Iteration 11, loss = 0.13935461
Iteration 12, loss = 0.12986110
Iteration 13, loss = 0.12211539
Iteration 14, loss = 0.11536965
Iteration 15, loss = 0.10988470
Iteration 16, loss = 0.10456688
Iteration 17, loss = 0.09957642
Iteration 18, loss = 0.09533260
Iteration 19, loss = 0.09138353
Iteration 20, loss = 0.08794506
Iteration 21, loss = 0.08471606
Iteration 22, loss = 0.08136801
Iteration 23, loss = 0.07844228
Iteration 24, loss = 0.07555255
Iteration 25, loss = 0.07294557
Iteration 26, loss = 0.07029370
Iteration 27, loss = 0.06816839
Iteration 28, loss = 0.06582674
Iteration 29, loss = 0.06358956
Iteration 30, loss = 0.06156250
Iteration 31, loss = 0.05963024
Iteration 32, loss = 0.05782685
Iteration 33, loss = 0.05627446
Iteration 34, loss = 0.05413625
Iteration 35, loss = 0.05255716
Iteration 36, loss = 0.05117177
Iteration 37, loss = 0.04970593
Iteration 38, loss = 0.04831848
Iteration 39, loss = 0.04691889
Iteration 40, loss = 0.04571154
Iteration 41, loss = 0.04449008
Iteration 42, loss = 0.04300170
Iteration 43, loss = 0.04187021
Iteration 44, loss = 0.04054704
Iteration 45, loss = 0.03940488
Iteration 46, loss = 0.03843949
Iteration 47, loss = 0.03713758
Iteration 48, loss = 0.03609067
Iteration 49, loss = 0.03532622
Iteration 50, loss = 0.03414093
Iteration 51, loss = 0.03316961
Iteration 52, loss = 0.03258917
Iteration 53, loss = 0.03136750
Iteration 54, loss = 0.03114172
Iteration 55, loss = 0.02997929
Iteration 56, loss = 0.02927549
Iteration 57, loss = 0.02821597
Iteration 58, loss = 0.02756053
Iteration 59, loss = 0.02657722
Iteration 60, loss = 0.02605037
Iteration 61, loss = 0.02516724
Iteration 62, loss = 0.02454773
Iteration 63, loss = 0.02391181
Iteration 64, loss = 0.02353012
Iteration 65, loss = 0.02268125
Iteration 66, loss = 0.02189620
Iteration 67, loss = 0.02131606
Iteration 68, loss = 0.02092566
Iteration 69, loss = 0.02027625
Iteration 70, loss = 0.01984579
Iteration 71, loss = 0.01927668
Iteration 72, loss = 0.01859047
Iteration 73, loss = 0.01810026
Iteration 74, loss = 0.01771223
Iteration 75, loss = 0.01717212
Iteration 76, loss = 0.01670985
Iteration 77, loss = 0.01622057
Iteration 78, loss = 0.01577562
Iteration 79, loss = 0.01546737
Iteration 80, loss = 0.01485801
Iteration 81, loss = 0.01453544
Iteration 82, loss = 0.01418787
Iteration 83, loss = 0.01379227
Iteration 84, loss = 0.01342623
Iteration 85, loss = 0.01301400
Iteration 86, loss = 0.01268475
Iteration 87, loss = 0.01239609
Iteration 88, loss = 0.01209823
Iteration 89, loss = 0.01188013
Iteration 90, loss = 0.01139793
Iteration 91, loss = 0.01102812
Iteration 92, loss = 0.01084658
Iteration 93, loss = 0.01073979
Iteration 94, loss = 0.01023885
Iteration 95, loss = 0.01004195
Iteration 96, loss = 0.00979777
Iteration 97, loss = 0.00948145
Iteration 98, loss = 0.00925451
Iteration 99, loss = 0.00892136
Iteration 100, loss = 0.00875876
Iteration 101, loss = 0.00856774
Iteration 102, loss = 0.00833923
Iteration 103, loss = 0.00818795
Iteration 104, loss = 0.00795947
Iteration 105, loss = 0.00771338
Iteration 106, loss = 0.00760551
Iteration 107, loss = 0.00739952
Iteration 108, loss = 0.00720633
Iteration 109, loss = 0.00699865
Iteration 110, loss = 0.00696047
Iteration 111, loss = 0.00674369
Iteration 112, loss = 0.00658440
Iteration 113, loss = 0.00639636
Iteration 114, loss = 0.00626013
Iteration 115, loss = 0.00615903
Iteration 116, loss = 0.00596862
Iteration 117, loss = 0.00594993
Iteration 118, loss = 0.00571740
Iteration 119, loss = 0.00560986
Iteration 120, loss = 0.00546876
Iteration 121, loss = 0.00540408
Iteration 122, loss = 0.00528690
Iteration 123, loss = 0.00511753
Iteration 124, loss = 0.00501992
Iteration 125, loss = 0.00492105
Iteration 126, loss = 0.00482491
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  70
Error rate: 24.56% ( 614/2500)
 - Class 1:  31, Class 2:  24, Class 3: 494, Class 4:  38, Class 5:  27
Iteration 1, loss = 1.28322072
Iteration 2, loss = 0.76544487
Iteration 3, loss = 0.49511614
Iteration 4, loss = 0.35344828
Iteration 5, loss = 0.27617387
Iteration 6, loss = 0.22901271
Iteration 7, loss = 0.19797415
Iteration 8, loss = 0.17603721
Iteration 9, loss = 0.16016517
Iteration 10, loss = 0.14723225
Iteration 11, loss = 0.13624571
Iteration 12, loss = 0.12754342
Iteration 13, loss = 0.12040991
Iteration 14, loss = 0.11412811
Iteration 15, loss = 0.10819558
Iteration 16, loss = 0.10336322
Iteration 17, loss = 0.09807989
Iteration 18, loss = 0.09436933
Iteration 19, loss = 0.09027809
Iteration 20, loss = 0.08701596
Iteration 21, loss = 0.08351692
Iteration 22, loss = 0.08020296
Iteration 23, loss = 0.07710205
Iteration 24, loss = 0.07473817
Iteration 25, loss = 0.07197129
Iteration 26, loss = 0.06951590
Iteration 27, loss = 0.06713673
Iteration 28, loss = 0.06476364
Iteration 29, loss = 0.06316611
Iteration 30, loss = 0.06104003
Iteration 31, loss = 0.05855963
Iteration 32, loss = 0.05670505
Iteration 33, loss = 0.05513532
Iteration 34, loss = 0.05368472
Iteration 35, loss = 0.05177303
Iteration 36, loss = 0.05035531
Iteration 37, loss = 0.04875606
Iteration 38, loss = 0.04709740
Iteration 39, loss = 0.04594636
Iteration 40, loss = 0.04447236
Iteration 41, loss = 0.04317187
Iteration 42, loss = 0.04174286
Iteration 43, loss = 0.04050578
Iteration 44, loss = 0.03967946
Iteration 45, loss = 0.03831109
Iteration 46, loss = 0.03722272
Iteration 47, loss = 0.03634848
Iteration 48, loss = 0.03518976
Iteration 49, loss = 0.03432481
Iteration 50, loss = 0.03338109
Iteration 51, loss = 0.03240259
Iteration 52, loss = 0.03121271
Iteration 53, loss = 0.03044110
Iteration 54, loss = 0.02974850
Iteration 55, loss = 0.02896218
Iteration 56, loss = 0.02793512
Iteration 57, loss = 0.02744321
Iteration 58, loss = 0.02641274
Iteration 59, loss = 0.02580612
Iteration 60, loss = 0.02518814
Iteration 61, loss = 0.02423688
Iteration 62, loss = 0.02354164
Iteration 63, loss = 0.02293025
Iteration 64, loss = 0.02214624
Iteration 65, loss = 0.02168972
Iteration 66, loss = 0.02101870
Iteration 67, loss = 0.02047981
Iteration 68, loss = 0.01973758
Iteration 69, loss = 0.01927671
Iteration 70, loss = 0.01875298
Iteration 71, loss = 0.01819786
Iteration 72, loss = 0.01778887
Iteration 73, loss = 0.01725925
Iteration 74, loss = 0.01672841
Iteration 75, loss = 0.01627889
Iteration 76, loss = 0.01576542
Iteration 77, loss = 0.01548942
Iteration 78, loss = 0.01494452
Iteration 79, loss = 0.01465273
Iteration 80, loss = 0.01414479
Iteration 81, loss = 0.01367481
Iteration 82, loss = 0.01329368
Iteration 83, loss = 0.01301541
Iteration 84, loss = 0.01266969
Iteration 85, loss = 0.01236218
Iteration 86, loss = 0.01209457
Iteration 87, loss = 0.01166355
Iteration 88, loss = 0.01142008
Iteration 89, loss = 0.01106860
Iteration 90, loss = 0.01075609
Iteration 91, loss = 0.01046409
Iteration 92, loss = 0.01024614
Iteration 93, loss = 0.00999227
Iteration 94, loss = 0.00972231
Iteration 95, loss = 0.00940703
Iteration 96, loss = 0.00913569
Iteration 97, loss = 0.00897038
Iteration 98, loss = 0.00872058
Iteration 99, loss = 0.00853588
Iteration 100, loss = 0.00838540
Iteration 101, loss = 0.00813464
Iteration 102, loss = 0.00788386
Iteration 103, loss = 0.00770159
Iteration 104, loss = 0.00757389
Iteration 105, loss = 0.00732743
Iteration 106, loss = 0.00717562
Iteration 107, loss = 0.00695536
Iteration 108, loss = 0.00682104
Iteration 109, loss = 0.00663624
Iteration 110, loss = 0.00653860
Iteration 111, loss = 0.00637640
Iteration 112, loss = 0.00618266
Iteration 113, loss = 0.00605149
Iteration 114, loss = 0.00591036
Iteration 115, loss = 0.00581717
Iteration 116, loss = 0.00565445
Iteration 117, loss = 0.00553294
Iteration 118, loss = 0.00538400
Iteration 119, loss = 0.00526893
Iteration 120, loss = 0.00517692
Iteration 121, loss = 0.00504378
Iteration 122, loss = 0.00490335
Iteration 123, loss = 0.00482207
Iteration 124, loss = 0.00473015
Iteration 125, loss = 0.00460727
Iteration 126, loss = 0.00454450
Iteration 127, loss = 0.00444728
Iteration 128, loss = 0.00433379
Iteration 129, loss = 0.00427938
Iteration 130, loss = 0.00417394
Iteration 131, loss = 0.00406891
Iteration 132, loss = 0.00402894
Iteration 133, loss = 0.00393797
Iteration 134, loss = 0.00385030
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  75
Error rate: 23.84% ( 596/2500)
 - Class 1:  16, Class 2:  19, Class 3: 480, Class 4:  46, Class 5:  35
Iteration 1, loss = 1.24973474
Iteration 2, loss = 0.72678929
Iteration 3, loss = 0.46184962
Iteration 4, loss = 0.33060248
Iteration 5, loss = 0.25895609
Iteration 6, loss = 0.21678358
Iteration 7, loss = 0.18794353
Iteration 8, loss = 0.16788128
Iteration 9, loss = 0.15211517
Iteration 10, loss = 0.14044949
Iteration 11, loss = 0.13014198
Iteration 12, loss = 0.12220720
Iteration 13, loss = 0.11457205
Iteration 14, loss = 0.10889032
Iteration 15, loss = 0.10304758
Iteration 16, loss = 0.09837672
Iteration 17, loss = 0.09440040
Iteration 18, loss = 0.08978322
Iteration 19, loss = 0.08601942
Iteration 20, loss = 0.08277963
Iteration 21, loss = 0.07977365
Iteration 22, loss = 0.07681723
Iteration 23, loss = 0.07393414
Iteration 24, loss = 0.07108645
Iteration 25, loss = 0.06842517
Iteration 26, loss = 0.06649312
Iteration 27, loss = 0.06448921
Iteration 28, loss = 0.06206323
Iteration 29, loss = 0.05990858
Iteration 30, loss = 0.05789882
Iteration 31, loss = 0.05609074
Iteration 32, loss = 0.05471867
Iteration 33, loss = 0.05250863
Iteration 34, loss = 0.05094303
Iteration 35, loss = 0.04941015
Iteration 36, loss = 0.04794663
Iteration 37, loss = 0.04632193
Iteration 38, loss = 0.04507172
Iteration 39, loss = 0.04336846
Iteration 40, loss = 0.04222146
Iteration 41, loss = 0.04087486
Iteration 42, loss = 0.03990513
Iteration 43, loss = 0.03860562
Iteration 44, loss = 0.03788624
Iteration 45, loss = 0.03653143
Iteration 46, loss = 0.03511655
Iteration 47, loss = 0.03428023
Iteration 48, loss = 0.03317661
Iteration 49, loss = 0.03236997
Iteration 50, loss = 0.03141774
Iteration 51, loss = 0.03054983
Iteration 52, loss = 0.02972369
Iteration 53, loss = 0.02876721
Iteration 54, loss = 0.02784109
Iteration 55, loss = 0.02708431
Iteration 56, loss = 0.02631373
Iteration 57, loss = 0.02555020
Iteration 58, loss = 0.02499772
Iteration 59, loss = 0.02421936
Iteration 60, loss = 0.02366338
Iteration 61, loss = 0.02306621
Iteration 62, loss = 0.02214303
Iteration 63, loss = 0.02133686
Iteration 64, loss = 0.02077834
Iteration 65, loss = 0.02009742
Iteration 66, loss = 0.01961629
Iteration 67, loss = 0.01904169
Iteration 68, loss = 0.01838235
Iteration 69, loss = 0.01787849
Iteration 70, loss = 0.01752216
Iteration 71, loss = 0.01689169
Iteration 72, loss = 0.01631400
Iteration 73, loss = 0.01585102
Iteration 74, loss = 0.01545090
Iteration 75, loss = 0.01494983
Iteration 76, loss = 0.01482889
Iteration 77, loss = 0.01423032
Iteration 78, loss = 0.01376551
Iteration 79, loss = 0.01336744
Iteration 80, loss = 0.01308124
Iteration 81, loss = 0.01255768
Iteration 82, loss = 0.01222975
Iteration 83, loss = 0.01185694
Iteration 84, loss = 0.01154581
Iteration 85, loss = 0.01124661
Iteration 86, loss = 0.01089028
Iteration 87, loss = 0.01060591
Iteration 88, loss = 0.01031392
Iteration 89, loss = 0.01006402
Iteration 90, loss = 0.00977117
Iteration 91, loss = 0.00953269
Iteration 92, loss = 0.00932194
Iteration 93, loss = 0.00897816
Iteration 94, loss = 0.00871848
Iteration 95, loss = 0.00860547
Iteration 96, loss = 0.00831294
Iteration 97, loss = 0.00806811
Iteration 98, loss = 0.00789215
Iteration 99, loss = 0.00771594
Iteration 100, loss = 0.00745022
Iteration 101, loss = 0.00737404
Iteration 102, loss = 0.00716168
Iteration 103, loss = 0.00698392
Iteration 104, loss = 0.00677854
Iteration 105, loss = 0.00658799
Iteration 106, loss = 0.00644667
Iteration 107, loss = 0.00627225
Iteration 108, loss = 0.00614427
Iteration 109, loss = 0.00601710
Iteration 110, loss = 0.00589290
Iteration 111, loss = 0.00572942
Iteration 112, loss = 0.00561246
Iteration 113, loss = 0.00553358
Iteration 114, loss = 0.00536737
Iteration 115, loss = 0.00520450
Iteration 116, loss = 0.00510732
Iteration 117, loss = 0.00498475
Iteration 118, loss = 0.00487786
Iteration 119, loss = 0.00477149
Iteration 120, loss = 0.00468346
Iteration 121, loss = 0.00459500
Iteration 122, loss = 0.00452778
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  80
Error rate: 25.08% ( 627/2500)
 - Class 1:  43, Class 2:  16, Class 3: 500, Class 4:  48, Class 5:  20
Iteration 1, loss = 1.25106614
Iteration 2, loss = 0.72728697
Iteration 3, loss = 0.45863393
Iteration 4, loss = 0.32757685
Iteration 5, loss = 0.25760548
Iteration 6, loss = 0.21499739
Iteration 7, loss = 0.18687237
Iteration 8, loss = 0.16686715
Iteration 9, loss = 0.15124972
Iteration 10, loss = 0.13922334
Iteration 11, loss = 0.12932703
Iteration 12, loss = 0.12125451
Iteration 13, loss = 0.11427767
Iteration 14, loss = 0.10856221
Iteration 15, loss = 0.10222005
Iteration 16, loss = 0.09762082
Iteration 17, loss = 0.09353205
Iteration 18, loss = 0.08902558
Iteration 19, loss = 0.08539599
Iteration 20, loss = 0.08191275
Iteration 21, loss = 0.07850934
Iteration 22, loss = 0.07520438
Iteration 23, loss = 0.07260897
Iteration 24, loss = 0.07010133
Iteration 25, loss = 0.06797722
Iteration 26, loss = 0.06513764
Iteration 27, loss = 0.06241917
Iteration 28, loss = 0.06037109
Iteration 29, loss = 0.05845594
Iteration 30, loss = 0.05676098
Iteration 31, loss = 0.05418409
Iteration 32, loss = 0.05244260
Iteration 33, loss = 0.05094271
Iteration 34, loss = 0.04932415
Iteration 35, loss = 0.04764804
Iteration 36, loss = 0.04589921
Iteration 37, loss = 0.04471345
Iteration 38, loss = 0.04384359
Iteration 39, loss = 0.04210124
Iteration 40, loss = 0.04108444
Iteration 41, loss = 0.03953071
Iteration 42, loss = 0.03840153
Iteration 43, loss = 0.03739690
Iteration 44, loss = 0.03597310
Iteration 45, loss = 0.03495619
Iteration 46, loss = 0.03380885
Iteration 47, loss = 0.03278551
Iteration 48, loss = 0.03204362
Iteration 49, loss = 0.03100826
Iteration 50, loss = 0.02996334
Iteration 51, loss = 0.02909728
Iteration 52, loss = 0.02820449
Iteration 53, loss = 0.02745733
Iteration 54, loss = 0.02683257
Iteration 55, loss = 0.02588867
Iteration 56, loss = 0.02519624
Iteration 57, loss = 0.02452699
Iteration 58, loss = 0.02377538
Iteration 59, loss = 0.02288411
Iteration 60, loss = 0.02220797
Iteration 61, loss = 0.02155530
Iteration 62, loss = 0.02082839
Iteration 63, loss = 0.02030720
Iteration 64, loss = 0.01989956
Iteration 65, loss = 0.01919925
Iteration 66, loss = 0.01878459
Iteration 67, loss = 0.01810823
Iteration 68, loss = 0.01744196
Iteration 69, loss = 0.01706159
Iteration 70, loss = 0.01636713
Iteration 71, loss = 0.01587166
Iteration 72, loss = 0.01536818
Iteration 73, loss = 0.01488910
Iteration 74, loss = 0.01458442
Iteration 75, loss = 0.01402144
Iteration 76, loss = 0.01353812
Iteration 77, loss = 0.01319679
Iteration 78, loss = 0.01306550
Iteration 79, loss = 0.01255648
Iteration 80, loss = 0.01209707
Iteration 81, loss = 0.01171285
Iteration 82, loss = 0.01136948
Iteration 83, loss = 0.01101632
Iteration 84, loss = 0.01068917
Iteration 85, loss = 0.01042837
Iteration 86, loss = 0.01012341
Iteration 87, loss = 0.00986634
Iteration 88, loss = 0.00955398
Iteration 89, loss = 0.00927538
Iteration 90, loss = 0.00899287
Iteration 91, loss = 0.00881606
Iteration 92, loss = 0.00853555
Iteration 93, loss = 0.00838479
Iteration 94, loss = 0.00811548
Iteration 95, loss = 0.00794392
Iteration 96, loss = 0.00767557
Iteration 97, loss = 0.00747075
Iteration 98, loss = 0.00730447
Iteration 99, loss = 0.00711212
Iteration 100, loss = 0.00692342
Iteration 101, loss = 0.00676531
Iteration 102, loss = 0.00654916
Iteration 103, loss = 0.00645440
Iteration 104, loss = 0.00626432
Iteration 105, loss = 0.00613329
Iteration 106, loss = 0.00601256
Iteration 107, loss = 0.00582167
Iteration 108, loss = 0.00569479
Iteration 109, loss = 0.00555598
Iteration 110, loss = 0.00547324
Iteration 111, loss = 0.00529211
Iteration 112, loss = 0.00516209
Iteration 113, loss = 0.00508699
Iteration 114, loss = 0.00493343
Iteration 115, loss = 0.00482727
Iteration 116, loss = 0.00472176
Iteration 117, loss = 0.00463118
Iteration 118, loss = 0.00451576
Iteration 119, loss = 0.00441293
Iteration 120, loss = 0.00433325
Iteration 121, loss = 0.00423459
Iteration 122, loss = 0.00413704
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  85
Error rate: 25.24% ( 631/2500)
 - Class 1:  49, Class 2:  20, Class 3: 489, Class 4:  39, Class 5:  34
Iteration 1, loss = 1.24946506
Iteration 2, loss = 0.71706548
Iteration 3, loss = 0.45316866
Iteration 4, loss = 0.32466868
Iteration 5, loss = 0.25544019
Iteration 6, loss = 0.21390966
Iteration 7, loss = 0.18610185
Iteration 8, loss = 0.16599290
Iteration 9, loss = 0.15112729
Iteration 10, loss = 0.13975704
Iteration 11, loss = 0.12934527
Iteration 12, loss = 0.12094081
Iteration 13, loss = 0.11417364
Iteration 14, loss = 0.10781567
Iteration 15, loss = 0.10320629
Iteration 16, loss = 0.09798589
Iteration 17, loss = 0.09313243
Iteration 18, loss = 0.08902720
Iteration 19, loss = 0.08556986
Iteration 20, loss = 0.08187657
Iteration 21, loss = 0.07839345
Iteration 22, loss = 0.07567351
Iteration 23, loss = 0.07297184
Iteration 24, loss = 0.07049701
Iteration 25, loss = 0.06760796
Iteration 26, loss = 0.06523589
Iteration 27, loss = 0.06270895
Iteration 28, loss = 0.06060241
Iteration 29, loss = 0.05854367
Iteration 30, loss = 0.05690564
Iteration 31, loss = 0.05475789
Iteration 32, loss = 0.05285745
Iteration 33, loss = 0.05139340
Iteration 34, loss = 0.04995609
Iteration 35, loss = 0.04793251
Iteration 36, loss = 0.04633982
Iteration 37, loss = 0.04492397
Iteration 38, loss = 0.04335774
Iteration 39, loss = 0.04205396
Iteration 40, loss = 0.04093875
Iteration 41, loss = 0.03950106
Iteration 42, loss = 0.03852941
Iteration 43, loss = 0.03769221
Iteration 44, loss = 0.03616757
Iteration 45, loss = 0.03483406
Iteration 46, loss = 0.03386793
Iteration 47, loss = 0.03297895
Iteration 48, loss = 0.03195930
Iteration 49, loss = 0.03099180
Iteration 50, loss = 0.03008353
Iteration 51, loss = 0.02888998
Iteration 52, loss = 0.02840242
Iteration 53, loss = 0.02751445
Iteration 54, loss = 0.02642062
Iteration 55, loss = 0.02586451
Iteration 56, loss = 0.02498615
Iteration 57, loss = 0.02423323
Iteration 58, loss = 0.02355842
Iteration 59, loss = 0.02267988
Iteration 60, loss = 0.02190828
Iteration 61, loss = 0.02132115
Iteration 62, loss = 0.02067910
Iteration 63, loss = 0.02007530
Iteration 64, loss = 0.01972248
Iteration 65, loss = 0.01883444
Iteration 66, loss = 0.01833445
Iteration 67, loss = 0.01770240
Iteration 68, loss = 0.01698428
Iteration 69, loss = 0.01657958
Iteration 70, loss = 0.01612267
Iteration 71, loss = 0.01551748
Iteration 72, loss = 0.01509213
Iteration 73, loss = 0.01478933
Iteration 74, loss = 0.01425639
Iteration 75, loss = 0.01395122
Iteration 76, loss = 0.01338207
Iteration 77, loss = 0.01299403
Iteration 78, loss = 0.01265224
Iteration 79, loss = 0.01224106
Iteration 80, loss = 0.01187122
Iteration 81, loss = 0.01150515
Iteration 82, loss = 0.01133315
Iteration 83, loss = 0.01092945
Iteration 84, loss = 0.01056525
Iteration 85, loss = 0.01026611
Iteration 86, loss = 0.01000034
Iteration 87, loss = 0.00971564
Iteration 88, loss = 0.00945278
Iteration 89, loss = 0.00912169
Iteration 90, loss = 0.00892482
Iteration 91, loss = 0.00870353
Iteration 92, loss = 0.00843880
Iteration 93, loss = 0.00821682
Iteration 94, loss = 0.00798889
Iteration 95, loss = 0.00772542
Iteration 96, loss = 0.00762276
Iteration 97, loss = 0.00756971
Iteration 98, loss = 0.00723727
Iteration 99, loss = 0.00696465
Iteration 100, loss = 0.00683376
Iteration 101, loss = 0.00668763
Iteration 102, loss = 0.00651648
Iteration 103, loss = 0.00640313
Iteration 104, loss = 0.00614557
Iteration 105, loss = 0.00599081
Iteration 106, loss = 0.00590398
Iteration 107, loss = 0.00571242
Iteration 108, loss = 0.00556945
Iteration 109, loss = 0.00545956
Iteration 110, loss = 0.00534927
Iteration 111, loss = 0.00517517
Iteration 112, loss = 0.00507984
Iteration 113, loss = 0.00496310
Iteration 114, loss = 0.00485300
Iteration 115, loss = 0.00474017
Iteration 116, loss = 0.00464283
Iteration 117, loss = 0.00452941
Iteration 118, loss = 0.00448464
Iteration 119, loss = 0.00432159
Iteration 120, loss = 0.00426394
Iteration 121, loss = 0.00415280
Iteration 122, loss = 0.00408564
Iteration 123, loss = 0.00398831
Iteration 124, loss = 0.00389660
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  90
Error rate: 25.64% ( 641/2500)
 - Class 1:  44, Class 2:  16, Class 3: 481, Class 4:  27, Class 5:  73
Iteration 1, loss = 1.26661879
Iteration 2, loss = 0.73940033
Iteration 3, loss = 0.46578192
Iteration 4, loss = 0.32836406
Iteration 5, loss = 0.25545515
Iteration 6, loss = 0.21231124
Iteration 7, loss = 0.18428760
Iteration 8, loss = 0.16362348
Iteration 9, loss = 0.14885313
Iteration 10, loss = 0.13661283
Iteration 11, loss = 0.12684728
Iteration 12, loss = 0.11902136
Iteration 13, loss = 0.11239386
Iteration 14, loss = 0.10619984
Iteration 15, loss = 0.10105934
Iteration 16, loss = 0.09578864
Iteration 17, loss = 0.09186046
Iteration 18, loss = 0.08807223
Iteration 19, loss = 0.08391381
Iteration 20, loss = 0.08065627
Iteration 21, loss = 0.07753509
Iteration 22, loss = 0.07419224
Iteration 23, loss = 0.07169890
Iteration 24, loss = 0.06920785
Iteration 25, loss = 0.06640197
Iteration 26, loss = 0.06407399
Iteration 27, loss = 0.06167408
Iteration 28, loss = 0.05992574
Iteration 29, loss = 0.05801941
Iteration 30, loss = 0.05580687
Iteration 31, loss = 0.05389187
Iteration 32, loss = 0.05196015
Iteration 33, loss = 0.05028841
Iteration 34, loss = 0.04909093
Iteration 35, loss = 0.04758147
Iteration 36, loss = 0.04587106
Iteration 37, loss = 0.04431337
Iteration 38, loss = 0.04306153
Iteration 39, loss = 0.04158558
Iteration 40, loss = 0.04035789
Iteration 41, loss = 0.03899168
Iteration 42, loss = 0.03795682
Iteration 43, loss = 0.03705376
Iteration 44, loss = 0.03558915
Iteration 45, loss = 0.03468202
Iteration 46, loss = 0.03353510
Iteration 47, loss = 0.03236296
Iteration 48, loss = 0.03176108
Iteration 49, loss = 0.03095206
Iteration 50, loss = 0.02968589
Iteration 51, loss = 0.02868599
Iteration 52, loss = 0.02796794
Iteration 53, loss = 0.02687152
Iteration 54, loss = 0.02615759
Iteration 55, loss = 0.02545476
Iteration 56, loss = 0.02472705
Iteration 57, loss = 0.02403932
Iteration 58, loss = 0.02363555
Iteration 59, loss = 0.02267877
Iteration 60, loss = 0.02186624
Iteration 61, loss = 0.02157176
Iteration 62, loss = 0.02045640
Iteration 63, loss = 0.01993509
Iteration 64, loss = 0.01937061
Iteration 65, loss = 0.01879367
Iteration 66, loss = 0.01827944
Iteration 67, loss = 0.01780886
Iteration 68, loss = 0.01712188
Iteration 69, loss = 0.01663272
Iteration 70, loss = 0.01615819
Iteration 71, loss = 0.01575681
Iteration 72, loss = 0.01514802
Iteration 73, loss = 0.01473450
Iteration 74, loss = 0.01433911
Iteration 75, loss = 0.01380335
Iteration 76, loss = 0.01333357
Iteration 77, loss = 0.01301794
Iteration 78, loss = 0.01253875
Iteration 79, loss = 0.01227660
Iteration 80, loss = 0.01185459
Iteration 81, loss = 0.01155869
Iteration 82, loss = 0.01112864
Iteration 83, loss = 0.01088744
Iteration 84, loss = 0.01056160
Iteration 85, loss = 0.01028435
Iteration 86, loss = 0.00996348
Iteration 87, loss = 0.00964447
Iteration 88, loss = 0.00938624
Iteration 89, loss = 0.00912690
Iteration 90, loss = 0.00889614
Iteration 91, loss = 0.00858392
Iteration 92, loss = 0.00838203
Iteration 93, loss = 0.00815989
Iteration 94, loss = 0.00793651
Iteration 95, loss = 0.00779729
Iteration 96, loss = 0.00757461
Iteration 97, loss = 0.00729911
Iteration 98, loss = 0.00711535
Iteration 99, loss = 0.00695445
Iteration 100, loss = 0.00672352
Iteration 101, loss = 0.00655863
Iteration 102, loss = 0.00639622
Iteration 103, loss = 0.00623620
Iteration 104, loss = 0.00611313
Iteration 105, loss = 0.00593385
Iteration 106, loss = 0.00580058
Iteration 107, loss = 0.00570950
Iteration 108, loss = 0.00558544
Iteration 109, loss = 0.00539562
Iteration 110, loss = 0.00526054
Iteration 111, loss = 0.00512823
Iteration 112, loss = 0.00503560
Iteration 113, loss = 0.00491038
Iteration 114, loss = 0.00478856
Iteration 115, loss = 0.00469225
Iteration 116, loss = 0.00461224
Iteration 117, loss = 0.00446378
Iteration 118, loss = 0.00437640
Iteration 119, loss = 0.00428166
Iteration 120, loss = 0.00419931
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  95
Error rate: 16.60% ( 415/2500)
 - Class 1:  21, Class 2:  27, Class 3: 296, Class 4:  43, Class 5:  28
Iteration 1, loss = 1.24004336
Iteration 2, loss = 0.69806529
Iteration 3, loss = 0.43513752
Iteration 4, loss = 0.31087902
Iteration 5, loss = 0.24473600
Iteration 6, loss = 0.20550429
Iteration 7, loss = 0.17916563
Iteration 8, loss = 0.16029561
Iteration 9, loss = 0.14540511
Iteration 10, loss = 0.13417258
Iteration 11, loss = 0.12563212
Iteration 12, loss = 0.11803874
Iteration 13, loss = 0.11131244
Iteration 14, loss = 0.10614606
Iteration 15, loss = 0.09993830
Iteration 16, loss = 0.09516912
Iteration 17, loss = 0.09139388
Iteration 18, loss = 0.08696044
Iteration 19, loss = 0.08352857
Iteration 20, loss = 0.08024612
Iteration 21, loss = 0.07661902
Iteration 22, loss = 0.07387987
Iteration 23, loss = 0.07147246
Iteration 24, loss = 0.06816639
Iteration 25, loss = 0.06605142
Iteration 26, loss = 0.06361625
Iteration 27, loss = 0.06187493
Iteration 28, loss = 0.05939302
Iteration 29, loss = 0.05729283
Iteration 30, loss = 0.05534412
Iteration 31, loss = 0.05320562
Iteration 32, loss = 0.05184497
Iteration 33, loss = 0.05002776
Iteration 34, loss = 0.04836054
Iteration 35, loss = 0.04698446
Iteration 36, loss = 0.04543496
Iteration 37, loss = 0.04435410
Iteration 38, loss = 0.04257736
Iteration 39, loss = 0.04133716
Iteration 40, loss = 0.03990814
Iteration 41, loss = 0.03896917
Iteration 42, loss = 0.03746696
Iteration 43, loss = 0.03644920
Iteration 44, loss = 0.03554051
Iteration 45, loss = 0.03510443
Iteration 46, loss = 0.03355845
Iteration 47, loss = 0.03300396
Iteration 48, loss = 0.03132203
Iteration 49, loss = 0.03050337
Iteration 50, loss = 0.02948978
Iteration 51, loss = 0.02878276
Iteration 52, loss = 0.02783778
Iteration 53, loss = 0.02686318
Iteration 54, loss = 0.02596078
Iteration 55, loss = 0.02527457
Iteration 56, loss = 0.02473955
Iteration 57, loss = 0.02387450
Iteration 58, loss = 0.02300048
Iteration 59, loss = 0.02222064
Iteration 60, loss = 0.02162267
Iteration 61, loss = 0.02098390
Iteration 62, loss = 0.02034131
Iteration 63, loss = 0.01985533
Iteration 64, loss = 0.01914899
Iteration 65, loss = 0.01840938
Iteration 66, loss = 0.01816045
Iteration 67, loss = 0.01763708
Iteration 68, loss = 0.01718308
Iteration 69, loss = 0.01635055
Iteration 70, loss = 0.01607304
Iteration 71, loss = 0.01544544
Iteration 72, loss = 0.01493344
Iteration 73, loss = 0.01447261
Iteration 74, loss = 0.01400133
Iteration 75, loss = 0.01352374
Iteration 76, loss = 0.01315027
Iteration 77, loss = 0.01275135
Iteration 78, loss = 0.01237335
Iteration 79, loss = 0.01212679
Iteration 80, loss = 0.01164227
Iteration 81, loss = 0.01136497
Iteration 82, loss = 0.01105026
Iteration 83, loss = 0.01068277
Iteration 84, loss = 0.01034504
Iteration 85, loss = 0.01009490
Iteration 86, loss = 0.00980674
Iteration 87, loss = 0.00951544
Iteration 88, loss = 0.00920830
Iteration 89, loss = 0.00896668
Iteration 90, loss = 0.00883406
Iteration 91, loss = 0.00852792
Iteration 92, loss = 0.00833247
Iteration 93, loss = 0.00804363
Iteration 94, loss = 0.00782259
Iteration 95, loss = 0.00762607
Iteration 96, loss = 0.00736419
Iteration 97, loss = 0.00722650
Iteration 98, loss = 0.00698593
Iteration 99, loss = 0.00694993
Iteration 100, loss = 0.00664878
Iteration 101, loss = 0.00645463
Iteration 102, loss = 0.00630922
Iteration 103, loss = 0.00613734
Iteration 104, loss = 0.00607906
Iteration 105, loss = 0.00592271
Iteration 106, loss = 0.00576714
Iteration 107, loss = 0.00554837
Iteration 108, loss = 0.00545224
Iteration 109, loss = 0.00533885
Iteration 110, loss = 0.00517505
Iteration 111, loss = 0.00505685
Iteration 112, loss = 0.00492240
Iteration 113, loss = 0.00479746
Iteration 114, loss = 0.00470557
Iteration 115, loss = 0.00459928
Iteration 116, loss = 0.00449798
Iteration 117, loss = 0.00440472
Iteration 118, loss = 0.00431731
Iteration 119, loss = 0.00424694
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted: 100
Error rate: 25.12% ( 628/2500)
 - Class 1:  32, Class 2:  25, Class 3: 500, Class 4:  43, Class 5:  28
Iteration 1, loss = 1.58492306
Iteration 2, loss = 1.47150116
Iteration 3, loss = 1.38354925
Iteration 4, loss = 1.31043659
Iteration 5, loss = 1.24784049
Iteration 6, loss = 1.19145168
Iteration 7, loss = 1.14021454
Iteration 8, loss = 1.09160635
Iteration 9, loss = 1.04508664
Iteration 10, loss = 1.00013045
Iteration 11, loss = 0.95726469
Iteration 12, loss = 0.91719097
Iteration 13, loss = 0.87925969
Iteration 14, loss = 0.84314502
Iteration 15, loss = 0.80906129
Iteration 16, loss = 0.77699446
Iteration 17, loss = 0.74686328
Iteration 18, loss = 0.71890431
Iteration 19, loss = 0.69273651
Iteration 20, loss = 0.66836376
Iteration 21, loss = 0.64555141
Iteration 22, loss = 0.62416173
Iteration 23, loss = 0.60415895
Iteration 24, loss = 0.58525510
Iteration 25, loss = 0.56735280
Iteration 26, loss = 0.55051317
Iteration 27, loss = 0.53467122
Iteration 28, loss = 0.51955490
Iteration 29, loss = 0.50530554
Iteration 30, loss = 0.49184650
Iteration 31, loss = 0.47911406
Iteration 32, loss = 0.46671490
Iteration 33, loss = 0.45515633
Iteration 34, loss = 0.44389320
Iteration 35, loss = 0.43326590
Iteration 36, loss = 0.42328998
Iteration 37, loss = 0.41351050
Iteration 38, loss = 0.40417860
Iteration 39, loss = 0.39523752
Iteration 40, loss = 0.38673269
Iteration 41, loss = 0.37844267
Iteration 42, loss = 0.37077929
Iteration 43, loss = 0.36303524
Iteration 44, loss = 0.35575259
Iteration 45, loss = 0.34868318
Iteration 46, loss = 0.34174421
Iteration 47, loss = 0.33528367
Iteration 48, loss = 0.32901005
Iteration 49, loss = 0.32281446
Iteration 50, loss = 0.31687314
Iteration 51, loss = 0.31131261
Iteration 52, loss = 0.30593989
Iteration 53, loss = 0.30032738
Iteration 54, loss = 0.29511690
Iteration 55, loss = 0.29029242
Iteration 56, loss = 0.28539736
Iteration 57, loss = 0.28052576
Iteration 58, loss = 0.27630093
Iteration 59, loss = 0.27181266
Iteration 60, loss = 0.26738839
Iteration 61, loss = 0.26322991
Iteration 62, loss = 0.25906313
Iteration 63, loss = 0.25507104
Iteration 64, loss = 0.25118924
Iteration 65, loss = 0.24747559
Iteration 66, loss = 0.24398162
Iteration 67, loss = 0.24021224
Iteration 68, loss = 0.23671934
Iteration 69, loss = 0.23339767
Iteration 70, loss = 0.23010555
Iteration 71, loss = 0.22688876
Iteration 72, loss = 0.22365277
Iteration 73, loss = 0.22074125
Iteration 74, loss = 0.21754651
Iteration 75, loss = 0.21474564
Iteration 76, loss = 0.21165425
Iteration 77, loss = 0.20896227
Iteration 78, loss = 0.20618631
Iteration 79, loss = 0.20358607
Iteration 80, loss = 0.20083282
Iteration 81, loss = 0.19828132
Iteration 82, loss = 0.19574843
Iteration 83, loss = 0.19337211
Iteration 84, loss = 0.19074053
Iteration 85, loss = 0.18845632
Iteration 86, loss = 0.18607736
Iteration 87, loss = 0.18380776
Iteration 88, loss = 0.18186004
Iteration 89, loss = 0.17944438
Iteration 90, loss = 0.17726751
Iteration 91, loss = 0.17533100
Iteration 92, loss = 0.17324191
Iteration 93, loss = 0.17107038
Iteration 94, loss = 0.16915684
Iteration 95, loss = 0.16720507
Iteration 96, loss = 0.16535494
Iteration 97, loss = 0.16328356
Iteration 98, loss = 0.16150251
Iteration 99, loss = 0.15980404
Iteration 100, loss = 0.15816276
Iteration 101, loss = 0.15622530
Iteration 102, loss = 0.15455650
Iteration 103, loss = 0.15265754
Iteration 104, loss = 0.15113937
Iteration 105, loss = 0.14965795
Iteration 106, loss = 0.14809985
Iteration 107, loss = 0.14649667
Iteration 108, loss = 0.14498675
Iteration 109, loss = 0.14351031
Iteration 110, loss = 0.14193618
Iteration 111, loss = 0.14066438
Iteration 112, loss = 0.13899102
Iteration 113, loss = 0.13775782
Iteration 114, loss = 0.13648511
Iteration 115, loss = 0.13520211
Iteration 116, loss = 0.13370820
Iteration 117, loss = 0.13238934
Iteration 118, loss = 0.13114958
Iteration 119, loss = 0.12987769
Iteration 120, loss = 0.12880233
Iteration 121, loss = 0.12738614
Iteration 122, loss = 0.12617965
Iteration 123, loss = 0.12514427
Iteration 124, loss = 0.12391169
Iteration 125, loss = 0.12262061
Iteration 126, loss = 0.12171756
Iteration 127, loss = 0.12064556
Iteration 128, loss = 0.11954630
Iteration 129, loss = 0.11843265
Iteration 130, loss = 0.11723875
Iteration 131, loss = 0.11632957
Iteration 132, loss = 0.11515453
Iteration 133, loss = 0.11420312
Iteration 134, loss = 0.11328389
Iteration 135, loss = 0.11211841
Iteration 136, loss = 0.11125839
Iteration 137, loss = 0.11028220
Iteration 138, loss = 0.10933079
Iteration 139, loss = 0.10851000
Iteration 140, loss = 0.10755250
Iteration 141, loss = 0.10663754
Iteration 142, loss = 0.10579245
Iteration 143, loss = 0.10493363
Iteration 144, loss = 0.10403906
Iteration 145, loss = 0.10324534
Iteration 146, loss = 0.10244377
Iteration 147, loss = 0.10162555
Iteration 148, loss = 0.10083083
Iteration 149, loss = 0.10002622
Iteration 150, loss = 0.09922849
Iteration 151, loss = 0.09854434
Iteration 152, loss = 0.09776828
Iteration 153, loss = 0.09715412
Iteration 154, loss = 0.09616749
Iteration 155, loss = 0.09544739
Iteration 156, loss = 0.09479164
Iteration 157, loss = 0.09407666
Iteration 158, loss = 0.09330486
Iteration 159, loss = 0.09260112
Iteration 160, loss = 0.09187484
Iteration 161, loss = 0.09117801
Iteration 162, loss = 0.09049045
Iteration 163, loss = 0.08989095
Iteration 164, loss = 0.08917113
Iteration 165, loss = 0.08852656
Iteration 166, loss = 0.08783687
Iteration 167, loss = 0.08718457
Iteration 168, loss = 0.08655197
Iteration 169, loss = 0.08589564
Iteration 170, loss = 0.08551149
Iteration 171, loss = 0.08465221
Iteration 172, loss = 0.08402744
Iteration 173, loss = 0.08349821
Iteration 174, loss = 0.08276542
Iteration 175, loss = 0.08221185
Iteration 176, loss = 0.08162316
Iteration 177, loss = 0.08098385
Iteration 178, loss = 0.08040926
Iteration 179, loss = 0.07982462
Iteration 180, loss = 0.07927453
Iteration 181, loss = 0.07851318
Iteration 182, loss = 0.07793002
Iteration 183, loss = 0.07741832
Iteration 184, loss = 0.07684644
Iteration 185, loss = 0.07639748
Iteration 186, loss = 0.07590554
Iteration 187, loss = 0.07520011
Iteration 188, loss = 0.07475436
Iteration 189, loss = 0.07424867
Iteration 190, loss = 0.07372921
Iteration 191, loss = 0.07313925
Iteration 192, loss = 0.07266725
Iteration 193, loss = 0.07225465
Iteration 194, loss = 0.07174444
Iteration 195, loss = 0.07118320
Iteration 196, loss = 0.07067204
Iteration 197, loss = 0.07019215
Iteration 198, loss = 0.06980278
Iteration 199, loss = 0.06939202
Iteration 200, loss = 0.06885357
Number of features extracted:   5
Error rate: 23.24% ( 581/2500)
 - Class 1:  14, Class 2:  14, Class 3: 500, Class 4:  21, Class 5:  32
Iteration 1, loss = 1.52082968
Iteration 2, loss = 1.32713754
Iteration 3, loss = 1.18615379
Iteration 4, loss = 1.07642526
Iteration 5, loss = 0.98452055
Iteration 6, loss = 0.90410016
Iteration 7, loss = 0.83239750
Iteration 8, loss = 0.76805330
Iteration 9, loss = 0.71073098
Iteration 10, loss = 0.65939611
Iteration 11, loss = 0.61365329
Iteration 12, loss = 0.57284078
Iteration 13, loss = 0.53547589
Iteration 14, loss = 0.50176758
Iteration 15, loss = 0.47102772
Iteration 16, loss = 0.44279090
Iteration 17, loss = 0.41716038
Iteration 18, loss = 0.39401408
Iteration 19, loss = 0.37282640
Iteration 20, loss = 0.35363503
Iteration 21, loss = 0.33600487
Iteration 22, loss = 0.32019496
Iteration 23, loss = 0.30570922
Iteration 24, loss = 0.29228534
Iteration 25, loss = 0.27954276
Iteration 26, loss = 0.26830478
Iteration 27, loss = 0.25777362
Iteration 28, loss = 0.24787602
Iteration 29, loss = 0.23870387
Iteration 30, loss = 0.23032839
Iteration 31, loss = 0.22251987
Iteration 32, loss = 0.21502423
Iteration 33, loss = 0.20797945
Iteration 34, loss = 0.20146523
Iteration 35, loss = 0.19521519
Iteration 36, loss = 0.18930929
Iteration 37, loss = 0.18385197
Iteration 38, loss = 0.17889185
Iteration 39, loss = 0.17369719
Iteration 40, loss = 0.16899063
Iteration 41, loss = 0.16441484
Iteration 42, loss = 0.16001879
Iteration 43, loss = 0.15608559
Iteration 44, loss = 0.15198596
Iteration 45, loss = 0.14811931
Iteration 46, loss = 0.14456363
Iteration 47, loss = 0.14095577
Iteration 48, loss = 0.13759231
Iteration 49, loss = 0.13408305
Iteration 50, loss = 0.13101707
Iteration 51, loss = 0.12807226
Iteration 52, loss = 0.12499057
Iteration 53, loss = 0.12200986
Iteration 54, loss = 0.11935244
Iteration 55, loss = 0.11664791
Iteration 56, loss = 0.11402664
Iteration 57, loss = 0.11179128
Iteration 58, loss = 0.10949188
Iteration 59, loss = 0.10688454
Iteration 60, loss = 0.10484521
Iteration 61, loss = 0.10264621
Iteration 62, loss = 0.10059614
Iteration 63, loss = 0.09864187
Iteration 64, loss = 0.09663752
Iteration 65, loss = 0.09480407
Iteration 66, loss = 0.09311456
Iteration 67, loss = 0.09136600
Iteration 68, loss = 0.08985078
Iteration 69, loss = 0.08813799
Iteration 70, loss = 0.08649923
Iteration 71, loss = 0.08518052
Iteration 72, loss = 0.08354610
Iteration 73, loss = 0.08197705
Iteration 74, loss = 0.08065561
Iteration 75, loss = 0.07949675
Iteration 76, loss = 0.07798357
Iteration 77, loss = 0.07669336
Iteration 78, loss = 0.07545099
Iteration 79, loss = 0.07419383
Iteration 80, loss = 0.07323366
Iteration 81, loss = 0.07185901
Iteration 82, loss = 0.07069738
Iteration 83, loss = 0.06959028
Iteration 84, loss = 0.06872908
Iteration 85, loss = 0.06759550
Iteration 86, loss = 0.06656791
Iteration 87, loss = 0.06548533
Iteration 88, loss = 0.06450777
Iteration 89, loss = 0.06359664
Iteration 90, loss = 0.06256626
Iteration 91, loss = 0.06174307
Iteration 92, loss = 0.06069468
Iteration 93, loss = 0.05988031
Iteration 94, loss = 0.05906446
Iteration 95, loss = 0.05813984
Iteration 96, loss = 0.05740717
Iteration 97, loss = 0.05654920
Iteration 98, loss = 0.05589613
Iteration 99, loss = 0.05503086
Iteration 100, loss = 0.05415628
Iteration 101, loss = 0.05338033
Iteration 102, loss = 0.05270470
Iteration 103, loss = 0.05211397
Iteration 104, loss = 0.05139592
Iteration 105, loss = 0.05062415
Iteration 106, loss = 0.04993867
Iteration 107, loss = 0.04927163
Iteration 108, loss = 0.04863616
Iteration 109, loss = 0.04802233
Iteration 110, loss = 0.04749529
Iteration 111, loss = 0.04678245
Iteration 112, loss = 0.04627130
Iteration 113, loss = 0.04558370
Iteration 114, loss = 0.04510790
Iteration 115, loss = 0.04457773
Iteration 116, loss = 0.04396971
Iteration 117, loss = 0.04344246
Iteration 118, loss = 0.04299748
Iteration 119, loss = 0.04232322
Iteration 120, loss = 0.04191382
Iteration 121, loss = 0.04144947
Iteration 122, loss = 0.04090100
Iteration 123, loss = 0.04043440
Iteration 124, loss = 0.03993590
Iteration 125, loss = 0.03942958
Iteration 126, loss = 0.03892664
Iteration 127, loss = 0.03862733
Iteration 128, loss = 0.03818351
Iteration 129, loss = 0.03777703
Iteration 130, loss = 0.03739197
Iteration 131, loss = 0.03679908
Iteration 132, loss = 0.03649913
Iteration 133, loss = 0.03602915
Iteration 134, loss = 0.03564339
Iteration 135, loss = 0.03512783
Iteration 136, loss = 0.03479206
Iteration 137, loss = 0.03442853
Iteration 138, loss = 0.03404273
Iteration 139, loss = 0.03359088
Iteration 140, loss = 0.03334930
Iteration 141, loss = 0.03302132
Iteration 142, loss = 0.03261668
Iteration 143, loss = 0.03234487
Iteration 144, loss = 0.03191093
Iteration 145, loss = 0.03153154
Iteration 146, loss = 0.03117468
Iteration 147, loss = 0.03086647
Iteration 148, loss = 0.03056480
Iteration 149, loss = 0.03018279
Iteration 150, loss = 0.02999185
Iteration 151, loss = 0.02959765
Iteration 152, loss = 0.02929172
Iteration 153, loss = 0.02896433
Iteration 154, loss = 0.02868639
Iteration 155, loss = 0.02841149
Iteration 156, loss = 0.02808369
Iteration 157, loss = 0.02784972
Iteration 158, loss = 0.02752682
Iteration 159, loss = 0.02717878
Iteration 160, loss = 0.02693119
Iteration 161, loss = 0.02678465
Iteration 162, loss = 0.02643925
Iteration 163, loss = 0.02612114
Iteration 164, loss = 0.02586746
Iteration 165, loss = 0.02558688
Iteration 166, loss = 0.02532201
Iteration 167, loss = 0.02504540
Iteration 168, loss = 0.02494649
Iteration 169, loss = 0.02457998
Iteration 170, loss = 0.02430382
Iteration 171, loss = 0.02411271
Iteration 172, loss = 0.02385851
Iteration 173, loss = 0.02360294
Iteration 174, loss = 0.02334149
Iteration 175, loss = 0.02314247
Iteration 176, loss = 0.02295441
Iteration 177, loss = 0.02262697
Iteration 178, loss = 0.02243765
Iteration 179, loss = 0.02222963
Iteration 180, loss = 0.02199521
Iteration 181, loss = 0.02177302
Iteration 182, loss = 0.02155357
Iteration 183, loss = 0.02137041
Iteration 184, loss = 0.02111724
Iteration 185, loss = 0.02089087
Iteration 186, loss = 0.02069060
Iteration 187, loss = 0.02045194
Iteration 188, loss = 0.02026346
Iteration 189, loss = 0.02005658
Iteration 190, loss = 0.01987856
Iteration 191, loss = 0.01960244
Iteration 192, loss = 0.01940764
Iteration 193, loss = 0.01920149
Iteration 194, loss = 0.01901075
Iteration 195, loss = 0.01880641
Iteration 196, loss = 0.01870018
Iteration 197, loss = 0.01845630
Iteration 198, loss = 0.01833863
Iteration 199, loss = 0.01810324
Iteration 200, loss = 0.01790308
Number of features extracted:  10
Error rate: 8.16% ( 204/2500)
 - Class 1:  27, Class 2:  18, Class 3:  72, Class 4:  57, Class 5:  30
Iteration 1, loss = 1.48576084
Iteration 2, loss = 1.25823674
Iteration 3, loss = 1.09378561
Iteration 4, loss = 0.96212107
Iteration 5, loss = 0.85122595
Iteration 6, loss = 0.75354633
Iteration 7, loss = 0.66705511
Iteration 8, loss = 0.59167900
Iteration 9, loss = 0.52699186
Iteration 10, loss = 0.47153499
Iteration 11, loss = 0.42481282
Iteration 12, loss = 0.38541892
Iteration 13, loss = 0.35258477
Iteration 14, loss = 0.32495881
Iteration 15, loss = 0.30107842
Iteration 16, loss = 0.28078904
Iteration 17, loss = 0.26287251
Iteration 18, loss = 0.24712007
Iteration 19, loss = 0.23343873
Iteration 20, loss = 0.22149930
Iteration 21, loss = 0.21101042
Iteration 22, loss = 0.20068738
Iteration 23, loss = 0.19169248
Iteration 24, loss = 0.18370258
Iteration 25, loss = 0.17648689
Iteration 26, loss = 0.16973665
Iteration 27, loss = 0.16361942
Iteration 28, loss = 0.15797525
Iteration 29, loss = 0.15256054
Iteration 30, loss = 0.14773410
Iteration 31, loss = 0.14337744
Iteration 32, loss = 0.13859829
Iteration 33, loss = 0.13472298
Iteration 34, loss = 0.13079723
Iteration 35, loss = 0.12743008
Iteration 36, loss = 0.12393284
Iteration 37, loss = 0.12061025
Iteration 38, loss = 0.11754881
Iteration 39, loss = 0.11453720
Iteration 40, loss = 0.11178086
Iteration 41, loss = 0.10903472
Iteration 42, loss = 0.10656712
Iteration 43, loss = 0.10415109
Iteration 44, loss = 0.10193745
Iteration 45, loss = 0.09930931
Iteration 46, loss = 0.09730004
Iteration 47, loss = 0.09498111
Iteration 48, loss = 0.09306966
Iteration 49, loss = 0.09124546
Iteration 50, loss = 0.08940545
Iteration 51, loss = 0.08733757
Iteration 52, loss = 0.08558737
Iteration 53, loss = 0.08404228
Iteration 54, loss = 0.08236014
Iteration 55, loss = 0.08067433
Iteration 56, loss = 0.07908575
Iteration 57, loss = 0.07758297
Iteration 58, loss = 0.07609643
Iteration 59, loss = 0.07475629
Iteration 60, loss = 0.07320839
Iteration 61, loss = 0.07217581
Iteration 62, loss = 0.07069204
Iteration 63, loss = 0.06924119
Iteration 64, loss = 0.06796533
Iteration 65, loss = 0.06696253
Iteration 66, loss = 0.06560102
Iteration 67, loss = 0.06435524
Iteration 68, loss = 0.06339772
Iteration 69, loss = 0.06223384
Iteration 70, loss = 0.06133930
Iteration 71, loss = 0.06023601
Iteration 72, loss = 0.05903258
Iteration 73, loss = 0.05821160
Iteration 74, loss = 0.05697021
Iteration 75, loss = 0.05612412
Iteration 76, loss = 0.05529986
Iteration 77, loss = 0.05439607
Iteration 78, loss = 0.05336429
Iteration 79, loss = 0.05248844
Iteration 80, loss = 0.05163704
Iteration 81, loss = 0.05074595
Iteration 82, loss = 0.04997038
Iteration 83, loss = 0.04931095
Iteration 84, loss = 0.04859750
Iteration 85, loss = 0.04762317
Iteration 86, loss = 0.04695114
Iteration 87, loss = 0.04612339
Iteration 88, loss = 0.04537764
Iteration 89, loss = 0.04482118
Iteration 90, loss = 0.04401636
Iteration 91, loss = 0.04331943
Iteration 92, loss = 0.04268501
Iteration 93, loss = 0.04210272
Iteration 94, loss = 0.04152781
Iteration 95, loss = 0.04076534
Iteration 96, loss = 0.04015483
Iteration 97, loss = 0.03952890
Iteration 98, loss = 0.03888818
Iteration 99, loss = 0.03840102
Iteration 100, loss = 0.03783549
Iteration 101, loss = 0.03724052
Iteration 102, loss = 0.03684001
Iteration 103, loss = 0.03617186
Iteration 104, loss = 0.03568593
Iteration 105, loss = 0.03509977
Iteration 106, loss = 0.03465718
Iteration 107, loss = 0.03411126
Iteration 108, loss = 0.03362775
Iteration 109, loss = 0.03322225
Iteration 110, loss = 0.03266112
Iteration 111, loss = 0.03221696
Iteration 112, loss = 0.03182062
Iteration 113, loss = 0.03128322
Iteration 114, loss = 0.03084788
Iteration 115, loss = 0.03037448
Iteration 116, loss = 0.02992862
Iteration 117, loss = 0.02959338
Iteration 118, loss = 0.02937988
Iteration 119, loss = 0.02886595
Iteration 120, loss = 0.02831723
Iteration 121, loss = 0.02801642
Iteration 122, loss = 0.02769140
Iteration 123, loss = 0.02732766
Iteration 124, loss = 0.02687487
Iteration 125, loss = 0.02654656
Iteration 126, loss = 0.02614603
Iteration 127, loss = 0.02576749
Iteration 128, loss = 0.02542770
Iteration 129, loss = 0.02502874
Iteration 130, loss = 0.02473458
Iteration 131, loss = 0.02442944
Iteration 132, loss = 0.02412041
Iteration 133, loss = 0.02372928
Iteration 134, loss = 0.02353349
Iteration 135, loss = 0.02311659
Iteration 136, loss = 0.02300048
Iteration 137, loss = 0.02261142
Iteration 138, loss = 0.02233831
Iteration 139, loss = 0.02202479
Iteration 140, loss = 0.02172617
Iteration 141, loss = 0.02145987
Iteration 142, loss = 0.02121395
Iteration 143, loss = 0.02097038
Iteration 144, loss = 0.02068637
Iteration 145, loss = 0.02043338
Iteration 146, loss = 0.02018837
Iteration 147, loss = 0.01998364
Iteration 148, loss = 0.01969690
Iteration 149, loss = 0.01953528
Iteration 150, loss = 0.01924951
Iteration 151, loss = 0.01904918
Iteration 152, loss = 0.01883015
Iteration 153, loss = 0.01862481
Iteration 154, loss = 0.01841446
Iteration 155, loss = 0.01816068
Iteration 156, loss = 0.01798966
Iteration 157, loss = 0.01779032
Iteration 158, loss = 0.01761853
Iteration 159, loss = 0.01735195
Iteration 160, loss = 0.01717877
Iteration 161, loss = 0.01699452
Iteration 162, loss = 0.01681583
Iteration 163, loss = 0.01657652
Iteration 164, loss = 0.01643376
Iteration 165, loss = 0.01620826
Iteration 166, loss = 0.01609257
Iteration 167, loss = 0.01582826
Iteration 168, loss = 0.01565363
Iteration 169, loss = 0.01548475
Iteration 170, loss = 0.01535337
Iteration 171, loss = 0.01514446
Iteration 172, loss = 0.01497296
Iteration 173, loss = 0.01480636
Iteration 174, loss = 0.01467657
Iteration 175, loss = 0.01450343
Iteration 176, loss = 0.01433654
Iteration 177, loss = 0.01417148
Iteration 178, loss = 0.01408934
Iteration 179, loss = 0.01388571
Iteration 180, loss = 0.01373946
Iteration 181, loss = 0.01359596
Iteration 182, loss = 0.01343476
Iteration 183, loss = 0.01326582
Iteration 184, loss = 0.01312789
Iteration 185, loss = 0.01301192
Iteration 186, loss = 0.01288308
Iteration 187, loss = 0.01275364
Iteration 188, loss = 0.01257364
Iteration 189, loss = 0.01245264
Iteration 190, loss = 0.01229555
Iteration 191, loss = 0.01219603
Iteration 192, loss = 0.01204885
Iteration 193, loss = 0.01191414
Iteration 194, loss = 0.01178886
Iteration 195, loss = 0.01163979
Iteration 196, loss = 0.01147432
Iteration 197, loss = 0.01135531
Iteration 198, loss = 0.01123501
Iteration 199, loss = 0.01111873
Iteration 200, loss = 0.01100722
Number of features extracted:  15
Error rate: 10.92% ( 273/2500)
 - Class 1:  31, Class 2:  19, Class 3: 158, Class 4:  44, Class 5:  21
Iteration 1, loss = 1.49117275
Iteration 2, loss = 1.19194921
Iteration 3, loss = 0.98962187
Iteration 4, loss = 0.83490039
Iteration 5, loss = 0.71024773
Iteration 6, loss = 0.60909978
Iteration 7, loss = 0.52695058
Iteration 8, loss = 0.46079412
Iteration 9, loss = 0.40720896
Iteration 10, loss = 0.36353884
Iteration 11, loss = 0.32768655
Iteration 12, loss = 0.29800289
Iteration 13, loss = 0.27345531
Iteration 14, loss = 0.25259136
Iteration 15, loss = 0.23484489
Iteration 16, loss = 0.21952958
Iteration 17, loss = 0.20618813
Iteration 18, loss = 0.19493644
Iteration 19, loss = 0.18457509
Iteration 20, loss = 0.17525896
Iteration 21, loss = 0.16721644
Iteration 22, loss = 0.15993991
Iteration 23, loss = 0.15313080
Iteration 24, loss = 0.14729571
Iteration 25, loss = 0.14179124
Iteration 26, loss = 0.13661539
Iteration 27, loss = 0.13174674
Iteration 28, loss = 0.12732827
Iteration 29, loss = 0.12333839
Iteration 30, loss = 0.11945695
Iteration 31, loss = 0.11579285
Iteration 32, loss = 0.11226174
Iteration 33, loss = 0.10900162
Iteration 34, loss = 0.10605630
Iteration 35, loss = 0.10313997
Iteration 36, loss = 0.10044951
Iteration 37, loss = 0.09768537
Iteration 38, loss = 0.09525813
Iteration 39, loss = 0.09297842
Iteration 40, loss = 0.09042620
Iteration 41, loss = 0.08819597
Iteration 42, loss = 0.08625242
Iteration 43, loss = 0.08424022
Iteration 44, loss = 0.08201486
Iteration 45, loss = 0.08023642
Iteration 46, loss = 0.07828380
Iteration 47, loss = 0.07657706
Iteration 48, loss = 0.07495311
Iteration 49, loss = 0.07336150
Iteration 50, loss = 0.07173883
Iteration 51, loss = 0.07023033
Iteration 52, loss = 0.06873244
Iteration 53, loss = 0.06718653
Iteration 54, loss = 0.06577560
Iteration 55, loss = 0.06453979
Iteration 56, loss = 0.06312839
Iteration 57, loss = 0.06186229
Iteration 58, loss = 0.06071522
Iteration 59, loss = 0.05946109
Iteration 60, loss = 0.05823024
Iteration 61, loss = 0.05705670
Iteration 62, loss = 0.05605885
Iteration 63, loss = 0.05503333
Iteration 64, loss = 0.05420555
Iteration 65, loss = 0.05315338
Iteration 66, loss = 0.05185107
Iteration 67, loss = 0.05095384
Iteration 68, loss = 0.05010814
Iteration 69, loss = 0.04905156
Iteration 70, loss = 0.04816156
Iteration 71, loss = 0.04730623
Iteration 72, loss = 0.04653517
Iteration 73, loss = 0.04557717
Iteration 74, loss = 0.04471443
Iteration 75, loss = 0.04380697
Iteration 76, loss = 0.04309689
Iteration 77, loss = 0.04228786
Iteration 78, loss = 0.04151739
Iteration 79, loss = 0.04079362
Iteration 80, loss = 0.04012034
Iteration 81, loss = 0.03933211
Iteration 82, loss = 0.03863205
Iteration 83, loss = 0.03808864
Iteration 84, loss = 0.03747778
Iteration 85, loss = 0.03674599
Iteration 86, loss = 0.03609199
Iteration 87, loss = 0.03541330
Iteration 88, loss = 0.03490891
Iteration 89, loss = 0.03427170
Iteration 90, loss = 0.03378362
Iteration 91, loss = 0.03306048
Iteration 92, loss = 0.03255526
Iteration 93, loss = 0.03199183
Iteration 94, loss = 0.03146001
Iteration 95, loss = 0.03098596
Iteration 96, loss = 0.03042253
Iteration 97, loss = 0.03007884
Iteration 98, loss = 0.02962851
Iteration 99, loss = 0.02895611
Iteration 100, loss = 0.02851603
Iteration 101, loss = 0.02804275
Iteration 102, loss = 0.02763699
Iteration 103, loss = 0.02714260
Iteration 104, loss = 0.02664647
Iteration 105, loss = 0.02624725
Iteration 106, loss = 0.02580982
Iteration 107, loss = 0.02544032
Iteration 108, loss = 0.02498112
Iteration 109, loss = 0.02465992
Iteration 110, loss = 0.02424724
Iteration 111, loss = 0.02391375
Iteration 112, loss = 0.02352519
Iteration 113, loss = 0.02311173
Iteration 114, loss = 0.02273640
Iteration 115, loss = 0.02241378
Iteration 116, loss = 0.02208539
Iteration 117, loss = 0.02186367
Iteration 118, loss = 0.02142157
Iteration 119, loss = 0.02118567
Iteration 120, loss = 0.02084489
Iteration 121, loss = 0.02056727
Iteration 122, loss = 0.02023009
Iteration 123, loss = 0.01994817
Iteration 124, loss = 0.01978291
Iteration 125, loss = 0.01940315
Iteration 126, loss = 0.01907814
Iteration 127, loss = 0.01881562
Iteration 128, loss = 0.01852391
Iteration 129, loss = 0.01819757
Iteration 130, loss = 0.01796808
Iteration 131, loss = 0.01776254
Iteration 132, loss = 0.01753831
Iteration 133, loss = 0.01724885
Iteration 134, loss = 0.01700821
Iteration 135, loss = 0.01672566
Iteration 136, loss = 0.01647404
Iteration 137, loss = 0.01625809
Iteration 138, loss = 0.01605815
Iteration 139, loss = 0.01578712
Iteration 140, loss = 0.01562530
Iteration 141, loss = 0.01537739
Iteration 142, loss = 0.01513668
Iteration 143, loss = 0.01494074
Iteration 144, loss = 0.01474746
Iteration 145, loss = 0.01452325
Iteration 146, loss = 0.01436417
Iteration 147, loss = 0.01414773
Iteration 148, loss = 0.01397975
Iteration 149, loss = 0.01374237
Iteration 150, loss = 0.01356433
Iteration 151, loss = 0.01341962
Iteration 152, loss = 0.01322175
Iteration 153, loss = 0.01305721
Iteration 154, loss = 0.01287320
Iteration 155, loss = 0.01272230
Iteration 156, loss = 0.01259112
Iteration 157, loss = 0.01236780
Iteration 158, loss = 0.01222940
Iteration 159, loss = 0.01202444
Iteration 160, loss = 0.01188924
Iteration 161, loss = 0.01172150
Iteration 162, loss = 0.01155828
Iteration 163, loss = 0.01143738
Iteration 164, loss = 0.01127137
Iteration 165, loss = 0.01114743
Iteration 166, loss = 0.01099069
Iteration 167, loss = 0.01085434
Iteration 168, loss = 0.01073537
Iteration 169, loss = 0.01057683
Iteration 170, loss = 0.01041706
Iteration 171, loss = 0.01032614
Iteration 172, loss = 0.01016219
Iteration 173, loss = 0.01005854
Iteration 174, loss = 0.00994382
Iteration 175, loss = 0.00983902
Iteration 176, loss = 0.00968319
Iteration 177, loss = 0.00955543
Iteration 178, loss = 0.00942023
Iteration 179, loss = 0.00931600
Iteration 180, loss = 0.00914744
Iteration 181, loss = 0.00901966
Iteration 182, loss = 0.00891976
Iteration 183, loss = 0.00878826
Iteration 184, loss = 0.00865279
Iteration 185, loss = 0.00856392
Iteration 186, loss = 0.00843820
Iteration 187, loss = 0.00836022
Iteration 188, loss = 0.00823462
Iteration 189, loss = 0.00814948
Iteration 190, loss = 0.00807663
Iteration 191, loss = 0.00796306
Iteration 192, loss = 0.00787503
Iteration 193, loss = 0.00779988
Iteration 194, loss = 0.00767901
Iteration 195, loss = 0.00760326
Iteration 196, loss = 0.00752135
Iteration 197, loss = 0.00743825
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  20
Error rate: 23.80% ( 595/2500)
 - Class 1:  33, Class 2:  20, Class 3: 500, Class 4:  23, Class 5:  19
Iteration 1, loss = 1.45677736
Iteration 2, loss = 1.13799633
Iteration 3, loss = 0.91184919
Iteration 4, loss = 0.74255361
Iteration 5, loss = 0.61206504
Iteration 6, loss = 0.51167381
Iteration 7, loss = 0.43318087
Iteration 8, loss = 0.37306464
Iteration 9, loss = 0.32691738
Iteration 10, loss = 0.29062504
Iteration 11, loss = 0.26204483
Iteration 12, loss = 0.23898272
Iteration 13, loss = 0.22041616
Iteration 14, loss = 0.20430801
Iteration 15, loss = 0.19112013
Iteration 16, loss = 0.17959035
Iteration 17, loss = 0.16978776
Iteration 18, loss = 0.16071250
Iteration 19, loss = 0.15321958
Iteration 20, loss = 0.14615462
Iteration 21, loss = 0.14001566
Iteration 22, loss = 0.13437303
Iteration 23, loss = 0.12914972
Iteration 24, loss = 0.12445468
Iteration 25, loss = 0.12003695
Iteration 26, loss = 0.11591724
Iteration 27, loss = 0.11214441
Iteration 28, loss = 0.10850616
Iteration 29, loss = 0.10517551
Iteration 30, loss = 0.10203469
Iteration 31, loss = 0.09906142
Iteration 32, loss = 0.09629820
Iteration 33, loss = 0.09349348
Iteration 34, loss = 0.09093353
Iteration 35, loss = 0.08855189
Iteration 36, loss = 0.08647100
Iteration 37, loss = 0.08396936
Iteration 38, loss = 0.08166787
Iteration 39, loss = 0.07964254
Iteration 40, loss = 0.07762257
Iteration 41, loss = 0.07563456
Iteration 42, loss = 0.07388740
Iteration 43, loss = 0.07208910
Iteration 44, loss = 0.07028942
Iteration 45, loss = 0.06879111
Iteration 46, loss = 0.06693085
Iteration 47, loss = 0.06551076
Iteration 48, loss = 0.06390070
Iteration 49, loss = 0.06264209
Iteration 50, loss = 0.06120628
Iteration 51, loss = 0.06008978
Iteration 52, loss = 0.05831847
Iteration 53, loss = 0.05701412
Iteration 54, loss = 0.05576875
Iteration 55, loss = 0.05469812
Iteration 56, loss = 0.05345950
Iteration 57, loss = 0.05276717
Iteration 58, loss = 0.05118196
Iteration 59, loss = 0.05021832
Iteration 60, loss = 0.04902329
Iteration 61, loss = 0.04817523
Iteration 62, loss = 0.04710427
Iteration 63, loss = 0.04612769
Iteration 64, loss = 0.04508533
Iteration 65, loss = 0.04411247
Iteration 66, loss = 0.04322696
Iteration 67, loss = 0.04237671
Iteration 68, loss = 0.04174131
Iteration 69, loss = 0.04070196
Iteration 70, loss = 0.03999783
Iteration 71, loss = 0.03915546
Iteration 72, loss = 0.03842842
Iteration 73, loss = 0.03779543
Iteration 74, loss = 0.03697387
Iteration 75, loss = 0.03618370
Iteration 76, loss = 0.03554063
Iteration 77, loss = 0.03483026
Iteration 78, loss = 0.03414625
Iteration 79, loss = 0.03343751
Iteration 80, loss = 0.03285657
Iteration 81, loss = 0.03218715
Iteration 82, loss = 0.03172951
Iteration 83, loss = 0.03093562
Iteration 84, loss = 0.03037689
Iteration 85, loss = 0.02980124
Iteration 86, loss = 0.02925616
Iteration 87, loss = 0.02866767
Iteration 88, loss = 0.02819641
Iteration 89, loss = 0.02765221
Iteration 90, loss = 0.02708107
Iteration 91, loss = 0.02657190
Iteration 92, loss = 0.02611572
Iteration 93, loss = 0.02569934
Iteration 94, loss = 0.02533171
Iteration 95, loss = 0.02465998
Iteration 96, loss = 0.02433714
Iteration 97, loss = 0.02377639
Iteration 98, loss = 0.02339053
Iteration 99, loss = 0.02310319
Iteration 100, loss = 0.02247279
Iteration 101, loss = 0.02208673
Iteration 102, loss = 0.02169258
Iteration 103, loss = 0.02129444
Iteration 104, loss = 0.02094246
Iteration 105, loss = 0.02049285
Iteration 106, loss = 0.02015090
Iteration 107, loss = 0.01978945
Iteration 108, loss = 0.01951807
Iteration 109, loss = 0.01924474
Iteration 110, loss = 0.01891179
Iteration 111, loss = 0.01852677
Iteration 112, loss = 0.01823366
Iteration 113, loss = 0.01785709
Iteration 114, loss = 0.01758830
Iteration 115, loss = 0.01727457
Iteration 116, loss = 0.01697335
Iteration 117, loss = 0.01667231
Iteration 118, loss = 0.01643119
Iteration 119, loss = 0.01617984
Iteration 120, loss = 0.01590119
Iteration 121, loss = 0.01565809
Iteration 122, loss = 0.01538786
Iteration 123, loss = 0.01515320
Iteration 124, loss = 0.01492082
Iteration 125, loss = 0.01465523
Iteration 126, loss = 0.01441600
Iteration 127, loss = 0.01421558
Iteration 128, loss = 0.01396396
Iteration 129, loss = 0.01370356
Iteration 130, loss = 0.01353154
Iteration 131, loss = 0.01332774
Iteration 132, loss = 0.01315417
Iteration 133, loss = 0.01292788
Iteration 134, loss = 0.01267101
Iteration 135, loss = 0.01252247
Iteration 136, loss = 0.01232051
Iteration 137, loss = 0.01217731
Iteration 138, loss = 0.01197961
Iteration 139, loss = 0.01176363
Iteration 140, loss = 0.01159884
Iteration 141, loss = 0.01143522
Iteration 142, loss = 0.01125130
Iteration 143, loss = 0.01107611
Iteration 144, loss = 0.01094145
Iteration 145, loss = 0.01080015
Iteration 146, loss = 0.01065565
Iteration 147, loss = 0.01046225
Iteration 148, loss = 0.01027917
Iteration 149, loss = 0.01016304
Iteration 150, loss = 0.00996726
Iteration 151, loss = 0.00980070
Iteration 152, loss = 0.00964314
Iteration 153, loss = 0.00950783
Iteration 154, loss = 0.00934789
Iteration 155, loss = 0.00917841
Iteration 156, loss = 0.00908063
Iteration 157, loss = 0.00891550
Iteration 158, loss = 0.00876863
Iteration 159, loss = 0.00861955
Iteration 160, loss = 0.00849225
Iteration 161, loss = 0.00839348
Iteration 162, loss = 0.00823994
Iteration 163, loss = 0.00811093
Iteration 164, loss = 0.00799604
Iteration 165, loss = 0.00786340
Iteration 166, loss = 0.00775474
Iteration 167, loss = 0.00763663
Iteration 168, loss = 0.00750908
Iteration 169, loss = 0.00739269
Iteration 170, loss = 0.00726945
Iteration 171, loss = 0.00717200
Iteration 172, loss = 0.00708533
Iteration 173, loss = 0.00695859
Iteration 174, loss = 0.00685576
Iteration 175, loss = 0.00673917
Iteration 176, loss = 0.00665939
Iteration 177, loss = 0.00655870
Iteration 178, loss = 0.00647523
Iteration 179, loss = 0.00636839
Iteration 180, loss = 0.00627127
Iteration 181, loss = 0.00619091
Iteration 182, loss = 0.00609547
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  25
Error rate: 24.88% ( 622/2500)
 - Class 1:  42, Class 2:  18, Class 3: 500, Class 4:  36, Class 5:  26
Iteration 1, loss = 1.40015908
Iteration 2, loss = 1.05365228
Iteration 3, loss = 0.81852223
Iteration 4, loss = 0.65092751
Iteration 5, loss = 0.52761836
Iteration 6, loss = 0.43694278
Iteration 7, loss = 0.37041789
Iteration 8, loss = 0.32085778
Iteration 9, loss = 0.28325449
Iteration 10, loss = 0.25364474
Iteration 11, loss = 0.23022006
Iteration 12, loss = 0.21155566
Iteration 13, loss = 0.19544724
Iteration 14, loss = 0.18225972
Iteration 15, loss = 0.17077489
Iteration 16, loss = 0.16094447
Iteration 17, loss = 0.15240049
Iteration 18, loss = 0.14471311
Iteration 19, loss = 0.13783527
Iteration 20, loss = 0.13146579
Iteration 21, loss = 0.12612632
Iteration 22, loss = 0.12102325
Iteration 23, loss = 0.11637742
Iteration 24, loss = 0.11191656
Iteration 25, loss = 0.10802271
Iteration 26, loss = 0.10435639
Iteration 27, loss = 0.10071689
Iteration 28, loss = 0.09776139
Iteration 29, loss = 0.09439913
Iteration 30, loss = 0.09143443
Iteration 31, loss = 0.08901138
Iteration 32, loss = 0.08623067
Iteration 33, loss = 0.08351936
Iteration 34, loss = 0.08126097
Iteration 35, loss = 0.07929287
Iteration 36, loss = 0.07676069
Iteration 37, loss = 0.07472683
Iteration 38, loss = 0.07283589
Iteration 39, loss = 0.07098089
Iteration 40, loss = 0.06902883
Iteration 41, loss = 0.06726020
Iteration 42, loss = 0.06557967
Iteration 43, loss = 0.06420472
Iteration 44, loss = 0.06250881
Iteration 45, loss = 0.06142485
Iteration 46, loss = 0.05993052
Iteration 47, loss = 0.05797748
Iteration 48, loss = 0.05679797
Iteration 49, loss = 0.05549673
Iteration 50, loss = 0.05409848
Iteration 51, loss = 0.05288753
Iteration 52, loss = 0.05169130
Iteration 53, loss = 0.05058002
Iteration 54, loss = 0.04941213
Iteration 55, loss = 0.04829035
Iteration 56, loss = 0.04749314
Iteration 57, loss = 0.04684041
Iteration 58, loss = 0.04515420
Iteration 59, loss = 0.04430100
Iteration 60, loss = 0.04348388
Iteration 61, loss = 0.04246996
Iteration 62, loss = 0.04151155
Iteration 63, loss = 0.04068032
Iteration 64, loss = 0.04000672
Iteration 65, loss = 0.03912177
Iteration 66, loss = 0.03834712
Iteration 67, loss = 0.03744995
Iteration 68, loss = 0.03685253
Iteration 69, loss = 0.03595228
Iteration 70, loss = 0.03550110
Iteration 71, loss = 0.03472694
Iteration 72, loss = 0.03392303
Iteration 73, loss = 0.03322283
Iteration 74, loss = 0.03281624
Iteration 75, loss = 0.03195372
Iteration 76, loss = 0.03129641
Iteration 77, loss = 0.03061047
Iteration 78, loss = 0.03011591
Iteration 79, loss = 0.02961237
Iteration 80, loss = 0.02896252
Iteration 81, loss = 0.02855312
Iteration 82, loss = 0.02786148
Iteration 83, loss = 0.02730968
Iteration 84, loss = 0.02675421
Iteration 85, loss = 0.02623413
Iteration 86, loss = 0.02576221
Iteration 87, loss = 0.02531457
Iteration 88, loss = 0.02479554
Iteration 89, loss = 0.02437494
Iteration 90, loss = 0.02383951
Iteration 91, loss = 0.02334068
Iteration 92, loss = 0.02285625
Iteration 93, loss = 0.02237334
Iteration 94, loss = 0.02201265
Iteration 95, loss = 0.02158396
Iteration 96, loss = 0.02115370
Iteration 97, loss = 0.02075503
Iteration 98, loss = 0.02043307
Iteration 99, loss = 0.02002989
Iteration 100, loss = 0.01964361
Iteration 101, loss = 0.01937183
Iteration 102, loss = 0.01905209
Iteration 103, loss = 0.01850242
Iteration 104, loss = 0.01817464
Iteration 105, loss = 0.01785903
Iteration 106, loss = 0.01754935
Iteration 107, loss = 0.01720227
Iteration 108, loss = 0.01694792
Iteration 109, loss = 0.01653437
Iteration 110, loss = 0.01627849
Iteration 111, loss = 0.01599578
Iteration 112, loss = 0.01568808
Iteration 113, loss = 0.01531990
Iteration 114, loss = 0.01500753
Iteration 115, loss = 0.01477300
Iteration 116, loss = 0.01454200
Iteration 117, loss = 0.01423267
Iteration 118, loss = 0.01396778
Iteration 119, loss = 0.01365613
Iteration 120, loss = 0.01345407
Iteration 121, loss = 0.01312711
Iteration 122, loss = 0.01290766
Iteration 123, loss = 0.01268210
Iteration 124, loss = 0.01247326
Iteration 125, loss = 0.01220567
Iteration 126, loss = 0.01211523
Iteration 127, loss = 0.01183239
Iteration 128, loss = 0.01156087
Iteration 129, loss = 0.01134376
Iteration 130, loss = 0.01113384
Iteration 131, loss = 0.01093438
Iteration 132, loss = 0.01077968
Iteration 133, loss = 0.01054292
Iteration 134, loss = 0.01036143
Iteration 135, loss = 0.01021920
Iteration 136, loss = 0.01001850
Iteration 137, loss = 0.00981025
Iteration 138, loss = 0.00973423
Iteration 139, loss = 0.00947508
Iteration 140, loss = 0.00930704
Iteration 141, loss = 0.00916232
Iteration 142, loss = 0.00907717
Iteration 143, loss = 0.00882233
Iteration 144, loss = 0.00873176
Iteration 145, loss = 0.00856825
Iteration 146, loss = 0.00845157
Iteration 147, loss = 0.00829996
Iteration 148, loss = 0.00814063
Iteration 149, loss = 0.00800656
Iteration 150, loss = 0.00788616
Iteration 151, loss = 0.00776081
Iteration 152, loss = 0.00762226
Iteration 153, loss = 0.00751046
Iteration 154, loss = 0.00740859
Iteration 155, loss = 0.00726757
Iteration 156, loss = 0.00716595
Iteration 157, loss = 0.00703883
Iteration 158, loss = 0.00691076
Iteration 159, loss = 0.00679599
Iteration 160, loss = 0.00670282
Iteration 161, loss = 0.00660167
Iteration 162, loss = 0.00650156
Iteration 163, loss = 0.00642675
Iteration 164, loss = 0.00631270
Iteration 165, loss = 0.00624209
Iteration 166, loss = 0.00611992
Iteration 167, loss = 0.00601646
Iteration 168, loss = 0.00594723
Iteration 169, loss = 0.00585865
Iteration 170, loss = 0.00577741
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  30
Error rate: 41.68% (1042/2500)
 - Class 1:  22, Class 2: 487, Class 3: 499, Class 4:  22, Class 5:  12
Iteration 1, loss = 1.39502490
Iteration 2, loss = 1.01966839
Iteration 3, loss = 0.76689969
Iteration 4, loss = 0.59229423
Iteration 5, loss = 0.46980004
Iteration 6, loss = 0.38395699
Iteration 7, loss = 0.32312237
Iteration 8, loss = 0.27870036
Iteration 9, loss = 0.24581761
Iteration 10, loss = 0.22061659
Iteration 11, loss = 0.20078192
Iteration 12, loss = 0.18436590
Iteration 13, loss = 0.17154735
Iteration 14, loss = 0.16036131
Iteration 15, loss = 0.15063810
Iteration 16, loss = 0.14268366
Iteration 17, loss = 0.13547311
Iteration 18, loss = 0.12909374
Iteration 19, loss = 0.12337480
Iteration 20, loss = 0.11804673
Iteration 21, loss = 0.11355867
Iteration 22, loss = 0.10936979
Iteration 23, loss = 0.10516000
Iteration 24, loss = 0.10112142
Iteration 25, loss = 0.09774603
Iteration 26, loss = 0.09443079
Iteration 27, loss = 0.09151688
Iteration 28, loss = 0.08876799
Iteration 29, loss = 0.08589257
Iteration 30, loss = 0.08326404
Iteration 31, loss = 0.08072721
Iteration 32, loss = 0.07863660
Iteration 33, loss = 0.07640776
Iteration 34, loss = 0.07423054
Iteration 35, loss = 0.07235096
Iteration 36, loss = 0.07004269
Iteration 37, loss = 0.06831833
Iteration 38, loss = 0.06637573
Iteration 39, loss = 0.06449348
Iteration 40, loss = 0.06272910
Iteration 41, loss = 0.06154594
Iteration 42, loss = 0.05998556
Iteration 43, loss = 0.05837957
Iteration 44, loss = 0.05671241
Iteration 45, loss = 0.05547231
Iteration 46, loss = 0.05388605
Iteration 47, loss = 0.05256851
Iteration 48, loss = 0.05123307
Iteration 49, loss = 0.05007026
Iteration 50, loss = 0.04898554
Iteration 51, loss = 0.04786618
Iteration 52, loss = 0.04664409
Iteration 53, loss = 0.04558034
Iteration 54, loss = 0.04454262
Iteration 55, loss = 0.04354829
Iteration 56, loss = 0.04255089
Iteration 57, loss = 0.04176379
Iteration 58, loss = 0.04058326
Iteration 59, loss = 0.03973519
Iteration 60, loss = 0.03869850
Iteration 61, loss = 0.03785722
Iteration 62, loss = 0.03718205
Iteration 63, loss = 0.03623097
Iteration 64, loss = 0.03543527
Iteration 65, loss = 0.03474072
Iteration 66, loss = 0.03395337
Iteration 67, loss = 0.03323264
Iteration 68, loss = 0.03255919
Iteration 69, loss = 0.03188038
Iteration 70, loss = 0.03121207
Iteration 71, loss = 0.03062161
Iteration 72, loss = 0.02983915
Iteration 73, loss = 0.02917644
Iteration 74, loss = 0.02854986
Iteration 75, loss = 0.02804304
Iteration 76, loss = 0.02747943
Iteration 77, loss = 0.02695026
Iteration 78, loss = 0.02625421
Iteration 79, loss = 0.02580863
Iteration 80, loss = 0.02535920
Iteration 81, loss = 0.02475704
Iteration 82, loss = 0.02431886
Iteration 83, loss = 0.02375645
Iteration 84, loss = 0.02325868
Iteration 85, loss = 0.02290392
Iteration 86, loss = 0.02226795
Iteration 87, loss = 0.02193935
Iteration 88, loss = 0.02151755
Iteration 89, loss = 0.02113347
Iteration 90, loss = 0.02056121
Iteration 91, loss = 0.02010010
Iteration 92, loss = 0.01970042
Iteration 93, loss = 0.01941863
Iteration 94, loss = 0.01894831
Iteration 95, loss = 0.01861603
Iteration 96, loss = 0.01826071
Iteration 97, loss = 0.01789767
Iteration 98, loss = 0.01747497
Iteration 99, loss = 0.01709782
Iteration 100, loss = 0.01671791
Iteration 101, loss = 0.01645627
Iteration 102, loss = 0.01615176
Iteration 103, loss = 0.01573887
Iteration 104, loss = 0.01539100
Iteration 105, loss = 0.01511646
Iteration 106, loss = 0.01483371
Iteration 107, loss = 0.01450115
Iteration 108, loss = 0.01426934
Iteration 109, loss = 0.01398240
Iteration 110, loss = 0.01368608
Iteration 111, loss = 0.01337389
Iteration 112, loss = 0.01316062
Iteration 113, loss = 0.01283645
Iteration 114, loss = 0.01265340
Iteration 115, loss = 0.01244292
Iteration 116, loss = 0.01212852
Iteration 117, loss = 0.01190074
Iteration 118, loss = 0.01168191
Iteration 119, loss = 0.01145281
Iteration 120, loss = 0.01126573
Iteration 121, loss = 0.01102867
Iteration 122, loss = 0.01080972
Iteration 123, loss = 0.01062609
Iteration 124, loss = 0.01047802
Iteration 125, loss = 0.01026438
Iteration 126, loss = 0.01007582
Iteration 127, loss = 0.00987238
Iteration 128, loss = 0.00965367
Iteration 129, loss = 0.00946878
Iteration 130, loss = 0.00927690
Iteration 131, loss = 0.00918597
Iteration 132, loss = 0.00894272
Iteration 133, loss = 0.00880020
Iteration 134, loss = 0.00863682
Iteration 135, loss = 0.00848674
Iteration 136, loss = 0.00833374
Iteration 137, loss = 0.00816443
Iteration 138, loss = 0.00800641
Iteration 139, loss = 0.00788135
Iteration 140, loss = 0.00776539
Iteration 141, loss = 0.00760025
Iteration 142, loss = 0.00746856
Iteration 143, loss = 0.00733350
Iteration 144, loss = 0.00719487
Iteration 145, loss = 0.00706853
Iteration 146, loss = 0.00697011
Iteration 147, loss = 0.00681693
Iteration 148, loss = 0.00671368
Iteration 149, loss = 0.00659428
Iteration 150, loss = 0.00646330
Iteration 151, loss = 0.00635656
Iteration 152, loss = 0.00623970
Iteration 153, loss = 0.00614100
Iteration 154, loss = 0.00606362
Iteration 155, loss = 0.00593868
Iteration 156, loss = 0.00584964
Iteration 157, loss = 0.00575546
Iteration 158, loss = 0.00563476
Iteration 159, loss = 0.00555492
Iteration 160, loss = 0.00546536
Iteration 161, loss = 0.00536015
Iteration 162, loss = 0.00528693
Iteration 163, loss = 0.00519156
Iteration 164, loss = 0.00509970
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  35
Error rate: 23.32% ( 583/2500)
 - Class 1:  27, Class 2:  16, Class 3: 500, Class 4:  28, Class 5:  12
Iteration 1, loss = 1.38739220
Iteration 2, loss = 0.98997523
Iteration 3, loss = 0.72859356
Iteration 4, loss = 0.55277848
Iteration 5, loss = 0.43502775
Iteration 6, loss = 0.35434985
Iteration 7, loss = 0.29911423
Iteration 8, loss = 0.25893619
Iteration 9, loss = 0.22886771
Iteration 10, loss = 0.20579255
Iteration 11, loss = 0.18749989
Iteration 12, loss = 0.17289330
Iteration 13, loss = 0.16093927
Iteration 14, loss = 0.15075465
Iteration 15, loss = 0.14165536
Iteration 16, loss = 0.13413856
Iteration 17, loss = 0.12754627
Iteration 18, loss = 0.12152310
Iteration 19, loss = 0.11581727
Iteration 20, loss = 0.11142910
Iteration 21, loss = 0.10684250
Iteration 22, loss = 0.10292613
Iteration 23, loss = 0.09898793
Iteration 24, loss = 0.09535252
Iteration 25, loss = 0.09207585
Iteration 26, loss = 0.08881456
Iteration 27, loss = 0.08617184
Iteration 28, loss = 0.08324314
Iteration 29, loss = 0.08053026
Iteration 30, loss = 0.07811403
Iteration 31, loss = 0.07618486
Iteration 32, loss = 0.07416477
Iteration 33, loss = 0.07155284
Iteration 34, loss = 0.06973128
Iteration 35, loss = 0.06750073
Iteration 36, loss = 0.06578418
Iteration 37, loss = 0.06387005
Iteration 38, loss = 0.06249734
Iteration 39, loss = 0.06085050
Iteration 40, loss = 0.05914134
Iteration 41, loss = 0.05737029
Iteration 42, loss = 0.05612864
Iteration 43, loss = 0.05446009
Iteration 44, loss = 0.05317363
Iteration 45, loss = 0.05181556
Iteration 46, loss = 0.05074853
Iteration 47, loss = 0.04935785
Iteration 48, loss = 0.04821932
Iteration 49, loss = 0.04727827
Iteration 50, loss = 0.04586084
Iteration 51, loss = 0.04493262
Iteration 52, loss = 0.04359091
Iteration 53, loss = 0.04256186
Iteration 54, loss = 0.04149918
Iteration 55, loss = 0.04059539
Iteration 56, loss = 0.03970478
Iteration 57, loss = 0.03875433
Iteration 58, loss = 0.03787948
Iteration 59, loss = 0.03698379
Iteration 60, loss = 0.03622953
Iteration 61, loss = 0.03521150
Iteration 62, loss = 0.03454596
Iteration 63, loss = 0.03364709
Iteration 64, loss = 0.03331203
Iteration 65, loss = 0.03209957
Iteration 66, loss = 0.03129263
Iteration 67, loss = 0.03078917
Iteration 68, loss = 0.02997183
Iteration 69, loss = 0.02927570
Iteration 70, loss = 0.02858685
Iteration 71, loss = 0.02794496
Iteration 72, loss = 0.02739503
Iteration 73, loss = 0.02670467
Iteration 74, loss = 0.02619222
Iteration 75, loss = 0.02567253
Iteration 76, loss = 0.02506148
Iteration 77, loss = 0.02476910
Iteration 78, loss = 0.02376483
Iteration 79, loss = 0.02317194
Iteration 80, loss = 0.02264169
Iteration 81, loss = 0.02224340
Iteration 82, loss = 0.02164110
Iteration 83, loss = 0.02123213
Iteration 84, loss = 0.02087903
Iteration 85, loss = 0.02022419
Iteration 86, loss = 0.01972506
Iteration 87, loss = 0.01932829
Iteration 88, loss = 0.01889621
Iteration 89, loss = 0.01852768
Iteration 90, loss = 0.01805832
Iteration 91, loss = 0.01772887
Iteration 92, loss = 0.01723519
Iteration 93, loss = 0.01689911
Iteration 94, loss = 0.01657454
Iteration 95, loss = 0.01619550
Iteration 96, loss = 0.01585147
Iteration 97, loss = 0.01546415
Iteration 98, loss = 0.01519941
Iteration 99, loss = 0.01484105
Iteration 100, loss = 0.01444761
Iteration 101, loss = 0.01424990
Iteration 102, loss = 0.01387426
Iteration 103, loss = 0.01354236
Iteration 104, loss = 0.01328296
Iteration 105, loss = 0.01295994
Iteration 106, loss = 0.01274743
Iteration 107, loss = 0.01238523
Iteration 108, loss = 0.01216358
Iteration 109, loss = 0.01189978
Iteration 110, loss = 0.01171111
Iteration 111, loss = 0.01150112
Iteration 112, loss = 0.01130924
Iteration 113, loss = 0.01108240
Iteration 114, loss = 0.01080188
Iteration 115, loss = 0.01055981
Iteration 116, loss = 0.01034362
Iteration 117, loss = 0.01016646
Iteration 118, loss = 0.00993573
Iteration 119, loss = 0.00975378
Iteration 120, loss = 0.00952516
Iteration 121, loss = 0.00939850
Iteration 122, loss = 0.00925074
Iteration 123, loss = 0.00904317
Iteration 124, loss = 0.00886949
Iteration 125, loss = 0.00870954
Iteration 126, loss = 0.00852703
Iteration 127, loss = 0.00836738
Iteration 128, loss = 0.00819179
Iteration 129, loss = 0.00807116
Iteration 130, loss = 0.00791590
Iteration 131, loss = 0.00777819
Iteration 132, loss = 0.00760770
Iteration 133, loss = 0.00747901
Iteration 134, loss = 0.00737196
Iteration 135, loss = 0.00721383
Iteration 136, loss = 0.00709371
Iteration 137, loss = 0.00697189
Iteration 138, loss = 0.00687136
Iteration 139, loss = 0.00674024
Iteration 140, loss = 0.00662107
Iteration 141, loss = 0.00648640
Iteration 142, loss = 0.00637933
Iteration 143, loss = 0.00626257
Iteration 144, loss = 0.00614672
Iteration 145, loss = 0.00607158
Iteration 146, loss = 0.00594196
Iteration 147, loss = 0.00581624
Iteration 148, loss = 0.00568544
Iteration 149, loss = 0.00559263
Iteration 150, loss = 0.00543715
Iteration 151, loss = 0.00532236
Iteration 152, loss = 0.00522830
Iteration 153, loss = 0.00512864
Iteration 154, loss = 0.00506242
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  40
Error rate: 24.48% ( 612/2500)
 - Class 1:  33, Class 2:  16, Class 3: 500, Class 4:  33, Class 5:  30
Iteration 1, loss = 1.32857899
Iteration 2, loss = 0.90410908
Iteration 3, loss = 0.64712551
Iteration 4, loss = 0.48470398
Iteration 5, loss = 0.38018295
Iteration 6, loss = 0.31214283
Iteration 7, loss = 0.26538698
Iteration 8, loss = 0.23235196
Iteration 9, loss = 0.20721056
Iteration 10, loss = 0.18802215
Iteration 11, loss = 0.17247748
Iteration 12, loss = 0.16024027
Iteration 13, loss = 0.14957642
Iteration 14, loss = 0.14061989
Iteration 15, loss = 0.13341403
Iteration 16, loss = 0.12619113
Iteration 17, loss = 0.12079661
Iteration 18, loss = 0.11550304
Iteration 19, loss = 0.11024870
Iteration 20, loss = 0.10548156
Iteration 21, loss = 0.10141589
Iteration 22, loss = 0.09769561
Iteration 23, loss = 0.09432706
Iteration 24, loss = 0.09066519
Iteration 25, loss = 0.08777342
Iteration 26, loss = 0.08460411
Iteration 27, loss = 0.08172205
Iteration 28, loss = 0.07939820
Iteration 29, loss = 0.07640141
Iteration 30, loss = 0.07408922
Iteration 31, loss = 0.07183753
Iteration 32, loss = 0.07013143
Iteration 33, loss = 0.06770895
Iteration 34, loss = 0.06599012
Iteration 35, loss = 0.06438830
Iteration 36, loss = 0.06189691
Iteration 37, loss = 0.06018069
Iteration 38, loss = 0.05843478
Iteration 39, loss = 0.05703065
Iteration 40, loss = 0.05550631
Iteration 41, loss = 0.05370279
Iteration 42, loss = 0.05241015
Iteration 43, loss = 0.05072849
Iteration 44, loss = 0.04925897
Iteration 45, loss = 0.04812479
Iteration 46, loss = 0.04691298
Iteration 47, loss = 0.04563278
Iteration 48, loss = 0.04443093
Iteration 49, loss = 0.04313054
Iteration 50, loss = 0.04225802
Iteration 51, loss = 0.04118996
Iteration 52, loss = 0.03993094
Iteration 53, loss = 0.03913107
Iteration 54, loss = 0.03799071
Iteration 55, loss = 0.03708762
Iteration 56, loss = 0.03614895
Iteration 57, loss = 0.03517209
Iteration 58, loss = 0.03437565
Iteration 59, loss = 0.03338907
Iteration 60, loss = 0.03273376
Iteration 61, loss = 0.03207126
Iteration 62, loss = 0.03108272
Iteration 63, loss = 0.03020493
Iteration 64, loss = 0.02961600
Iteration 65, loss = 0.02896425
Iteration 66, loss = 0.02820531
Iteration 67, loss = 0.02748438
Iteration 68, loss = 0.02688468
Iteration 69, loss = 0.02640340
Iteration 70, loss = 0.02585875
Iteration 71, loss = 0.02498750
Iteration 72, loss = 0.02469521
Iteration 73, loss = 0.02380147
Iteration 74, loss = 0.02325646
Iteration 75, loss = 0.02270760
Iteration 76, loss = 0.02208333
Iteration 77, loss = 0.02171097
Iteration 78, loss = 0.02102104
Iteration 79, loss = 0.02053961
Iteration 80, loss = 0.02004668
Iteration 81, loss = 0.01952936
Iteration 82, loss = 0.01904625
Iteration 83, loss = 0.01869712
Iteration 84, loss = 0.01824101
Iteration 85, loss = 0.01778787
Iteration 86, loss = 0.01735082
Iteration 87, loss = 0.01698537
Iteration 88, loss = 0.01658083
Iteration 89, loss = 0.01615462
Iteration 90, loss = 0.01583680
Iteration 91, loss = 0.01537801
Iteration 92, loss = 0.01503550
Iteration 93, loss = 0.01461304
Iteration 94, loss = 0.01431568
Iteration 95, loss = 0.01400062
Iteration 96, loss = 0.01368210
Iteration 97, loss = 0.01332598
Iteration 98, loss = 0.01300936
Iteration 99, loss = 0.01273754
Iteration 100, loss = 0.01246921
Iteration 101, loss = 0.01217890
Iteration 102, loss = 0.01190790
Iteration 103, loss = 0.01165247
Iteration 104, loss = 0.01145029
Iteration 105, loss = 0.01106454
Iteration 106, loss = 0.01085289
Iteration 107, loss = 0.01061273
Iteration 108, loss = 0.01036560
Iteration 109, loss = 0.01010058
Iteration 110, loss = 0.00988769
Iteration 111, loss = 0.00967199
Iteration 112, loss = 0.00942502
Iteration 113, loss = 0.00924157
Iteration 114, loss = 0.00903487
Iteration 115, loss = 0.00882528
Iteration 116, loss = 0.00863595
Iteration 117, loss = 0.00844025
Iteration 118, loss = 0.00824650
Iteration 119, loss = 0.00807461
Iteration 120, loss = 0.00787046
Iteration 121, loss = 0.00773246
Iteration 122, loss = 0.00762289
Iteration 123, loss = 0.00741668
Iteration 124, loss = 0.00724764
Iteration 125, loss = 0.00714334
Iteration 126, loss = 0.00696585
Iteration 127, loss = 0.00682410
Iteration 128, loss = 0.00668723
Iteration 129, loss = 0.00654190
Iteration 130, loss = 0.00643465
Iteration 131, loss = 0.00633738
Iteration 132, loss = 0.00618496
Iteration 133, loss = 0.00612337
Iteration 134, loss = 0.00596981
Iteration 135, loss = 0.00583994
Iteration 136, loss = 0.00569114
Iteration 137, loss = 0.00559915
Iteration 138, loss = 0.00549536
Iteration 139, loss = 0.00537234
Iteration 140, loss = 0.00530897
Iteration 141, loss = 0.00522258
Iteration 142, loss = 0.00508732
Iteration 143, loss = 0.00501557
Iteration 144, loss = 0.00492467
Iteration 145, loss = 0.00479965
Iteration 146, loss = 0.00473022
Iteration 147, loss = 0.00465130
Iteration 148, loss = 0.00457974
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  45
Error rate: 24.48% ( 612/2500)
 - Class 1:  29, Class 2:  18, Class 3: 499, Class 4:  24, Class 5:  42
Iteration 1, loss = 1.35034811
Iteration 2, loss = 0.92747242
Iteration 3, loss = 0.65964767
Iteration 4, loss = 0.48935566
Iteration 5, loss = 0.38043485
Iteration 6, loss = 0.30941816
Iteration 7, loss = 0.26086539
Iteration 8, loss = 0.22705953
Iteration 9, loss = 0.20199476
Iteration 10, loss = 0.18240636
Iteration 11, loss = 0.16688437
Iteration 12, loss = 0.15443268
Iteration 13, loss = 0.14399898
Iteration 14, loss = 0.13517288
Iteration 15, loss = 0.12800298
Iteration 16, loss = 0.12126337
Iteration 17, loss = 0.11539078
Iteration 18, loss = 0.11005974
Iteration 19, loss = 0.10537775
Iteration 20, loss = 0.10103725
Iteration 21, loss = 0.09685275
Iteration 22, loss = 0.09309962
Iteration 23, loss = 0.08975005
Iteration 24, loss = 0.08679118
Iteration 25, loss = 0.08346307
Iteration 26, loss = 0.08058548
Iteration 27, loss = 0.07789474
Iteration 28, loss = 0.07507865
Iteration 29, loss = 0.07295233
Iteration 30, loss = 0.07082654
Iteration 31, loss = 0.06843978
Iteration 32, loss = 0.06604914
Iteration 33, loss = 0.06431239
Iteration 34, loss = 0.06251914
Iteration 35, loss = 0.06039790
Iteration 36, loss = 0.05889424
Iteration 37, loss = 0.05716475
Iteration 38, loss = 0.05551898
Iteration 39, loss = 0.05390508
Iteration 40, loss = 0.05239934
Iteration 41, loss = 0.05101910
Iteration 42, loss = 0.04957134
Iteration 43, loss = 0.04809529
Iteration 44, loss = 0.04705222
Iteration 45, loss = 0.04581730
Iteration 46, loss = 0.04452412
Iteration 47, loss = 0.04324638
Iteration 48, loss = 0.04197648
Iteration 49, loss = 0.04119868
Iteration 50, loss = 0.04011468
Iteration 51, loss = 0.03885148
Iteration 52, loss = 0.03802811
Iteration 53, loss = 0.03715591
Iteration 54, loss = 0.03622873
Iteration 55, loss = 0.03533772
Iteration 56, loss = 0.03419237
Iteration 57, loss = 0.03326694
Iteration 58, loss = 0.03259736
Iteration 59, loss = 0.03180686
Iteration 60, loss = 0.03083106
Iteration 61, loss = 0.02995758
Iteration 62, loss = 0.02959110
Iteration 63, loss = 0.02872081
Iteration 64, loss = 0.02781011
Iteration 65, loss = 0.02713069
Iteration 66, loss = 0.02650451
Iteration 67, loss = 0.02583938
Iteration 68, loss = 0.02510290
Iteration 69, loss = 0.02447473
Iteration 70, loss = 0.02381748
Iteration 71, loss = 0.02336046
Iteration 72, loss = 0.02271146
Iteration 73, loss = 0.02228141
Iteration 74, loss = 0.02172040
Iteration 75, loss = 0.02112908
Iteration 76, loss = 0.02063245
Iteration 77, loss = 0.02008490
Iteration 78, loss = 0.01960214
Iteration 79, loss = 0.01909838
Iteration 80, loss = 0.01869698
Iteration 81, loss = 0.01828773
Iteration 82, loss = 0.01774532
Iteration 83, loss = 0.01740182
Iteration 84, loss = 0.01695936
Iteration 85, loss = 0.01666249
Iteration 86, loss = 0.01610890
Iteration 87, loss = 0.01567667
Iteration 88, loss = 0.01532746
Iteration 89, loss = 0.01500928
Iteration 90, loss = 0.01468092
Iteration 91, loss = 0.01430467
Iteration 92, loss = 0.01401978
Iteration 93, loss = 0.01359068
Iteration 94, loss = 0.01328709
Iteration 95, loss = 0.01299556
Iteration 96, loss = 0.01271264
Iteration 97, loss = 0.01239813
Iteration 98, loss = 0.01211031
Iteration 99, loss = 0.01181117
Iteration 100, loss = 0.01154934
Iteration 101, loss = 0.01135555
Iteration 102, loss = 0.01109299
Iteration 103, loss = 0.01078994
Iteration 104, loss = 0.01053757
Iteration 105, loss = 0.01030573
Iteration 106, loss = 0.01016113
Iteration 107, loss = 0.00983704
Iteration 108, loss = 0.00965375
Iteration 109, loss = 0.00940492
Iteration 110, loss = 0.00919094
Iteration 111, loss = 0.00897791
Iteration 112, loss = 0.00879508
Iteration 113, loss = 0.00864187
Iteration 114, loss = 0.00850089
Iteration 115, loss = 0.00831086
Iteration 116, loss = 0.00811859
Iteration 117, loss = 0.00791807
Iteration 118, loss = 0.00774124
Iteration 119, loss = 0.00758055
Iteration 120, loss = 0.00744576
Iteration 121, loss = 0.00726059
Iteration 122, loss = 0.00712905
Iteration 123, loss = 0.00697985
Iteration 124, loss = 0.00681862
Iteration 125, loss = 0.00672366
Iteration 126, loss = 0.00653769
Iteration 127, loss = 0.00640493
Iteration 128, loss = 0.00630388
Iteration 129, loss = 0.00615685
Iteration 130, loss = 0.00603740
Iteration 131, loss = 0.00595030
Iteration 132, loss = 0.00581397
Iteration 133, loss = 0.00569522
Iteration 134, loss = 0.00557793
Iteration 135, loss = 0.00547016
Iteration 136, loss = 0.00536761
Iteration 137, loss = 0.00525588
Iteration 138, loss = 0.00517606
Iteration 139, loss = 0.00508603
Iteration 140, loss = 0.00495724
Iteration 141, loss = 0.00488261
Iteration 142, loss = 0.00479528
Iteration 143, loss = 0.00469044
Iteration 144, loss = 0.00461543
Iteration 145, loss = 0.00453354
Iteration 146, loss = 0.00443444
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  50
Error rate: 25.24% ( 631/2500)
 - Class 1:  17, Class 2:  21, Class 3:  60, Class 4: 499, Class 5:  34
Iteration 1, loss = 1.38198909
Iteration 2, loss = 0.93073609
Iteration 3, loss = 0.65387803
Iteration 4, loss = 0.47952754
Iteration 5, loss = 0.37175914
Iteration 6, loss = 0.30222156
Iteration 7, loss = 0.25559304
Iteration 8, loss = 0.22260303
Iteration 9, loss = 0.19817061
Iteration 10, loss = 0.17964389
Iteration 11, loss = 0.16443884
Iteration 12, loss = 0.15248153
Iteration 13, loss = 0.14261914
Iteration 14, loss = 0.13377347
Iteration 15, loss = 0.12656506
Iteration 16, loss = 0.12024409
Iteration 17, loss = 0.11439386
Iteration 18, loss = 0.10915371
Iteration 19, loss = 0.10461847
Iteration 20, loss = 0.10014263
Iteration 21, loss = 0.09630228
Iteration 22, loss = 0.09277024
Iteration 23, loss = 0.08917781
Iteration 24, loss = 0.08639281
Iteration 25, loss = 0.08311944
Iteration 26, loss = 0.08036219
Iteration 27, loss = 0.07802316
Iteration 28, loss = 0.07537784
Iteration 29, loss = 0.07305949
Iteration 30, loss = 0.07083061
Iteration 31, loss = 0.06839426
Iteration 32, loss = 0.06631364
Iteration 33, loss = 0.06430160
Iteration 34, loss = 0.06244989
Iteration 35, loss = 0.06056352
Iteration 36, loss = 0.05887352
Iteration 37, loss = 0.05736623
Iteration 38, loss = 0.05566545
Iteration 39, loss = 0.05468648
Iteration 40, loss = 0.05285729
Iteration 41, loss = 0.05114935
Iteration 42, loss = 0.04967929
Iteration 43, loss = 0.04829020
Iteration 44, loss = 0.04717780
Iteration 45, loss = 0.04584853
Iteration 46, loss = 0.04465239
Iteration 47, loss = 0.04338005
Iteration 48, loss = 0.04230559
Iteration 49, loss = 0.04118000
Iteration 50, loss = 0.04013774
Iteration 51, loss = 0.03925883
Iteration 52, loss = 0.03823811
Iteration 53, loss = 0.03717009
Iteration 54, loss = 0.03644411
Iteration 55, loss = 0.03568490
Iteration 56, loss = 0.03455011
Iteration 57, loss = 0.03358980
Iteration 58, loss = 0.03298380
Iteration 59, loss = 0.03196576
Iteration 60, loss = 0.03116559
Iteration 61, loss = 0.03038544
Iteration 62, loss = 0.02975252
Iteration 63, loss = 0.02899505
Iteration 64, loss = 0.02817855
Iteration 65, loss = 0.02755012
Iteration 66, loss = 0.02696568
Iteration 67, loss = 0.02621872
Iteration 68, loss = 0.02573476
Iteration 69, loss = 0.02517054
Iteration 70, loss = 0.02440090
Iteration 71, loss = 0.02385496
Iteration 72, loss = 0.02323053
Iteration 73, loss = 0.02271477
Iteration 74, loss = 0.02209637
Iteration 75, loss = 0.02154266
Iteration 76, loss = 0.02100755
Iteration 77, loss = 0.02060820
Iteration 78, loss = 0.02009086
Iteration 79, loss = 0.01964738
Iteration 80, loss = 0.01939371
Iteration 81, loss = 0.01884855
Iteration 82, loss = 0.01824684
Iteration 83, loss = 0.01784877
Iteration 84, loss = 0.01732936
Iteration 85, loss = 0.01692113
Iteration 86, loss = 0.01647632
Iteration 87, loss = 0.01607948
Iteration 88, loss = 0.01572907
Iteration 89, loss = 0.01530793
Iteration 90, loss = 0.01504833
Iteration 91, loss = 0.01466267
Iteration 92, loss = 0.01444701
Iteration 93, loss = 0.01392723
Iteration 94, loss = 0.01364798
Iteration 95, loss = 0.01329348
Iteration 96, loss = 0.01300225
Iteration 97, loss = 0.01274070
Iteration 98, loss = 0.01254399
Iteration 99, loss = 0.01215414
Iteration 100, loss = 0.01192331
Iteration 101, loss = 0.01174744
Iteration 102, loss = 0.01131883
Iteration 103, loss = 0.01104088
Iteration 104, loss = 0.01089038
Iteration 105, loss = 0.01064072
Iteration 106, loss = 0.01031255
Iteration 107, loss = 0.01013968
Iteration 108, loss = 0.00983111
Iteration 109, loss = 0.00969557
Iteration 110, loss = 0.00950369
Iteration 111, loss = 0.00918078
Iteration 112, loss = 0.00903528
Iteration 113, loss = 0.00882997
Iteration 114, loss = 0.00862622
Iteration 115, loss = 0.00841939
Iteration 116, loss = 0.00827243
Iteration 117, loss = 0.00805830
Iteration 118, loss = 0.00793550
Iteration 119, loss = 0.00781997
Iteration 120, loss = 0.00757319
Iteration 121, loss = 0.00743619
Iteration 122, loss = 0.00728591
Iteration 123, loss = 0.00718002
Iteration 124, loss = 0.00700303
Iteration 125, loss = 0.00691680
Iteration 126, loss = 0.00670663
Iteration 127, loss = 0.00659334
Iteration 128, loss = 0.00648415
Iteration 129, loss = 0.00631174
Iteration 130, loss = 0.00618694
Iteration 131, loss = 0.00609078
Iteration 132, loss = 0.00594738
Iteration 133, loss = 0.00585045
Iteration 134, loss = 0.00574215
Iteration 135, loss = 0.00563464
Iteration 136, loss = 0.00553635
Iteration 137, loss = 0.00543075
Iteration 138, loss = 0.00529604
Iteration 139, loss = 0.00522910
Iteration 140, loss = 0.00515907
Iteration 141, loss = 0.00502624
Iteration 142, loss = 0.00494897
Iteration 143, loss = 0.00485438
Iteration 144, loss = 0.00479253
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  55
Error rate: 12.56% ( 314/2500)
 - Class 1:  16, Class 2:  12, Class 3: 118, Class 4:  66, Class 5: 102
Iteration 1, loss = 1.32421492
Iteration 2, loss = 0.85232222
Iteration 3, loss = 0.57166912
Iteration 4, loss = 0.41211873
Iteration 5, loss = 0.32023038
Iteration 6, loss = 0.26312349
Iteration 7, loss = 0.22539148
Iteration 8, loss = 0.19813528
Iteration 9, loss = 0.17816479
Iteration 10, loss = 0.16264689
Iteration 11, loss = 0.15037453
Iteration 12, loss = 0.14027349
Iteration 13, loss = 0.13190595
Iteration 14, loss = 0.12433377
Iteration 15, loss = 0.11789982
Iteration 16, loss = 0.11232642
Iteration 17, loss = 0.10745619
Iteration 18, loss = 0.10279867
Iteration 19, loss = 0.09906156
Iteration 20, loss = 0.09481287
Iteration 21, loss = 0.09079582
Iteration 22, loss = 0.08767620
Iteration 23, loss = 0.08448502
Iteration 24, loss = 0.08146617
Iteration 25, loss = 0.07885484
Iteration 26, loss = 0.07598576
Iteration 27, loss = 0.07344560
Iteration 28, loss = 0.07102206
Iteration 29, loss = 0.06915795
Iteration 30, loss = 0.06690286
Iteration 31, loss = 0.06448939
Iteration 32, loss = 0.06295194
Iteration 33, loss = 0.06113968
Iteration 34, loss = 0.05891235
Iteration 35, loss = 0.05727434
Iteration 36, loss = 0.05562460
Iteration 37, loss = 0.05431133
Iteration 38, loss = 0.05245477
Iteration 39, loss = 0.05110930
Iteration 40, loss = 0.05023996
Iteration 41, loss = 0.04834472
Iteration 42, loss = 0.04731474
Iteration 43, loss = 0.04649117
Iteration 44, loss = 0.04474421
Iteration 45, loss = 0.04322681
Iteration 46, loss = 0.04206775
Iteration 47, loss = 0.04093600
Iteration 48, loss = 0.04009078
Iteration 49, loss = 0.03878183
Iteration 50, loss = 0.03792112
Iteration 51, loss = 0.03692135
Iteration 52, loss = 0.03582348
Iteration 53, loss = 0.03542654
Iteration 54, loss = 0.03406408
Iteration 55, loss = 0.03316094
Iteration 56, loss = 0.03242429
Iteration 57, loss = 0.03151408
Iteration 58, loss = 0.03065739
Iteration 59, loss = 0.02983137
Iteration 60, loss = 0.02934343
Iteration 61, loss = 0.02876855
Iteration 62, loss = 0.02780457
Iteration 63, loss = 0.02698673
Iteration 64, loss = 0.02627687
Iteration 65, loss = 0.02562358
Iteration 66, loss = 0.02512128
Iteration 67, loss = 0.02421104
Iteration 68, loss = 0.02364691
Iteration 69, loss = 0.02301726
Iteration 70, loss = 0.02245829
Iteration 71, loss = 0.02203816
Iteration 72, loss = 0.02136706
Iteration 73, loss = 0.02079380
Iteration 74, loss = 0.02041242
Iteration 75, loss = 0.01979715
Iteration 76, loss = 0.01925694
Iteration 77, loss = 0.01877411
Iteration 78, loss = 0.01817400
Iteration 79, loss = 0.01779800
Iteration 80, loss = 0.01726461
Iteration 81, loss = 0.01691405
Iteration 82, loss = 0.01640814
Iteration 83, loss = 0.01605057
Iteration 84, loss = 0.01560764
Iteration 85, loss = 0.01518207
Iteration 86, loss = 0.01478933
Iteration 87, loss = 0.01444416
Iteration 88, loss = 0.01408487
Iteration 89, loss = 0.01377208
Iteration 90, loss = 0.01341038
Iteration 91, loss = 0.01303711
Iteration 92, loss = 0.01275774
Iteration 93, loss = 0.01243120
Iteration 94, loss = 0.01205538
Iteration 95, loss = 0.01169950
Iteration 96, loss = 0.01155573
Iteration 97, loss = 0.01122867
Iteration 98, loss = 0.01082841
Iteration 99, loss = 0.01062519
Iteration 100, loss = 0.01035630
Iteration 101, loss = 0.01005510
Iteration 102, loss = 0.00987034
Iteration 103, loss = 0.00959987
Iteration 104, loss = 0.00941048
Iteration 105, loss = 0.00915887
Iteration 106, loss = 0.00889207
Iteration 107, loss = 0.00874955
Iteration 108, loss = 0.00844480
Iteration 109, loss = 0.00822629
Iteration 110, loss = 0.00806188
Iteration 111, loss = 0.00785199
Iteration 112, loss = 0.00774661
Iteration 113, loss = 0.00753729
Iteration 114, loss = 0.00733997
Iteration 115, loss = 0.00716434
Iteration 116, loss = 0.00702374
Iteration 117, loss = 0.00680014
Iteration 118, loss = 0.00667581
Iteration 119, loss = 0.00651287
Iteration 120, loss = 0.00645483
Iteration 121, loss = 0.00625332
Iteration 122, loss = 0.00609425
Iteration 123, loss = 0.00595373
Iteration 124, loss = 0.00584120
Iteration 125, loss = 0.00574350
Iteration 126, loss = 0.00556462
Iteration 127, loss = 0.00547026
Iteration 128, loss = 0.00536242
Iteration 129, loss = 0.00525539
Iteration 130, loss = 0.00513477
Iteration 131, loss = 0.00501578
Iteration 132, loss = 0.00493174
Iteration 133, loss = 0.00484511
Iteration 134, loss = 0.00475904
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  60
Error rate: 25.24% ( 631/2500)
 - Class 1:  44, Class 2:  16, Class 3: 500, Class 4:  32, Class 5:  39
Iteration 1, loss = 1.30657314
Iteration 2, loss = 0.82833301
Iteration 3, loss = 0.55166223
Iteration 4, loss = 0.39556708
Iteration 5, loss = 0.30604779
Iteration 6, loss = 0.25121984
Iteration 7, loss = 0.21495155
Iteration 8, loss = 0.18944718
Iteration 9, loss = 0.17059424
Iteration 10, loss = 0.15584802
Iteration 11, loss = 0.14376432
Iteration 12, loss = 0.13425163
Iteration 13, loss = 0.12599716
Iteration 14, loss = 0.11928925
Iteration 15, loss = 0.11278714
Iteration 16, loss = 0.10740117
Iteration 17, loss = 0.10224378
Iteration 18, loss = 0.09830909
Iteration 19, loss = 0.09415412
Iteration 20, loss = 0.09005984
Iteration 21, loss = 0.08643726
Iteration 22, loss = 0.08333061
Iteration 23, loss = 0.08068745
Iteration 24, loss = 0.07770153
Iteration 25, loss = 0.07476973
Iteration 26, loss = 0.07215073
Iteration 27, loss = 0.07029456
Iteration 28, loss = 0.06734634
Iteration 29, loss = 0.06507454
Iteration 30, loss = 0.06306046
Iteration 31, loss = 0.06125761
Iteration 32, loss = 0.05924285
Iteration 33, loss = 0.05741973
Iteration 34, loss = 0.05561760
Iteration 35, loss = 0.05382065
Iteration 36, loss = 0.05270194
Iteration 37, loss = 0.05076399
Iteration 38, loss = 0.04959287
Iteration 39, loss = 0.04825476
Iteration 40, loss = 0.04673658
Iteration 41, loss = 0.04525229
Iteration 42, loss = 0.04380210
Iteration 43, loss = 0.04262691
Iteration 44, loss = 0.04152143
Iteration 45, loss = 0.04029621
Iteration 46, loss = 0.03901617
Iteration 47, loss = 0.03802891
Iteration 48, loss = 0.03704767
Iteration 49, loss = 0.03633007
Iteration 50, loss = 0.03500358
Iteration 51, loss = 0.03407969
Iteration 52, loss = 0.03307318
Iteration 53, loss = 0.03222487
Iteration 54, loss = 0.03124995
Iteration 55, loss = 0.03074981
Iteration 56, loss = 0.02952388
Iteration 57, loss = 0.02875259
Iteration 58, loss = 0.02819938
Iteration 59, loss = 0.02734290
Iteration 60, loss = 0.02661655
Iteration 61, loss = 0.02589474
Iteration 62, loss = 0.02528320
Iteration 63, loss = 0.02458241
Iteration 64, loss = 0.02392939
Iteration 65, loss = 0.02335324
Iteration 66, loss = 0.02260826
Iteration 67, loss = 0.02195693
Iteration 68, loss = 0.02138970
Iteration 69, loss = 0.02081808
Iteration 70, loss = 0.02019524
Iteration 71, loss = 0.01960723
Iteration 72, loss = 0.01916871
Iteration 73, loss = 0.01859515
Iteration 74, loss = 0.01803853
Iteration 75, loss = 0.01766479
Iteration 76, loss = 0.01721475
Iteration 77, loss = 0.01678485
Iteration 78, loss = 0.01629551
Iteration 79, loss = 0.01576243
Iteration 80, loss = 0.01533625
Iteration 81, loss = 0.01501365
Iteration 82, loss = 0.01455192
Iteration 83, loss = 0.01424783
Iteration 84, loss = 0.01381320
Iteration 85, loss = 0.01343870
Iteration 86, loss = 0.01307967
Iteration 87, loss = 0.01270473
Iteration 88, loss = 0.01239217
Iteration 89, loss = 0.01215199
Iteration 90, loss = 0.01173312
Iteration 91, loss = 0.01140313
Iteration 92, loss = 0.01115183
Iteration 93, loss = 0.01086886
Iteration 94, loss = 0.01055318
Iteration 95, loss = 0.01042499
Iteration 96, loss = 0.01014570
Iteration 97, loss = 0.00978996
Iteration 98, loss = 0.00954460
Iteration 99, loss = 0.00930611
Iteration 100, loss = 0.00913736
Iteration 101, loss = 0.00895207
Iteration 102, loss = 0.00864745
Iteration 103, loss = 0.00841396
Iteration 104, loss = 0.00821318
Iteration 105, loss = 0.00804847
Iteration 106, loss = 0.00788096
Iteration 107, loss = 0.00767371
Iteration 108, loss = 0.00746867
Iteration 109, loss = 0.00724965
Iteration 110, loss = 0.00712300
Iteration 111, loss = 0.00701726
Iteration 112, loss = 0.00679621
Iteration 113, loss = 0.00660624
Iteration 114, loss = 0.00646886
Iteration 115, loss = 0.00637261
Iteration 116, loss = 0.00622661
Iteration 117, loss = 0.00602961
Iteration 118, loss = 0.00590030
Iteration 119, loss = 0.00579562
Iteration 120, loss = 0.00565445
Iteration 121, loss = 0.00552599
Iteration 122, loss = 0.00539924
Iteration 123, loss = 0.00529921
Iteration 124, loss = 0.00517076
Iteration 125, loss = 0.00508745
Iteration 126, loss = 0.00499605
Iteration 127, loss = 0.00486558
Iteration 128, loss = 0.00477685
Iteration 129, loss = 0.00467548
Iteration 130, loss = 0.00457981
Iteration 131, loss = 0.00450752
Iteration 132, loss = 0.00441262
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  65
Error rate: 26.08% ( 652/2500)
 - Class 1:  70, Class 2:  17, Class 3: 500, Class 4:  37, Class 5:  28
Iteration 1, loss = 1.29734980
Iteration 2, loss = 0.80930037
Iteration 3, loss = 0.52826822
Iteration 4, loss = 0.37686627
Iteration 5, loss = 0.29230594
Iteration 6, loss = 0.24063564
Iteration 7, loss = 0.20671767
Iteration 8, loss = 0.18232596
Iteration 9, loss = 0.16429019
Iteration 10, loss = 0.15028334
Iteration 11, loss = 0.13910771
Iteration 12, loss = 0.12982334
Iteration 13, loss = 0.12207467
Iteration 14, loss = 0.11540454
Iteration 15, loss = 0.10936468
Iteration 16, loss = 0.10381897
Iteration 17, loss = 0.09947383
Iteration 18, loss = 0.09483899
Iteration 19, loss = 0.09091391
Iteration 20, loss = 0.08715466
Iteration 21, loss = 0.08395436
Iteration 22, loss = 0.08071545
Iteration 23, loss = 0.07831684
Iteration 24, loss = 0.07518635
Iteration 25, loss = 0.07280322
Iteration 26, loss = 0.06977832
Iteration 27, loss = 0.06729270
Iteration 28, loss = 0.06520206
Iteration 29, loss = 0.06275952
Iteration 30, loss = 0.06086981
Iteration 31, loss = 0.05875704
Iteration 32, loss = 0.05752866
Iteration 33, loss = 0.05542097
Iteration 34, loss = 0.05338683
Iteration 35, loss = 0.05179331
Iteration 36, loss = 0.05051904
Iteration 37, loss = 0.04911293
Iteration 38, loss = 0.04748271
Iteration 39, loss = 0.04581972
Iteration 40, loss = 0.04455804
Iteration 41, loss = 0.04340719
Iteration 42, loss = 0.04201112
Iteration 43, loss = 0.04082424
Iteration 44, loss = 0.03957051
Iteration 45, loss = 0.03862838
Iteration 46, loss = 0.03774702
Iteration 47, loss = 0.03667384
Iteration 48, loss = 0.03563965
Iteration 49, loss = 0.03411545
Iteration 50, loss = 0.03323888
Iteration 51, loss = 0.03234160
Iteration 52, loss = 0.03137774
Iteration 53, loss = 0.03045881
Iteration 54, loss = 0.02973008
Iteration 55, loss = 0.02888181
Iteration 56, loss = 0.02802021
Iteration 57, loss = 0.02730341
Iteration 58, loss = 0.02643318
Iteration 59, loss = 0.02564404
Iteration 60, loss = 0.02498944
Iteration 61, loss = 0.02434132
Iteration 62, loss = 0.02364797
Iteration 63, loss = 0.02300480
Iteration 64, loss = 0.02248460
Iteration 65, loss = 0.02226702
Iteration 66, loss = 0.02106724
Iteration 67, loss = 0.02046397
Iteration 68, loss = 0.01979011
Iteration 69, loss = 0.01930622
Iteration 70, loss = 0.01882205
Iteration 71, loss = 0.01835791
Iteration 72, loss = 0.01772630
Iteration 73, loss = 0.01730445
Iteration 74, loss = 0.01690832
Iteration 75, loss = 0.01641671
Iteration 76, loss = 0.01598985
Iteration 77, loss = 0.01536988
Iteration 78, loss = 0.01496646
Iteration 79, loss = 0.01455743
Iteration 80, loss = 0.01417782
Iteration 81, loss = 0.01377610
Iteration 82, loss = 0.01347992
Iteration 83, loss = 0.01314332
Iteration 84, loss = 0.01267644
Iteration 85, loss = 0.01243038
Iteration 86, loss = 0.01212022
Iteration 87, loss = 0.01176139
Iteration 88, loss = 0.01142313
Iteration 89, loss = 0.01120053
Iteration 90, loss = 0.01091377
Iteration 91, loss = 0.01063065
Iteration 92, loss = 0.01034503
Iteration 93, loss = 0.01010341
Iteration 94, loss = 0.00976953
Iteration 95, loss = 0.00951728
Iteration 96, loss = 0.00929799
Iteration 97, loss = 0.00906389
Iteration 98, loss = 0.00880699
Iteration 99, loss = 0.00862140
Iteration 100, loss = 0.00843771
Iteration 101, loss = 0.00825386
Iteration 102, loss = 0.00799923
Iteration 103, loss = 0.00783274
Iteration 104, loss = 0.00761780
Iteration 105, loss = 0.00744230
Iteration 106, loss = 0.00729333
Iteration 107, loss = 0.00713506
Iteration 108, loss = 0.00699939
Iteration 109, loss = 0.00677847
Iteration 110, loss = 0.00662619
Iteration 111, loss = 0.00650580
Iteration 112, loss = 0.00634096
Iteration 113, loss = 0.00619355
Iteration 114, loss = 0.00606143
Iteration 115, loss = 0.00592895
Iteration 116, loss = 0.00580069
Iteration 117, loss = 0.00567933
Iteration 118, loss = 0.00560857
Iteration 119, loss = 0.00546454
Iteration 120, loss = 0.00532097
Iteration 121, loss = 0.00518588
Iteration 122, loss = 0.00512641
Iteration 123, loss = 0.00503803
Iteration 124, loss = 0.00487233
Iteration 125, loss = 0.00475887
Iteration 126, loss = 0.00469055
Iteration 127, loss = 0.00460132
Iteration 128, loss = 0.00447818
Iteration 129, loss = 0.00440729
Iteration 130, loss = 0.00432364
Iteration 131, loss = 0.00425990
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  70
Error rate: 24.24% ( 606/2500)
 - Class 1:  50, Class 2:  16, Class 3: 452, Class 4:  45, Class 5:  43
Iteration 1, loss = 1.26187469
Iteration 2, loss = 0.76689962
Iteration 3, loss = 0.49830956
Iteration 4, loss = 0.35606029
Iteration 5, loss = 0.27761315
Iteration 6, loss = 0.22936185
Iteration 7, loss = 0.19793186
Iteration 8, loss = 0.17531140
Iteration 9, loss = 0.15848272
Iteration 10, loss = 0.14528364
Iteration 11, loss = 0.13495076
Iteration 12, loss = 0.12638421
Iteration 13, loss = 0.11853124
Iteration 14, loss = 0.11208919
Iteration 15, loss = 0.10633173
Iteration 16, loss = 0.10197026
Iteration 17, loss = 0.09727095
Iteration 18, loss = 0.09270353
Iteration 19, loss = 0.08881699
Iteration 20, loss = 0.08519348
Iteration 21, loss = 0.08192516
Iteration 22, loss = 0.07887262
Iteration 23, loss = 0.07609380
Iteration 24, loss = 0.07333864
Iteration 25, loss = 0.07084519
Iteration 26, loss = 0.06897780
Iteration 27, loss = 0.06622880
Iteration 28, loss = 0.06388404
Iteration 29, loss = 0.06203244
Iteration 30, loss = 0.05993753
Iteration 31, loss = 0.05829610
Iteration 32, loss = 0.05606090
Iteration 33, loss = 0.05418624
Iteration 34, loss = 0.05276765
Iteration 35, loss = 0.05093950
Iteration 36, loss = 0.04961033
Iteration 37, loss = 0.04851829
Iteration 38, loss = 0.04666837
Iteration 39, loss = 0.04520539
Iteration 40, loss = 0.04378800
Iteration 41, loss = 0.04240135
Iteration 42, loss = 0.04101179
Iteration 43, loss = 0.03992242
Iteration 44, loss = 0.03877744
Iteration 45, loss = 0.03761268
Iteration 46, loss = 0.03658390
Iteration 47, loss = 0.03544720
Iteration 48, loss = 0.03456190
Iteration 49, loss = 0.03352187
Iteration 50, loss = 0.03286192
Iteration 51, loss = 0.03195733
Iteration 52, loss = 0.03067018
Iteration 53, loss = 0.03012620
Iteration 54, loss = 0.02905791
Iteration 55, loss = 0.02822117
Iteration 56, loss = 0.02751438
Iteration 57, loss = 0.02650691
Iteration 58, loss = 0.02593420
Iteration 59, loss = 0.02546660
Iteration 60, loss = 0.02456895
Iteration 61, loss = 0.02348099
Iteration 62, loss = 0.02281577
Iteration 63, loss = 0.02224643
Iteration 64, loss = 0.02159778
Iteration 65, loss = 0.02110346
Iteration 66, loss = 0.02042050
Iteration 67, loss = 0.01996704
Iteration 68, loss = 0.01926800
Iteration 69, loss = 0.01861358
Iteration 70, loss = 0.01827116
Iteration 71, loss = 0.01762359
Iteration 72, loss = 0.01737738
Iteration 73, loss = 0.01675232
Iteration 74, loss = 0.01613287
Iteration 75, loss = 0.01584301
Iteration 76, loss = 0.01521623
Iteration 77, loss = 0.01484649
Iteration 78, loss = 0.01436720
Iteration 79, loss = 0.01404074
Iteration 80, loss = 0.01372325
Iteration 81, loss = 0.01340265
Iteration 82, loss = 0.01284951
Iteration 83, loss = 0.01246525
Iteration 84, loss = 0.01213632
Iteration 85, loss = 0.01177767
Iteration 86, loss = 0.01151408
Iteration 87, loss = 0.01114840
Iteration 88, loss = 0.01086141
Iteration 89, loss = 0.01050159
Iteration 90, loss = 0.01030651
Iteration 91, loss = 0.00998789
Iteration 92, loss = 0.00968197
Iteration 93, loss = 0.00945817
Iteration 94, loss = 0.00929738
Iteration 95, loss = 0.00897236
Iteration 96, loss = 0.00875759
Iteration 97, loss = 0.00854320
Iteration 98, loss = 0.00829453
Iteration 99, loss = 0.00811360
Iteration 100, loss = 0.00785486
Iteration 101, loss = 0.00770718
Iteration 102, loss = 0.00749685
Iteration 103, loss = 0.00730246
Iteration 104, loss = 0.00709014
Iteration 105, loss = 0.00689613
Iteration 106, loss = 0.00678500
Iteration 107, loss = 0.00662668
Iteration 108, loss = 0.00639901
Iteration 109, loss = 0.00634974
Iteration 110, loss = 0.00613174
Iteration 111, loss = 0.00597069
Iteration 112, loss = 0.00586500
Iteration 113, loss = 0.00575693
Iteration 114, loss = 0.00557982
Iteration 115, loss = 0.00547242
Iteration 116, loss = 0.00532626
Iteration 117, loss = 0.00524170
Iteration 118, loss = 0.00507721
Iteration 119, loss = 0.00496359
Iteration 120, loss = 0.00488468
Iteration 121, loss = 0.00476334
Iteration 122, loss = 0.00467054
Iteration 123, loss = 0.00458354
Iteration 124, loss = 0.00445217
Iteration 125, loss = 0.00435693
Iteration 126, loss = 0.00428823
Iteration 127, loss = 0.00416696
Iteration 128, loss = 0.00410816
Iteration 129, loss = 0.00401101
Iteration 130, loss = 0.00391752
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  75
Error rate: 24.60% ( 615/2500)
 - Class 1:  40, Class 2:  21, Class 3: 493, Class 4:  31, Class 5:  30
Iteration 1, loss = 1.23291697
Iteration 2, loss = 0.71513366
Iteration 3, loss = 0.45458797
Iteration 4, loss = 0.32725376
Iteration 5, loss = 0.25718665
Iteration 6, loss = 0.21522728
Iteration 7, loss = 0.18703623
Iteration 8, loss = 0.16705671
Iteration 9, loss = 0.15128215
Iteration 10, loss = 0.13910891
Iteration 11, loss = 0.12932095
Iteration 12, loss = 0.12157435
Iteration 13, loss = 0.11459329
Iteration 14, loss = 0.10877343
Iteration 15, loss = 0.10335388
Iteration 16, loss = 0.09768731
Iteration 17, loss = 0.09389320
Iteration 18, loss = 0.08956055
Iteration 19, loss = 0.08592277
Iteration 20, loss = 0.08243403
Iteration 21, loss = 0.07939360
Iteration 22, loss = 0.07621019
Iteration 23, loss = 0.07349457
Iteration 24, loss = 0.07100845
Iteration 25, loss = 0.06823407
Iteration 26, loss = 0.06603792
Iteration 27, loss = 0.06373639
Iteration 28, loss = 0.06175040
Iteration 29, loss = 0.05940931
Iteration 30, loss = 0.05744972
Iteration 31, loss = 0.05558571
Iteration 32, loss = 0.05357406
Iteration 33, loss = 0.05257725
Iteration 34, loss = 0.05049492
Iteration 35, loss = 0.04892623
Iteration 36, loss = 0.04721590
Iteration 37, loss = 0.04620583
Iteration 38, loss = 0.04488865
Iteration 39, loss = 0.04313388
Iteration 40, loss = 0.04217251
Iteration 41, loss = 0.04060683
Iteration 42, loss = 0.03944038
Iteration 43, loss = 0.03811475
Iteration 44, loss = 0.03718312
Iteration 45, loss = 0.03604327
Iteration 46, loss = 0.03510096
Iteration 47, loss = 0.03428544
Iteration 48, loss = 0.03305877
Iteration 49, loss = 0.03201642
Iteration 50, loss = 0.03112560
Iteration 51, loss = 0.03008836
Iteration 52, loss = 0.02926953
Iteration 53, loss = 0.02843587
Iteration 54, loss = 0.02768686
Iteration 55, loss = 0.02676385
Iteration 56, loss = 0.02603367
Iteration 57, loss = 0.02564676
Iteration 58, loss = 0.02456343
Iteration 59, loss = 0.02373996
Iteration 60, loss = 0.02307458
Iteration 61, loss = 0.02239226
Iteration 62, loss = 0.02180578
Iteration 63, loss = 0.02113493
Iteration 64, loss = 0.02063525
Iteration 65, loss = 0.01983405
Iteration 66, loss = 0.01956257
Iteration 67, loss = 0.01872543
Iteration 68, loss = 0.01833592
Iteration 69, loss = 0.01772766
Iteration 70, loss = 0.01744631
Iteration 71, loss = 0.01658913
Iteration 72, loss = 0.01624527
Iteration 73, loss = 0.01570303
Iteration 74, loss = 0.01529577
Iteration 75, loss = 0.01488763
Iteration 76, loss = 0.01432044
Iteration 77, loss = 0.01402174
Iteration 78, loss = 0.01361506
Iteration 79, loss = 0.01327373
Iteration 80, loss = 0.01294040
Iteration 81, loss = 0.01252679
Iteration 82, loss = 0.01210386
Iteration 83, loss = 0.01171522
Iteration 84, loss = 0.01148407
Iteration 85, loss = 0.01107646
Iteration 86, loss = 0.01098105
Iteration 87, loss = 0.01058421
Iteration 88, loss = 0.01020796
Iteration 89, loss = 0.00996967
Iteration 90, loss = 0.00974620
Iteration 91, loss = 0.00942344
Iteration 92, loss = 0.00915852
Iteration 93, loss = 0.00896691
Iteration 94, loss = 0.00869998
Iteration 95, loss = 0.00843899
Iteration 96, loss = 0.00821949
Iteration 97, loss = 0.00806133
Iteration 98, loss = 0.00780142
Iteration 99, loss = 0.00771425
Iteration 100, loss = 0.00750200
Iteration 101, loss = 0.00722915
Iteration 102, loss = 0.00707735
Iteration 103, loss = 0.00692057
Iteration 104, loss = 0.00679935
Iteration 105, loss = 0.00657942
Iteration 106, loss = 0.00630657
Iteration 107, loss = 0.00622181
Iteration 108, loss = 0.00603597
Iteration 109, loss = 0.00596494
Iteration 110, loss = 0.00580876
Iteration 111, loss = 0.00565330
Iteration 112, loss = 0.00551695
Iteration 113, loss = 0.00538937
Iteration 114, loss = 0.00527697
Iteration 115, loss = 0.00514366
Iteration 116, loss = 0.00500475
Iteration 117, loss = 0.00490430
Iteration 118, loss = 0.00480258
Iteration 119, loss = 0.00468250
Iteration 120, loss = 0.00459152
Iteration 121, loss = 0.00452427
Iteration 122, loss = 0.00443008
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  80
Error rate: 24.60% ( 615/2500)
 - Class 1:  35, Class 2:  22, Class 3: 498, Class 4:  36, Class 5:  24
Iteration 1, loss = 1.25323938
Iteration 2, loss = 0.73374537
Iteration 3, loss = 0.46466127
Iteration 4, loss = 0.33048774
Iteration 5, loss = 0.25853709
Iteration 6, loss = 0.21577101
Iteration 7, loss = 0.18763962
Iteration 8, loss = 0.16631273
Iteration 9, loss = 0.15152355
Iteration 10, loss = 0.13935392
Iteration 11, loss = 0.12965220
Iteration 12, loss = 0.12112649
Iteration 13, loss = 0.11443672
Iteration 14, loss = 0.10789715
Iteration 15, loss = 0.10241917
Iteration 16, loss = 0.09729528
Iteration 17, loss = 0.09335712
Iteration 18, loss = 0.08882899
Iteration 19, loss = 0.08517208
Iteration 20, loss = 0.08206464
Iteration 21, loss = 0.07862571
Iteration 22, loss = 0.07532594
Iteration 23, loss = 0.07282834
Iteration 24, loss = 0.07019440
Iteration 25, loss = 0.06766064
Iteration 26, loss = 0.06546299
Iteration 27, loss = 0.06294209
Iteration 28, loss = 0.06073588
Iteration 29, loss = 0.05856046
Iteration 30, loss = 0.05642890
Iteration 31, loss = 0.05454628
Iteration 32, loss = 0.05327903
Iteration 33, loss = 0.05105285
Iteration 34, loss = 0.04978923
Iteration 35, loss = 0.04764421
Iteration 36, loss = 0.04616978
Iteration 37, loss = 0.04488560
Iteration 38, loss = 0.04328057
Iteration 39, loss = 0.04223059
Iteration 40, loss = 0.04088400
Iteration 41, loss = 0.03961840
Iteration 42, loss = 0.03866590
Iteration 43, loss = 0.03744189
Iteration 44, loss = 0.03619452
Iteration 45, loss = 0.03513321
Iteration 46, loss = 0.03392663
Iteration 47, loss = 0.03295462
Iteration 48, loss = 0.03207891
Iteration 49, loss = 0.03105179
Iteration 50, loss = 0.03032318
Iteration 51, loss = 0.02935198
Iteration 52, loss = 0.02833192
Iteration 53, loss = 0.02769395
Iteration 54, loss = 0.02692276
Iteration 55, loss = 0.02597581
Iteration 56, loss = 0.02514179
Iteration 57, loss = 0.02433679
Iteration 58, loss = 0.02393152
Iteration 59, loss = 0.02297635
Iteration 60, loss = 0.02253771
Iteration 61, loss = 0.02198820
Iteration 62, loss = 0.02099875
Iteration 63, loss = 0.02026514
Iteration 64, loss = 0.01962351
Iteration 65, loss = 0.01906053
Iteration 66, loss = 0.01867309
Iteration 67, loss = 0.01792669
Iteration 68, loss = 0.01754276
Iteration 69, loss = 0.01694980
Iteration 70, loss = 0.01643032
Iteration 71, loss = 0.01601343
Iteration 72, loss = 0.01563465
Iteration 73, loss = 0.01508436
Iteration 74, loss = 0.01456169
Iteration 75, loss = 0.01415290
Iteration 76, loss = 0.01371689
Iteration 77, loss = 0.01335545
Iteration 78, loss = 0.01295204
Iteration 79, loss = 0.01251920
Iteration 80, loss = 0.01218207
Iteration 81, loss = 0.01183593
Iteration 82, loss = 0.01151820
Iteration 83, loss = 0.01122268
Iteration 84, loss = 0.01095650
Iteration 85, loss = 0.01058243
Iteration 86, loss = 0.01029318
Iteration 87, loss = 0.01000885
Iteration 88, loss = 0.00972229
Iteration 89, loss = 0.00948737
Iteration 90, loss = 0.00920735
Iteration 91, loss = 0.00898255
Iteration 92, loss = 0.00873208
Iteration 93, loss = 0.00852120
Iteration 94, loss = 0.00821678
Iteration 95, loss = 0.00802541
Iteration 96, loss = 0.00781066
Iteration 97, loss = 0.00763278
Iteration 98, loss = 0.00742239
Iteration 99, loss = 0.00726311
Iteration 100, loss = 0.00701329
Iteration 101, loss = 0.00683085
Iteration 102, loss = 0.00663678
Iteration 103, loss = 0.00651500
Iteration 104, loss = 0.00636312
Iteration 105, loss = 0.00623259
Iteration 106, loss = 0.00608339
Iteration 107, loss = 0.00590946
Iteration 108, loss = 0.00578640
Iteration 109, loss = 0.00567812
Iteration 110, loss = 0.00550488
Iteration 111, loss = 0.00543541
Iteration 112, loss = 0.00527146
Iteration 113, loss = 0.00512519
Iteration 114, loss = 0.00502438
Iteration 115, loss = 0.00488468
Iteration 116, loss = 0.00477199
Iteration 117, loss = 0.00466417
Iteration 118, loss = 0.00456502
Iteration 119, loss = 0.00445815
Iteration 120, loss = 0.00436041
Iteration 121, loss = 0.00427898
Iteration 122, loss = 0.00419099
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  85
Error rate: 25.44% ( 636/2500)
 - Class 1:  24, Class 2:  16, Class 3: 473, Class 4:  50, Class 5:  73
Iteration 1, loss = 1.22190969
Iteration 2, loss = 0.68002727
Iteration 3, loss = 0.42883392
Iteration 4, loss = 0.30975404
Iteration 5, loss = 0.24577067
Iteration 6, loss = 0.20651783
Iteration 7, loss = 0.18092486
Iteration 8, loss = 0.16122600
Iteration 9, loss = 0.14729344
Iteration 10, loss = 0.13551345
Iteration 11, loss = 0.12641322
Iteration 12, loss = 0.11799268
Iteration 13, loss = 0.11218203
Iteration 14, loss = 0.10580965
Iteration 15, loss = 0.10003158
Iteration 16, loss = 0.09521728
Iteration 17, loss = 0.09114113
Iteration 18, loss = 0.08811958
Iteration 19, loss = 0.08358219
Iteration 20, loss = 0.08008383
Iteration 21, loss = 0.07651051
Iteration 22, loss = 0.07376118
Iteration 23, loss = 0.07110990
Iteration 24, loss = 0.06814731
Iteration 25, loss = 0.06573344
Iteration 26, loss = 0.06329357
Iteration 27, loss = 0.06113032
Iteration 28, loss = 0.05903158
Iteration 29, loss = 0.05696299
Iteration 30, loss = 0.05489059
Iteration 31, loss = 0.05345220
Iteration 32, loss = 0.05132154
Iteration 33, loss = 0.04962360
Iteration 34, loss = 0.04839781
Iteration 35, loss = 0.04645159
Iteration 36, loss = 0.04521371
Iteration 37, loss = 0.04367570
Iteration 38, loss = 0.04236231
Iteration 39, loss = 0.04093624
Iteration 40, loss = 0.04013046
Iteration 41, loss = 0.03851779
Iteration 42, loss = 0.03714980
Iteration 43, loss = 0.03590788
Iteration 44, loss = 0.03518775
Iteration 45, loss = 0.03386810
Iteration 46, loss = 0.03289055
Iteration 47, loss = 0.03174014
Iteration 48, loss = 0.03095279
Iteration 49, loss = 0.02984909
Iteration 50, loss = 0.02884901
Iteration 51, loss = 0.02798699
Iteration 52, loss = 0.02731586
Iteration 53, loss = 0.02630551
Iteration 54, loss = 0.02570362
Iteration 55, loss = 0.02485701
Iteration 56, loss = 0.02386029
Iteration 57, loss = 0.02318250
Iteration 58, loss = 0.02268967
Iteration 59, loss = 0.02184551
Iteration 60, loss = 0.02118411
Iteration 61, loss = 0.02053051
Iteration 62, loss = 0.02023468
Iteration 63, loss = 0.01948336
Iteration 64, loss = 0.01877930
Iteration 65, loss = 0.01807001
Iteration 66, loss = 0.01747360
Iteration 67, loss = 0.01697069
Iteration 68, loss = 0.01641521
Iteration 69, loss = 0.01597216
Iteration 70, loss = 0.01528827
Iteration 71, loss = 0.01490175
Iteration 72, loss = 0.01448972
Iteration 73, loss = 0.01409679
Iteration 74, loss = 0.01374978
Iteration 75, loss = 0.01326642
Iteration 76, loss = 0.01279354
Iteration 77, loss = 0.01249883
Iteration 78, loss = 0.01216169
Iteration 79, loss = 0.01169757
Iteration 80, loss = 0.01130893
Iteration 81, loss = 0.01107287
Iteration 82, loss = 0.01072933
Iteration 83, loss = 0.01036611
Iteration 84, loss = 0.01010708
Iteration 85, loss = 0.00981697
Iteration 86, loss = 0.00945629
Iteration 87, loss = 0.00925344
Iteration 88, loss = 0.00897301
Iteration 89, loss = 0.00871347
Iteration 90, loss = 0.00847251
Iteration 91, loss = 0.00829389
Iteration 92, loss = 0.00804174
Iteration 93, loss = 0.00794531
Iteration 94, loss = 0.00763680
Iteration 95, loss = 0.00749043
Iteration 96, loss = 0.00727353
Iteration 97, loss = 0.00706456
Iteration 98, loss = 0.00688606
Iteration 99, loss = 0.00669956
Iteration 100, loss = 0.00655452
Iteration 101, loss = 0.00641493
Iteration 102, loss = 0.00620851
Iteration 103, loss = 0.00605493
Iteration 104, loss = 0.00590384
Iteration 105, loss = 0.00576982
Iteration 106, loss = 0.00561736
Iteration 107, loss = 0.00548612
Iteration 108, loss = 0.00535021
Iteration 109, loss = 0.00521426
Iteration 110, loss = 0.00508552
Iteration 111, loss = 0.00498494
Iteration 112, loss = 0.00486605
Iteration 113, loss = 0.00475542
Iteration 114, loss = 0.00461437
Iteration 115, loss = 0.00450948
Iteration 116, loss = 0.00442721
Iteration 117, loss = 0.00432023
Iteration 118, loss = 0.00423740
Iteration 119, loss = 0.00416024
Iteration 120, loss = 0.00405413
Iteration 121, loss = 0.00401282
Iteration 122, loss = 0.00390951
Iteration 123, loss = 0.00381184
Iteration 124, loss = 0.00371970
Iteration 125, loss = 0.00365835
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  90
Error rate: 25.92% ( 648/2500)
 - Class 1:  61, Class 2:  29, Class 3: 499, Class 4:  37, Class 5:  22
Iteration 1, loss = 1.24095915
Iteration 2, loss = 0.70447962
Iteration 3, loss = 0.44342710
Iteration 4, loss = 0.31685735
Iteration 5, loss = 0.25017536
Iteration 6, loss = 0.20912606
Iteration 7, loss = 0.18209732
Iteration 8, loss = 0.16337550
Iteration 9, loss = 0.14858644
Iteration 10, loss = 0.13641221
Iteration 11, loss = 0.12685658
Iteration 12, loss = 0.11897469
Iteration 13, loss = 0.11227747
Iteration 14, loss = 0.10607868
Iteration 15, loss = 0.10053215
Iteration 16, loss = 0.09570160
Iteration 17, loss = 0.09143745
Iteration 18, loss = 0.08743080
Iteration 19, loss = 0.08409934
Iteration 20, loss = 0.08058403
Iteration 21, loss = 0.07787783
Iteration 22, loss = 0.07416704
Iteration 23, loss = 0.07153537
Iteration 24, loss = 0.06902304
Iteration 25, loss = 0.06620572
Iteration 26, loss = 0.06381967
Iteration 27, loss = 0.06172120
Iteration 28, loss = 0.05916068
Iteration 29, loss = 0.05758733
Iteration 30, loss = 0.05508689
Iteration 31, loss = 0.05367852
Iteration 32, loss = 0.05165240
Iteration 33, loss = 0.04986809
Iteration 34, loss = 0.04856307
Iteration 35, loss = 0.04694281
Iteration 36, loss = 0.04535843
Iteration 37, loss = 0.04371278
Iteration 38, loss = 0.04280642
Iteration 39, loss = 0.04149386
Iteration 40, loss = 0.03978611
Iteration 41, loss = 0.03890330
Iteration 42, loss = 0.03728249
Iteration 43, loss = 0.03645440
Iteration 44, loss = 0.03533010
Iteration 45, loss = 0.03459826
Iteration 46, loss = 0.03324812
Iteration 47, loss = 0.03208950
Iteration 48, loss = 0.03103800
Iteration 49, loss = 0.03024723
Iteration 50, loss = 0.02925144
Iteration 51, loss = 0.02824573
Iteration 52, loss = 0.02755793
Iteration 53, loss = 0.02672984
Iteration 54, loss = 0.02603228
Iteration 55, loss = 0.02505917
Iteration 56, loss = 0.02441319
Iteration 57, loss = 0.02351375
Iteration 58, loss = 0.02289140
Iteration 59, loss = 0.02231371
Iteration 60, loss = 0.02154517
Iteration 61, loss = 0.02087779
Iteration 62, loss = 0.02023971
Iteration 63, loss = 0.01958982
Iteration 64, loss = 0.01907703
Iteration 65, loss = 0.01849674
Iteration 66, loss = 0.01784343
Iteration 67, loss = 0.01726548
Iteration 68, loss = 0.01670013
Iteration 69, loss = 0.01620786
Iteration 70, loss = 0.01574367
Iteration 71, loss = 0.01534514
Iteration 72, loss = 0.01480980
Iteration 73, loss = 0.01427895
Iteration 74, loss = 0.01397530
Iteration 75, loss = 0.01359017
Iteration 76, loss = 0.01313468
Iteration 77, loss = 0.01262141
Iteration 78, loss = 0.01231561
Iteration 79, loss = 0.01193954
Iteration 80, loss = 0.01179855
Iteration 81, loss = 0.01142837
Iteration 82, loss = 0.01095803
Iteration 83, loss = 0.01057045
Iteration 84, loss = 0.01026479
Iteration 85, loss = 0.01012560
Iteration 86, loss = 0.00977027
Iteration 87, loss = 0.00944160
Iteration 88, loss = 0.00927246
Iteration 89, loss = 0.00898205
Iteration 90, loss = 0.00865737
Iteration 91, loss = 0.00845974
Iteration 92, loss = 0.00826676
Iteration 93, loss = 0.00800416
Iteration 94, loss = 0.00776290
Iteration 95, loss = 0.00755514
Iteration 96, loss = 0.00744562
Iteration 97, loss = 0.00714007
Iteration 98, loss = 0.00704002
Iteration 99, loss = 0.00677778
Iteration 100, loss = 0.00657713
Iteration 101, loss = 0.00651202
Iteration 102, loss = 0.00632222
Iteration 103, loss = 0.00611904
Iteration 104, loss = 0.00599808
Iteration 105, loss = 0.00580592
Iteration 106, loss = 0.00577223
Iteration 107, loss = 0.00566382
Iteration 108, loss = 0.00547459
Iteration 109, loss = 0.00530257
Iteration 110, loss = 0.00517401
Iteration 111, loss = 0.00505983
Iteration 112, loss = 0.00491388
Iteration 113, loss = 0.00481347
Iteration 114, loss = 0.00467924
Iteration 115, loss = 0.00459762
Iteration 116, loss = 0.00451486
Iteration 117, loss = 0.00438887
Iteration 118, loss = 0.00432782
Iteration 119, loss = 0.00421506
Iteration 120, loss = 0.00410191
Iteration 121, loss = 0.00401856
Iteration 122, loss = 0.00395578
Iteration 123, loss = 0.00387108
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  95
Error rate: 24.40% ( 610/2500)
 - Class 1:  31, Class 2:  18, Class 3: 500, Class 4:  29, Class 5:  32
Iteration 1, loss = 1.25123956
Iteration 2, loss = 0.71849188
Iteration 3, loss = 0.44619617
Iteration 4, loss = 0.31367464
Iteration 5, loss = 0.24468279
Iteration 6, loss = 0.20376597
Iteration 7, loss = 0.17678313
Iteration 8, loss = 0.15832124
Iteration 9, loss = 0.14339682
Iteration 10, loss = 0.13227186
Iteration 11, loss = 0.12364823
Iteration 12, loss = 0.11575753
Iteration 13, loss = 0.10883773
Iteration 14, loss = 0.10256291
Iteration 15, loss = 0.09755435
Iteration 16, loss = 0.09283279
Iteration 17, loss = 0.08884223
Iteration 18, loss = 0.08509702
Iteration 19, loss = 0.08106608
Iteration 20, loss = 0.07770834
Iteration 21, loss = 0.07489330
Iteration 22, loss = 0.07218306
Iteration 23, loss = 0.06938793
Iteration 24, loss = 0.06637584
Iteration 25, loss = 0.06386005
Iteration 26, loss = 0.06160757
Iteration 27, loss = 0.06004191
Iteration 28, loss = 0.05744787
Iteration 29, loss = 0.05541584
Iteration 30, loss = 0.05360709
Iteration 31, loss = 0.05189056
Iteration 32, loss = 0.05030294
Iteration 33, loss = 0.04862258
Iteration 34, loss = 0.04677750
Iteration 35, loss = 0.04513323
Iteration 36, loss = 0.04393219
Iteration 37, loss = 0.04250730
Iteration 38, loss = 0.04119351
Iteration 39, loss = 0.03978176
Iteration 40, loss = 0.03881577
Iteration 41, loss = 0.03794248
Iteration 42, loss = 0.03642715
Iteration 43, loss = 0.03545234
Iteration 44, loss = 0.03427646
Iteration 45, loss = 0.03336372
Iteration 46, loss = 0.03240193
Iteration 47, loss = 0.03114746
Iteration 48, loss = 0.03049857
Iteration 49, loss = 0.02942215
Iteration 50, loss = 0.02865686
Iteration 51, loss = 0.02752169
Iteration 52, loss = 0.02673828
Iteration 53, loss = 0.02622921
Iteration 54, loss = 0.02543370
Iteration 55, loss = 0.02482966
Iteration 56, loss = 0.02360908
Iteration 57, loss = 0.02272970
Iteration 58, loss = 0.02222138
Iteration 59, loss = 0.02185720
Iteration 60, loss = 0.02070440
Iteration 61, loss = 0.02020188
Iteration 62, loss = 0.01963120
Iteration 63, loss = 0.01894148
Iteration 64, loss = 0.01841627
Iteration 65, loss = 0.01778920
Iteration 66, loss = 0.01725918
Iteration 67, loss = 0.01687993
Iteration 68, loss = 0.01617882
Iteration 69, loss = 0.01568117
Iteration 70, loss = 0.01549346
Iteration 71, loss = 0.01476430
Iteration 72, loss = 0.01440831
Iteration 73, loss = 0.01399655
Iteration 74, loss = 0.01343290
Iteration 75, loss = 0.01311366
Iteration 76, loss = 0.01269418
Iteration 77, loss = 0.01239703
Iteration 78, loss = 0.01189684
Iteration 79, loss = 0.01154312
Iteration 80, loss = 0.01117955
Iteration 81, loss = 0.01088110
Iteration 82, loss = 0.01060026
Iteration 83, loss = 0.01044689
Iteration 84, loss = 0.01004565
Iteration 85, loss = 0.00970540
Iteration 86, loss = 0.00944209
Iteration 87, loss = 0.00910362
Iteration 88, loss = 0.00884473
Iteration 89, loss = 0.00855624
Iteration 90, loss = 0.00843113
Iteration 91, loss = 0.00835532
Iteration 92, loss = 0.00804749
Iteration 93, loss = 0.00767599
Iteration 94, loss = 0.00744147
Iteration 95, loss = 0.00731364
Iteration 96, loss = 0.00709009
Iteration 97, loss = 0.00692584
Iteration 98, loss = 0.00671064
Iteration 99, loss = 0.00653440
Iteration 100, loss = 0.00636201
Iteration 101, loss = 0.00622022
Iteration 102, loss = 0.00606650
Iteration 103, loss = 0.00586984
Iteration 104, loss = 0.00571731
Iteration 105, loss = 0.00555056
Iteration 106, loss = 0.00545481
Iteration 107, loss = 0.00532426
Iteration 108, loss = 0.00518922
Iteration 109, loss = 0.00509225
Iteration 110, loss = 0.00494202
Iteration 111, loss = 0.00483835
Iteration 112, loss = 0.00472818
Iteration 113, loss = 0.00460766
Iteration 114, loss = 0.00451092
Iteration 115, loss = 0.00440683
Iteration 116, loss = 0.00435143
Iteration 117, loss = 0.00421729
Iteration 118, loss = 0.00412904
Iteration 119, loss = 0.00403412
Iteration 120, loss = 0.00395413
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted: 100
Error rate: 23.08% ( 577/2500)
 - Class 1:  34, Class 2:  21, Class 3: 418, Class 4:  59, Class 5:  45
Iteration 1, loss = 1.57930151
Iteration 2, loss = 1.47491617
Iteration 3, loss = 1.39913570
Iteration 4, loss = 1.33579204
Iteration 5, loss = 1.27748645
Iteration 6, loss = 1.22271026
Iteration 7, loss = 1.17160093
Iteration 8, loss = 1.12365406
Iteration 9, loss = 1.07878624
Iteration 10, loss = 1.03653343
Iteration 11, loss = 0.99645782
Iteration 12, loss = 0.95811940
Iteration 13, loss = 0.92088096
Iteration 14, loss = 0.88490262
Iteration 15, loss = 0.85017841
Iteration 16, loss = 0.81665051
Iteration 17, loss = 0.78418419
Iteration 18, loss = 0.75302501
Iteration 19, loss = 0.72333308
Iteration 20, loss = 0.69525846
Iteration 21, loss = 0.66896964
Iteration 22, loss = 0.64452328
Iteration 23, loss = 0.62182586
Iteration 24, loss = 0.60034821
Iteration 25, loss = 0.58027426
Iteration 26, loss = 0.56131401
Iteration 27, loss = 0.54349149
Iteration 28, loss = 0.52655175
Iteration 29, loss = 0.51035804
Iteration 30, loss = 0.49533653
Iteration 31, loss = 0.48063502
Iteration 32, loss = 0.46682453
Iteration 33, loss = 0.45372021
Iteration 34, loss = 0.44124598
Iteration 35, loss = 0.42925437
Iteration 36, loss = 0.41776105
Iteration 37, loss = 0.40682465
Iteration 38, loss = 0.39644949
Iteration 39, loss = 0.38657197
Iteration 40, loss = 0.37679731
Iteration 41, loss = 0.36764195
Iteration 42, loss = 0.35883297
Iteration 43, loss = 0.35044756
Iteration 44, loss = 0.34235868
Iteration 45, loss = 0.33448196
Iteration 46, loss = 0.32710670
Iteration 47, loss = 0.31986218
Iteration 48, loss = 0.31320484
Iteration 49, loss = 0.30637981
Iteration 50, loss = 0.30009585
Iteration 51, loss = 0.29373001
Iteration 52, loss = 0.28790573
Iteration 53, loss = 0.28234568
Iteration 54, loss = 0.27661448
Iteration 55, loss = 0.27135732
Iteration 56, loss = 0.26630121
Iteration 57, loss = 0.26110408
Iteration 58, loss = 0.25615386
Iteration 59, loss = 0.25149806
Iteration 60, loss = 0.24706336
Iteration 61, loss = 0.24285823
Iteration 62, loss = 0.23858629
Iteration 63, loss = 0.23434083
Iteration 64, loss = 0.23022402
Iteration 65, loss = 0.22657841
Iteration 66, loss = 0.22282097
Iteration 67, loss = 0.21915044
Iteration 68, loss = 0.21559679
Iteration 69, loss = 0.21202237
Iteration 70, loss = 0.20871109
Iteration 71, loss = 0.20552517
Iteration 72, loss = 0.20221452
Iteration 73, loss = 0.19926601
Iteration 74, loss = 0.19609393
Iteration 75, loss = 0.19333609
Iteration 76, loss = 0.19046725
Iteration 77, loss = 0.18779850
Iteration 78, loss = 0.18491029
Iteration 79, loss = 0.18238919
Iteration 80, loss = 0.17974819
Iteration 81, loss = 0.17720980
Iteration 82, loss = 0.17468463
Iteration 83, loss = 0.17223984
Iteration 84, loss = 0.17001864
Iteration 85, loss = 0.16772944
Iteration 86, loss = 0.16566700
Iteration 87, loss = 0.16345809
Iteration 88, loss = 0.16132042
Iteration 89, loss = 0.15920758
Iteration 90, loss = 0.15716617
Iteration 91, loss = 0.15509663
Iteration 92, loss = 0.15300340
Iteration 93, loss = 0.15108922
Iteration 94, loss = 0.14924944
Iteration 95, loss = 0.14746529
Iteration 96, loss = 0.14582604
Iteration 97, loss = 0.14392435
Iteration 98, loss = 0.14229308
Iteration 99, loss = 0.14055252
Iteration 100, loss = 0.13905025
Iteration 101, loss = 0.13743008
Iteration 102, loss = 0.13581580
Iteration 103, loss = 0.13419466
Iteration 104, loss = 0.13266634
Iteration 105, loss = 0.13126609
Iteration 106, loss = 0.12978472
Iteration 107, loss = 0.12829394
Iteration 108, loss = 0.12691139
Iteration 109, loss = 0.12545273
Iteration 110, loss = 0.12417923
Iteration 111, loss = 0.12287145
Iteration 112, loss = 0.12155308
Iteration 113, loss = 0.12025574
Iteration 114, loss = 0.11895721
Iteration 115, loss = 0.11768481
Iteration 116, loss = 0.11631547
Iteration 117, loss = 0.11527643
Iteration 118, loss = 0.11418878
Iteration 119, loss = 0.11294313
Iteration 120, loss = 0.11171953
Iteration 121, loss = 0.11057098
Iteration 122, loss = 0.10940592
Iteration 123, loss = 0.10831676
Iteration 124, loss = 0.10734284
Iteration 125, loss = 0.10630347
Iteration 126, loss = 0.10525752
Iteration 127, loss = 0.10411949
Iteration 128, loss = 0.10305640
Iteration 129, loss = 0.10201138
Iteration 130, loss = 0.10086566
Iteration 131, loss = 0.09993179
Iteration 132, loss = 0.09887645
Iteration 133, loss = 0.09785951
Iteration 134, loss = 0.09676507
Iteration 135, loss = 0.09583309
Iteration 136, loss = 0.09485793
Iteration 137, loss = 0.09405235
Iteration 138, loss = 0.09306041
Iteration 139, loss = 0.09207082
Iteration 140, loss = 0.09120701
Iteration 141, loss = 0.09052183
Iteration 142, loss = 0.08957715
Iteration 143, loss = 0.08879196
Iteration 144, loss = 0.08797345
Iteration 145, loss = 0.08729346
Iteration 146, loss = 0.08647183
Iteration 147, loss = 0.08564069
Iteration 148, loss = 0.08486185
Iteration 149, loss = 0.08421179
Iteration 150, loss = 0.08347619
Iteration 151, loss = 0.08274105
Iteration 152, loss = 0.08202770
Iteration 153, loss = 0.08134180
Iteration 154, loss = 0.08067054
Iteration 155, loss = 0.07992392
Iteration 156, loss = 0.07923142
Iteration 157, loss = 0.07858926
Iteration 158, loss = 0.07783600
Iteration 159, loss = 0.07717932
Iteration 160, loss = 0.07654291
Iteration 161, loss = 0.07583658
Iteration 162, loss = 0.07533758
Iteration 163, loss = 0.07469931
Iteration 164, loss = 0.07399866
Iteration 165, loss = 0.07337442
Iteration 166, loss = 0.07285007
Iteration 167, loss = 0.07218428
Iteration 168, loss = 0.07158856
Iteration 169, loss = 0.07106441
Iteration 170, loss = 0.07048531
Iteration 171, loss = 0.06986733
Iteration 172, loss = 0.06939885
Iteration 173, loss = 0.06873864
Iteration 174, loss = 0.06814700
Iteration 175, loss = 0.06771934
Iteration 176, loss = 0.06724872
Iteration 177, loss = 0.06672234
Iteration 178, loss = 0.06609973
Iteration 179, loss = 0.06554040
Iteration 180, loss = 0.06516005
Iteration 181, loss = 0.06468595
Iteration 182, loss = 0.06407622
Iteration 183, loss = 0.06352722
Iteration 184, loss = 0.06312812
Iteration 185, loss = 0.06260790
Iteration 186, loss = 0.06224849
Iteration 187, loss = 0.06169362
Iteration 188, loss = 0.06127196
Iteration 189, loss = 0.06078529
Iteration 190, loss = 0.06044320
Iteration 191, loss = 0.05993025
Iteration 192, loss = 0.05948964
Iteration 193, loss = 0.05904833
Iteration 194, loss = 0.05860923
Iteration 195, loss = 0.05814917
Iteration 196, loss = 0.05778595
Iteration 197, loss = 0.05734151
Iteration 198, loss = 0.05705641
Iteration 199, loss = 0.05662772
Iteration 200, loss = 0.05617075
Number of features extracted:   5
Error rate: 8.60% ( 215/2500)
 - Class 1:  26, Class 2:  18, Class 3:  72, Class 4:  78, Class 5:  21
Iteration 1, loss = 1.51270027
Iteration 2, loss = 1.31173749
Iteration 3, loss = 1.16258830
Iteration 4, loss = 1.04222373
Iteration 5, loss = 0.94010647
Iteration 6, loss = 0.85116861
Iteration 7, loss = 0.77366614
Iteration 8, loss = 0.70622006
Iteration 9, loss = 0.64721694
Iteration 10, loss = 0.59534876
Iteration 11, loss = 0.54918697
Iteration 12, loss = 0.50823191
Iteration 13, loss = 0.47163754
Iteration 14, loss = 0.43944730
Iteration 15, loss = 0.41085529
Iteration 16, loss = 0.38529059
Iteration 17, loss = 0.36242418
Iteration 18, loss = 0.34201487
Iteration 19, loss = 0.32367227
Iteration 20, loss = 0.30709628
Iteration 21, loss = 0.29205398
Iteration 22, loss = 0.27863555
Iteration 23, loss = 0.26614903
Iteration 24, loss = 0.25450336
Iteration 25, loss = 0.24413891
Iteration 26, loss = 0.23434337
Iteration 27, loss = 0.22553244
Iteration 28, loss = 0.21719421
Iteration 29, loss = 0.20954437
Iteration 30, loss = 0.20252788
Iteration 31, loss = 0.19580161
Iteration 32, loss = 0.18957333
Iteration 33, loss = 0.18361118
Iteration 34, loss = 0.17817675
Iteration 35, loss = 0.17311619
Iteration 36, loss = 0.16815775
Iteration 37, loss = 0.16367815
Iteration 38, loss = 0.15919797
Iteration 39, loss = 0.15512404
Iteration 40, loss = 0.15128859
Iteration 41, loss = 0.14752340
Iteration 42, loss = 0.14413321
Iteration 43, loss = 0.14065176
Iteration 44, loss = 0.13729065
Iteration 45, loss = 0.13419344
Iteration 46, loss = 0.13119843
Iteration 47, loss = 0.12836469
Iteration 48, loss = 0.12565535
Iteration 49, loss = 0.12308119
Iteration 50, loss = 0.12062364
Iteration 51, loss = 0.11813464
Iteration 52, loss = 0.11576268
Iteration 53, loss = 0.11364866
Iteration 54, loss = 0.11120000
Iteration 55, loss = 0.10910998
Iteration 56, loss = 0.10692435
Iteration 57, loss = 0.10487087
Iteration 58, loss = 0.10287370
Iteration 59, loss = 0.10100855
Iteration 60, loss = 0.09910827
Iteration 61, loss = 0.09733717
Iteration 62, loss = 0.09583557
Iteration 63, loss = 0.09447830
Iteration 64, loss = 0.09224165
Iteration 65, loss = 0.09049129
Iteration 66, loss = 0.08897783
Iteration 67, loss = 0.08754671
Iteration 68, loss = 0.08597898
Iteration 69, loss = 0.08454067
Iteration 70, loss = 0.08316684
Iteration 71, loss = 0.08180067
Iteration 72, loss = 0.08046567
Iteration 73, loss = 0.07922857
Iteration 74, loss = 0.07800138
Iteration 75, loss = 0.07664047
Iteration 76, loss = 0.07539629
Iteration 77, loss = 0.07435506
Iteration 78, loss = 0.07329593
Iteration 79, loss = 0.07216147
Iteration 80, loss = 0.07106108
Iteration 81, loss = 0.06989276
Iteration 82, loss = 0.06884768
Iteration 83, loss = 0.06788133
Iteration 84, loss = 0.06680781
Iteration 85, loss = 0.06594769
Iteration 86, loss = 0.06488933
Iteration 87, loss = 0.06400870
Iteration 88, loss = 0.06326335
Iteration 89, loss = 0.06226806
Iteration 90, loss = 0.06132661
Iteration 91, loss = 0.06042536
Iteration 92, loss = 0.05979663
Iteration 93, loss = 0.05895668
Iteration 94, loss = 0.05802870
Iteration 95, loss = 0.05732724
Iteration 96, loss = 0.05643471
Iteration 97, loss = 0.05577812
Iteration 98, loss = 0.05503880
Iteration 99, loss = 0.05435580
Iteration 100, loss = 0.05358143
Iteration 101, loss = 0.05292035
Iteration 102, loss = 0.05228665
Iteration 103, loss = 0.05153745
Iteration 104, loss = 0.05089553
Iteration 105, loss = 0.05017025
Iteration 106, loss = 0.04962318
Iteration 107, loss = 0.04899400
Iteration 108, loss = 0.04840335
Iteration 109, loss = 0.04779618
Iteration 110, loss = 0.04719786
Iteration 111, loss = 0.04670880
Iteration 112, loss = 0.04609139
Iteration 113, loss = 0.04547259
Iteration 114, loss = 0.04488869
Iteration 115, loss = 0.04432455
Iteration 116, loss = 0.04376553
Iteration 117, loss = 0.04318743
Iteration 118, loss = 0.04273512
Iteration 119, loss = 0.04231182
Iteration 120, loss = 0.04180476
Iteration 121, loss = 0.04119500
Iteration 122, loss = 0.04068788
Iteration 123, loss = 0.04014518
Iteration 124, loss = 0.03979024
Iteration 125, loss = 0.03926223
Iteration 126, loss = 0.03887877
Iteration 127, loss = 0.03835109
Iteration 128, loss = 0.03788165
Iteration 129, loss = 0.03743723
Iteration 130, loss = 0.03704192
Iteration 131, loss = 0.03664737
Iteration 132, loss = 0.03619428
Iteration 133, loss = 0.03571015
Iteration 134, loss = 0.03535569
Iteration 135, loss = 0.03480078
Iteration 136, loss = 0.03446402
Iteration 137, loss = 0.03409222
Iteration 138, loss = 0.03356357
Iteration 139, loss = 0.03323375
Iteration 140, loss = 0.03280650
Iteration 141, loss = 0.03247225
Iteration 142, loss = 0.03207063
Iteration 143, loss = 0.03177905
Iteration 144, loss = 0.03133472
Iteration 145, loss = 0.03105122
Iteration 146, loss = 0.03065389
Iteration 147, loss = 0.03038934
Iteration 148, loss = 0.02993001
Iteration 149, loss = 0.02955352
Iteration 150, loss = 0.02923055
Iteration 151, loss = 0.02886012
Iteration 152, loss = 0.02853209
Iteration 153, loss = 0.02819257
Iteration 154, loss = 0.02787489
Iteration 155, loss = 0.02747400
Iteration 156, loss = 0.02717411
Iteration 157, loss = 0.02687186
Iteration 158, loss = 0.02649772
Iteration 159, loss = 0.02616185
Iteration 160, loss = 0.02588094
Iteration 161, loss = 0.02556492
Iteration 162, loss = 0.02524270
Iteration 163, loss = 0.02503244
Iteration 164, loss = 0.02476800
Iteration 165, loss = 0.02447646
Iteration 166, loss = 0.02412423
Iteration 167, loss = 0.02383526
Iteration 168, loss = 0.02361803
Iteration 169, loss = 0.02333768
Iteration 170, loss = 0.02304680
Iteration 171, loss = 0.02287102
Iteration 172, loss = 0.02259489
Iteration 173, loss = 0.02238315
Iteration 174, loss = 0.02219619
Iteration 175, loss = 0.02191794
Iteration 176, loss = 0.02166166
Iteration 177, loss = 0.02143556
Iteration 178, loss = 0.02122664
Iteration 179, loss = 0.02097537
Iteration 180, loss = 0.02075864
Iteration 181, loss = 0.02054305
Iteration 182, loss = 0.02036253
Iteration 183, loss = 0.02012078
Iteration 184, loss = 0.01993202
Iteration 185, loss = 0.01971275
Iteration 186, loss = 0.01953727
Iteration 187, loss = 0.01933320
Iteration 188, loss = 0.01913431
Iteration 189, loss = 0.01896868
Iteration 190, loss = 0.01876615
Iteration 191, loss = 0.01852368
Iteration 192, loss = 0.01837628
Iteration 193, loss = 0.01817099
Iteration 194, loss = 0.01798855
Iteration 195, loss = 0.01779186
Iteration 196, loss = 0.01760655
Iteration 197, loss = 0.01745854
Iteration 198, loss = 0.01726035
Iteration 199, loss = 0.01714274
Iteration 200, loss = 0.01693497
Number of features extracted:  10
Error rate: 23.76% ( 594/2500)
 - Class 1:  15, Class 2:  14, Class 3: 500, Class 4:  27, Class 5:  38
Iteration 1, loss = 1.46226197
Iteration 2, loss = 1.20582089
Iteration 3, loss = 1.02319621
Iteration 4, loss = 0.88189487
Iteration 5, loss = 0.76541663
Iteration 6, loss = 0.66862800
Iteration 7, loss = 0.58920230
Iteration 8, loss = 0.52323188
Iteration 9, loss = 0.46887095
Iteration 10, loss = 0.42400531
Iteration 11, loss = 0.38625703
Iteration 12, loss = 0.35453831
Iteration 13, loss = 0.32757644
Iteration 14, loss = 0.30431552
Iteration 15, loss = 0.28425523
Iteration 16, loss = 0.26669105
Iteration 17, loss = 0.25140875
Iteration 18, loss = 0.23757100
Iteration 19, loss = 0.22562314
Iteration 20, loss = 0.21453384
Iteration 21, loss = 0.20468864
Iteration 22, loss = 0.19562112
Iteration 23, loss = 0.18777277
Iteration 24, loss = 0.18029252
Iteration 25, loss = 0.17338056
Iteration 26, loss = 0.16674168
Iteration 27, loss = 0.16094002
Iteration 28, loss = 0.15568270
Iteration 29, loss = 0.15029739
Iteration 30, loss = 0.14581014
Iteration 31, loss = 0.14154266
Iteration 32, loss = 0.13683422
Iteration 33, loss = 0.13288124
Iteration 34, loss = 0.12914143
Iteration 35, loss = 0.12566512
Iteration 36, loss = 0.12220810
Iteration 37, loss = 0.11900258
Iteration 38, loss = 0.11607860
Iteration 39, loss = 0.11312496
Iteration 40, loss = 0.11024493
Iteration 41, loss = 0.10751670
Iteration 42, loss = 0.10506460
Iteration 43, loss = 0.10266483
Iteration 44, loss = 0.10013058
Iteration 45, loss = 0.09799150
Iteration 46, loss = 0.09599185
Iteration 47, loss = 0.09394837
Iteration 48, loss = 0.09158738
Iteration 49, loss = 0.08961760
Iteration 50, loss = 0.08786560
Iteration 51, loss = 0.08598949
Iteration 52, loss = 0.08419261
Iteration 53, loss = 0.08253036
Iteration 54, loss = 0.08074224
Iteration 55, loss = 0.07939128
Iteration 56, loss = 0.07768200
Iteration 57, loss = 0.07626134
Iteration 58, loss = 0.07467298
Iteration 59, loss = 0.07325328
Iteration 60, loss = 0.07191021
Iteration 61, loss = 0.07066411
Iteration 62, loss = 0.06934416
Iteration 63, loss = 0.06797459
Iteration 64, loss = 0.06679103
Iteration 65, loss = 0.06572510
Iteration 66, loss = 0.06447906
Iteration 67, loss = 0.06327916
Iteration 68, loss = 0.06238496
Iteration 69, loss = 0.06115983
Iteration 70, loss = 0.05999161
Iteration 71, loss = 0.05913860
Iteration 72, loss = 0.05810118
Iteration 73, loss = 0.05711899
Iteration 74, loss = 0.05617454
Iteration 75, loss = 0.05516397
Iteration 76, loss = 0.05434836
Iteration 77, loss = 0.05342410
Iteration 78, loss = 0.05258282
Iteration 79, loss = 0.05173193
Iteration 80, loss = 0.05086323
Iteration 81, loss = 0.05006983
Iteration 82, loss = 0.04936676
Iteration 83, loss = 0.04859036
Iteration 84, loss = 0.04774537
Iteration 85, loss = 0.04699155
Iteration 86, loss = 0.04626709
Iteration 87, loss = 0.04553994
Iteration 88, loss = 0.04485211
Iteration 89, loss = 0.04412284
Iteration 90, loss = 0.04347777
Iteration 91, loss = 0.04295404
Iteration 92, loss = 0.04224074
Iteration 93, loss = 0.04150917
Iteration 94, loss = 0.04080764
Iteration 95, loss = 0.04025531
Iteration 96, loss = 0.03973364
Iteration 97, loss = 0.03901835
Iteration 98, loss = 0.03848894
Iteration 99, loss = 0.03789715
Iteration 100, loss = 0.03731258
Iteration 101, loss = 0.03683985
Iteration 102, loss = 0.03634622
Iteration 103, loss = 0.03573367
Iteration 104, loss = 0.03521612
Iteration 105, loss = 0.03466024
Iteration 106, loss = 0.03410036
Iteration 107, loss = 0.03355942
Iteration 108, loss = 0.03313952
Iteration 109, loss = 0.03268425
Iteration 110, loss = 0.03223557
Iteration 111, loss = 0.03162288
Iteration 112, loss = 0.03120528
Iteration 113, loss = 0.03070960
Iteration 114, loss = 0.03028689
Iteration 115, loss = 0.02981167
Iteration 116, loss = 0.02933296
Iteration 117, loss = 0.02894679
Iteration 118, loss = 0.02856768
Iteration 119, loss = 0.02808587
Iteration 120, loss = 0.02769230
Iteration 121, loss = 0.02731620
Iteration 122, loss = 0.02692455
Iteration 123, loss = 0.02642865
Iteration 124, loss = 0.02611299
Iteration 125, loss = 0.02583023
Iteration 126, loss = 0.02538117
Iteration 127, loss = 0.02498066
Iteration 128, loss = 0.02473055
Iteration 129, loss = 0.02445899
Iteration 130, loss = 0.02398036
Iteration 131, loss = 0.02363968
Iteration 132, loss = 0.02332409
Iteration 133, loss = 0.02300310
Iteration 134, loss = 0.02269093
Iteration 135, loss = 0.02243108
Iteration 136, loss = 0.02208493
Iteration 137, loss = 0.02176145
Iteration 138, loss = 0.02150580
Iteration 139, loss = 0.02127503
Iteration 140, loss = 0.02102109
Iteration 141, loss = 0.02071743
Iteration 142, loss = 0.02038907
Iteration 143, loss = 0.02010615
Iteration 144, loss = 0.01993983
Iteration 145, loss = 0.01958567
Iteration 146, loss = 0.01930341
Iteration 147, loss = 0.01905969
Iteration 148, loss = 0.01881621
Iteration 149, loss = 0.01860896
Iteration 150, loss = 0.01839442
Iteration 151, loss = 0.01813160
Iteration 152, loss = 0.01790835
Iteration 153, loss = 0.01770321
Iteration 154, loss = 0.01746981
Iteration 155, loss = 0.01723947
Iteration 156, loss = 0.01701824
Iteration 157, loss = 0.01679975
Iteration 158, loss = 0.01658275
Iteration 159, loss = 0.01637498
Iteration 160, loss = 0.01619690
Iteration 161, loss = 0.01599513
Iteration 162, loss = 0.01577147
Iteration 163, loss = 0.01557706
Iteration 164, loss = 0.01542677
Iteration 165, loss = 0.01520940
Iteration 166, loss = 0.01505007
Iteration 167, loss = 0.01492979
Iteration 168, loss = 0.01469135
Iteration 169, loss = 0.01449318
Iteration 170, loss = 0.01432582
Iteration 171, loss = 0.01416774
Iteration 172, loss = 0.01403907
Iteration 173, loss = 0.01387467
Iteration 174, loss = 0.01370577
Iteration 175, loss = 0.01359984
Iteration 176, loss = 0.01340603
Iteration 177, loss = 0.01322004
Iteration 178, loss = 0.01305832
Iteration 179, loss = 0.01293352
Iteration 180, loss = 0.01280619
Iteration 181, loss = 0.01263756
Iteration 182, loss = 0.01251614
Iteration 183, loss = 0.01239417
Iteration 184, loss = 0.01225853
Iteration 185, loss = 0.01212035
Iteration 186, loss = 0.01198458
Iteration 187, loss = 0.01184005
Iteration 188, loss = 0.01173078
Iteration 189, loss = 0.01160600
Iteration 190, loss = 0.01147557
Iteration 191, loss = 0.01134850
Iteration 192, loss = 0.01121826
Iteration 193, loss = 0.01108079
Iteration 194, loss = 0.01097855
Iteration 195, loss = 0.01086972
Iteration 196, loss = 0.01074627
Iteration 197, loss = 0.01063541
Iteration 198, loss = 0.01053125
Iteration 199, loss = 0.01040121
Iteration 200, loss = 0.01028828
Number of features extracted:  15
Error rate: 9.40% ( 235/2500)
 - Class 1:  27, Class 2:  17, Class 3:  91, Class 4:  59, Class 5:  41
Iteration 1, loss = 1.45537017
Iteration 2, loss = 1.17226282
Iteration 3, loss = 0.96893079
Iteration 4, loss = 0.81074307
Iteration 5, loss = 0.68352310
Iteration 6, loss = 0.58166500
Iteration 7, loss = 0.50125277
Iteration 8, loss = 0.43763452
Iteration 9, loss = 0.38687674
Iteration 10, loss = 0.34659511
Iteration 11, loss = 0.31325435
Iteration 12, loss = 0.28607865
Iteration 13, loss = 0.26328596
Iteration 14, loss = 0.24419368
Iteration 15, loss = 0.22797754
Iteration 16, loss = 0.21382963
Iteration 17, loss = 0.20165537
Iteration 18, loss = 0.19063087
Iteration 19, loss = 0.18109558
Iteration 20, loss = 0.17245843
Iteration 21, loss = 0.16477705
Iteration 22, loss = 0.15791304
Iteration 23, loss = 0.15171631
Iteration 24, loss = 0.14590911
Iteration 25, loss = 0.14050888
Iteration 26, loss = 0.13544740
Iteration 27, loss = 0.13085288
Iteration 28, loss = 0.12647044
Iteration 29, loss = 0.12250839
Iteration 30, loss = 0.11874147
Iteration 31, loss = 0.11528675
Iteration 32, loss = 0.11178495
Iteration 33, loss = 0.10853965
Iteration 34, loss = 0.10569450
Iteration 35, loss = 0.10286336
Iteration 36, loss = 0.10017762
Iteration 37, loss = 0.09761372
Iteration 38, loss = 0.09511942
Iteration 39, loss = 0.09250422
Iteration 40, loss = 0.09031698
Iteration 41, loss = 0.08822804
Iteration 42, loss = 0.08609112
Iteration 43, loss = 0.08412951
Iteration 44, loss = 0.08238471
Iteration 45, loss = 0.08039251
Iteration 46, loss = 0.07851934
Iteration 47, loss = 0.07663087
Iteration 48, loss = 0.07491938
Iteration 49, loss = 0.07344608
Iteration 50, loss = 0.07191454
Iteration 51, loss = 0.07018944
Iteration 52, loss = 0.06871349
Iteration 53, loss = 0.06729043
Iteration 54, loss = 0.06593708
Iteration 55, loss = 0.06459977
Iteration 56, loss = 0.06327123
Iteration 57, loss = 0.06191679
Iteration 58, loss = 0.06084350
Iteration 59, loss = 0.05952791
Iteration 60, loss = 0.05842540
Iteration 61, loss = 0.05728677
Iteration 62, loss = 0.05627739
Iteration 63, loss = 0.05500871
Iteration 64, loss = 0.05400577
Iteration 65, loss = 0.05300675
Iteration 66, loss = 0.05197192
Iteration 67, loss = 0.05090166
Iteration 68, loss = 0.05005962
Iteration 69, loss = 0.04916502
Iteration 70, loss = 0.04810499
Iteration 71, loss = 0.04733926
Iteration 72, loss = 0.04643065
Iteration 73, loss = 0.04550465
Iteration 74, loss = 0.04489406
Iteration 75, loss = 0.04410162
Iteration 76, loss = 0.04315683
Iteration 77, loss = 0.04239843
Iteration 78, loss = 0.04177502
Iteration 79, loss = 0.04097371
Iteration 80, loss = 0.04011871
Iteration 81, loss = 0.03952252
Iteration 82, loss = 0.03873104
Iteration 83, loss = 0.03807381
Iteration 84, loss = 0.03755138
Iteration 85, loss = 0.03677990
Iteration 86, loss = 0.03624683
Iteration 87, loss = 0.03561743
Iteration 88, loss = 0.03509295
Iteration 89, loss = 0.03437486
Iteration 90, loss = 0.03378385
Iteration 91, loss = 0.03317005
Iteration 92, loss = 0.03272279
Iteration 93, loss = 0.03212653
Iteration 94, loss = 0.03146362
Iteration 95, loss = 0.03101773
Iteration 96, loss = 0.03052018
Iteration 97, loss = 0.02991408
Iteration 98, loss = 0.02947254
Iteration 99, loss = 0.02892602
Iteration 100, loss = 0.02847016
Iteration 101, loss = 0.02797490
Iteration 102, loss = 0.02748861
Iteration 103, loss = 0.02706406
Iteration 104, loss = 0.02667689
Iteration 105, loss = 0.02612377
Iteration 106, loss = 0.02571578
Iteration 107, loss = 0.02527805
Iteration 108, loss = 0.02487747
Iteration 109, loss = 0.02452662
Iteration 110, loss = 0.02415734
Iteration 111, loss = 0.02367764
Iteration 112, loss = 0.02333968
Iteration 113, loss = 0.02292042
Iteration 114, loss = 0.02259052
Iteration 115, loss = 0.02221728
Iteration 116, loss = 0.02186700
Iteration 117, loss = 0.02155968
Iteration 118, loss = 0.02122723
Iteration 119, loss = 0.02088486
Iteration 120, loss = 0.02058184
Iteration 121, loss = 0.02017530
Iteration 122, loss = 0.01981297
Iteration 123, loss = 0.01961325
Iteration 124, loss = 0.01934020
Iteration 125, loss = 0.01896917
Iteration 126, loss = 0.01871789
Iteration 127, loss = 0.01847757
Iteration 128, loss = 0.01814547
Iteration 129, loss = 0.01791486
Iteration 130, loss = 0.01759029
Iteration 131, loss = 0.01736590
Iteration 132, loss = 0.01707427
Iteration 133, loss = 0.01685994
Iteration 134, loss = 0.01657073
Iteration 135, loss = 0.01631285
Iteration 136, loss = 0.01608939
Iteration 137, loss = 0.01584050
Iteration 138, loss = 0.01555726
Iteration 139, loss = 0.01540662
Iteration 140, loss = 0.01514417
Iteration 141, loss = 0.01490389
Iteration 142, loss = 0.01469775
Iteration 143, loss = 0.01448737
Iteration 144, loss = 0.01428660
Iteration 145, loss = 0.01404163
Iteration 146, loss = 0.01389421
Iteration 147, loss = 0.01368747
Iteration 148, loss = 0.01347444
Iteration 149, loss = 0.01325209
Iteration 150, loss = 0.01305090
Iteration 151, loss = 0.01288448
Iteration 152, loss = 0.01267165
Iteration 153, loss = 0.01248132
Iteration 154, loss = 0.01226557
Iteration 155, loss = 0.01210773
Iteration 156, loss = 0.01193798
Iteration 157, loss = 0.01175699
Iteration 158, loss = 0.01158823
Iteration 159, loss = 0.01145686
Iteration 160, loss = 0.01130051
Iteration 161, loss = 0.01112267
Iteration 162, loss = 0.01093782
Iteration 163, loss = 0.01082505
Iteration 164, loss = 0.01065885
Iteration 165, loss = 0.01052412
Iteration 166, loss = 0.01035640
Iteration 167, loss = 0.01020354
Iteration 168, loss = 0.01004865
Iteration 169, loss = 0.00998182
Iteration 170, loss = 0.00980030
Iteration 171, loss = 0.00968719
Iteration 172, loss = 0.00956268
Iteration 173, loss = 0.00944546
Iteration 174, loss = 0.00929168
Iteration 175, loss = 0.00916857
Iteration 176, loss = 0.00903716
Iteration 177, loss = 0.00892380
Iteration 178, loss = 0.00881013
Iteration 179, loss = 0.00871938
Iteration 180, loss = 0.00857795
Iteration 181, loss = 0.00848742
Iteration 182, loss = 0.00837391
Iteration 183, loss = 0.00825675
Iteration 184, loss = 0.00818193
Iteration 185, loss = 0.00808061
Iteration 186, loss = 0.00794762
Iteration 187, loss = 0.00785939
Iteration 188, loss = 0.00776511
Iteration 189, loss = 0.00767640
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  20
Error rate: 11.88% ( 297/2500)
 - Class 1:  23, Class 2:  16, Class 3: 139, Class 4:  41, Class 5:  78
Iteration 1, loss = 1.44948256
Iteration 2, loss = 1.09590317
Iteration 3, loss = 0.87385979
Iteration 4, loss = 0.71452690
Iteration 5, loss = 0.59283009
Iteration 6, loss = 0.49953474
Iteration 7, loss = 0.42743321
Iteration 8, loss = 0.37193737
Iteration 9, loss = 0.32838239
Iteration 10, loss = 0.29357646
Iteration 11, loss = 0.26613297
Iteration 12, loss = 0.24317098
Iteration 13, loss = 0.22430923
Iteration 14, loss = 0.20848111
Iteration 15, loss = 0.19492432
Iteration 16, loss = 0.18324505
Iteration 17, loss = 0.17304897
Iteration 18, loss = 0.16390839
Iteration 19, loss = 0.15620464
Iteration 20, loss = 0.14895363
Iteration 21, loss = 0.14242837
Iteration 22, loss = 0.13661477
Iteration 23, loss = 0.13125353
Iteration 24, loss = 0.12631598
Iteration 25, loss = 0.12188887
Iteration 26, loss = 0.11747976
Iteration 27, loss = 0.11370749
Iteration 28, loss = 0.10993269
Iteration 29, loss = 0.10635515
Iteration 30, loss = 0.10323115
Iteration 31, loss = 0.10008850
Iteration 32, loss = 0.09695615
Iteration 33, loss = 0.09409104
Iteration 34, loss = 0.09153718
Iteration 35, loss = 0.08899117
Iteration 36, loss = 0.08644072
Iteration 37, loss = 0.08416465
Iteration 38, loss = 0.08201736
Iteration 39, loss = 0.07965378
Iteration 40, loss = 0.07752749
Iteration 41, loss = 0.07581908
Iteration 42, loss = 0.07406028
Iteration 43, loss = 0.07171405
Iteration 44, loss = 0.06985635
Iteration 45, loss = 0.06810515
Iteration 46, loss = 0.06656401
Iteration 47, loss = 0.06514957
Iteration 48, loss = 0.06359189
Iteration 49, loss = 0.06214515
Iteration 50, loss = 0.06050048
Iteration 51, loss = 0.05923357
Iteration 52, loss = 0.05796788
Iteration 53, loss = 0.05654796
Iteration 54, loss = 0.05531867
Iteration 55, loss = 0.05405055
Iteration 56, loss = 0.05293959
Iteration 57, loss = 0.05180495
Iteration 58, loss = 0.05065162
Iteration 59, loss = 0.04973107
Iteration 60, loss = 0.04847649
Iteration 61, loss = 0.04749448
Iteration 62, loss = 0.04649662
Iteration 63, loss = 0.04558477
Iteration 64, loss = 0.04486174
Iteration 65, loss = 0.04388015
Iteration 66, loss = 0.04290388
Iteration 67, loss = 0.04224196
Iteration 68, loss = 0.04118019
Iteration 69, loss = 0.04045280
Iteration 70, loss = 0.03962707
Iteration 71, loss = 0.03887330
Iteration 72, loss = 0.03826175
Iteration 73, loss = 0.03744612
Iteration 74, loss = 0.03682293
Iteration 75, loss = 0.03590089
Iteration 76, loss = 0.03525949
Iteration 77, loss = 0.03455041
Iteration 78, loss = 0.03397747
Iteration 79, loss = 0.03328526
Iteration 80, loss = 0.03256900
Iteration 81, loss = 0.03198410
Iteration 82, loss = 0.03146356
Iteration 83, loss = 0.03078320
Iteration 84, loss = 0.03025434
Iteration 85, loss = 0.02968976
Iteration 86, loss = 0.02908267
Iteration 87, loss = 0.02867822
Iteration 88, loss = 0.02817107
Iteration 89, loss = 0.02762791
Iteration 90, loss = 0.02723887
Iteration 91, loss = 0.02669818
Iteration 92, loss = 0.02620962
Iteration 93, loss = 0.02576198
Iteration 94, loss = 0.02527307
Iteration 95, loss = 0.02472625
Iteration 96, loss = 0.02434616
Iteration 97, loss = 0.02386593
Iteration 98, loss = 0.02345638
Iteration 99, loss = 0.02308041
Iteration 100, loss = 0.02258834
Iteration 101, loss = 0.02223980
Iteration 102, loss = 0.02180500
Iteration 103, loss = 0.02145992
Iteration 104, loss = 0.02109471
Iteration 105, loss = 0.02070406
Iteration 106, loss = 0.02038411
Iteration 107, loss = 0.02004955
Iteration 108, loss = 0.01975561
Iteration 109, loss = 0.01934206
Iteration 110, loss = 0.01903984
Iteration 111, loss = 0.01872579
Iteration 112, loss = 0.01835552
Iteration 113, loss = 0.01809649
Iteration 114, loss = 0.01781511
Iteration 115, loss = 0.01753512
Iteration 116, loss = 0.01719802
Iteration 117, loss = 0.01685661
Iteration 118, loss = 0.01654877
Iteration 119, loss = 0.01627943
Iteration 120, loss = 0.01602293
Iteration 121, loss = 0.01579982
Iteration 122, loss = 0.01545549
Iteration 123, loss = 0.01516975
Iteration 124, loss = 0.01496408
Iteration 125, loss = 0.01470146
Iteration 126, loss = 0.01443765
Iteration 127, loss = 0.01416251
Iteration 128, loss = 0.01394612
Iteration 129, loss = 0.01374425
Iteration 130, loss = 0.01345984
Iteration 131, loss = 0.01325576
Iteration 132, loss = 0.01299393
Iteration 133, loss = 0.01277835
Iteration 134, loss = 0.01253089
Iteration 135, loss = 0.01230409
Iteration 136, loss = 0.01217463
Iteration 137, loss = 0.01185791
Iteration 138, loss = 0.01172162
Iteration 139, loss = 0.01150024
Iteration 140, loss = 0.01128616
Iteration 141, loss = 0.01110883
Iteration 142, loss = 0.01095105
Iteration 143, loss = 0.01079291
Iteration 144, loss = 0.01058817
Iteration 145, loss = 0.01040564
Iteration 146, loss = 0.01028987
Iteration 147, loss = 0.01007402
Iteration 148, loss = 0.00995179
Iteration 149, loss = 0.00975187
Iteration 150, loss = 0.00958050
Iteration 151, loss = 0.00943944
Iteration 152, loss = 0.00930021
Iteration 153, loss = 0.00912070
Iteration 154, loss = 0.00897334
Iteration 155, loss = 0.00884873
Iteration 156, loss = 0.00869465
Iteration 157, loss = 0.00858132
Iteration 158, loss = 0.00846445
Iteration 159, loss = 0.00830591
Iteration 160, loss = 0.00814707
Iteration 161, loss = 0.00804174
Iteration 162, loss = 0.00793832
Iteration 163, loss = 0.00779034
Iteration 164, loss = 0.00769252
Iteration 165, loss = 0.00759538
Iteration 166, loss = 0.00746216
Iteration 167, loss = 0.00734291
Iteration 168, loss = 0.00725194
Iteration 169, loss = 0.00714918
Iteration 170, loss = 0.00704646
Iteration 171, loss = 0.00690896
Iteration 172, loss = 0.00680380
Iteration 173, loss = 0.00672793
Iteration 174, loss = 0.00663111
Iteration 175, loss = 0.00653073
Iteration 176, loss = 0.00645818
Iteration 177, loss = 0.00633991
Iteration 178, loss = 0.00627095
Iteration 179, loss = 0.00618534
Iteration 180, loss = 0.00611091
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  25
Error rate: 8.52% ( 213/2500)
 - Class 1:  13, Class 2:  15, Class 3:  94, Class 4:  51, Class 5:  40
Iteration 1, loss = 1.43975517
Iteration 2, loss = 1.10994576
Iteration 3, loss = 0.86852338
Iteration 4, loss = 0.68811339
Iteration 5, loss = 0.55514459
Iteration 6, loss = 0.45691458
Iteration 7, loss = 0.38499167
Iteration 8, loss = 0.33103433
Iteration 9, loss = 0.29040174
Iteration 10, loss = 0.25898166
Iteration 11, loss = 0.23406417
Iteration 12, loss = 0.21331528
Iteration 13, loss = 0.19690155
Iteration 14, loss = 0.18297937
Iteration 15, loss = 0.17130494
Iteration 16, loss = 0.16117104
Iteration 17, loss = 0.15229495
Iteration 18, loss = 0.14467133
Iteration 19, loss = 0.13756666
Iteration 20, loss = 0.13148072
Iteration 21, loss = 0.12583568
Iteration 22, loss = 0.12099508
Iteration 23, loss = 0.11626458
Iteration 24, loss = 0.11217647
Iteration 25, loss = 0.10803805
Iteration 26, loss = 0.10404899
Iteration 27, loss = 0.10066660
Iteration 28, loss = 0.09814403
Iteration 29, loss = 0.09423180
Iteration 30, loss = 0.09140251
Iteration 31, loss = 0.08870216
Iteration 32, loss = 0.08620695
Iteration 33, loss = 0.08361278
Iteration 34, loss = 0.08133447
Iteration 35, loss = 0.07870891
Iteration 36, loss = 0.07658652
Iteration 37, loss = 0.07454495
Iteration 38, loss = 0.07262255
Iteration 39, loss = 0.07060953
Iteration 40, loss = 0.06907744
Iteration 41, loss = 0.06729161
Iteration 42, loss = 0.06537192
Iteration 43, loss = 0.06388855
Iteration 44, loss = 0.06229217
Iteration 45, loss = 0.06079969
Iteration 46, loss = 0.05928217
Iteration 47, loss = 0.05791922
Iteration 48, loss = 0.05667520
Iteration 49, loss = 0.05528496
Iteration 50, loss = 0.05397349
Iteration 51, loss = 0.05262584
Iteration 52, loss = 0.05156535
Iteration 53, loss = 0.05044703
Iteration 54, loss = 0.04927687
Iteration 55, loss = 0.04801411
Iteration 56, loss = 0.04711162
Iteration 57, loss = 0.04606501
Iteration 58, loss = 0.04528309
Iteration 59, loss = 0.04430462
Iteration 60, loss = 0.04335106
Iteration 61, loss = 0.04243599
Iteration 62, loss = 0.04140451
Iteration 63, loss = 0.04060258
Iteration 64, loss = 0.03975991
Iteration 65, loss = 0.03891371
Iteration 66, loss = 0.03813411
Iteration 67, loss = 0.03727619
Iteration 68, loss = 0.03659056
Iteration 69, loss = 0.03591529
Iteration 70, loss = 0.03510866
Iteration 71, loss = 0.03446967
Iteration 72, loss = 0.03367479
Iteration 73, loss = 0.03310015
Iteration 74, loss = 0.03234938
Iteration 75, loss = 0.03168521
Iteration 76, loss = 0.03116165
Iteration 77, loss = 0.03057707
Iteration 78, loss = 0.03000805
Iteration 79, loss = 0.02936899
Iteration 80, loss = 0.02886862
Iteration 81, loss = 0.02828714
Iteration 82, loss = 0.02767810
Iteration 83, loss = 0.02714742
Iteration 84, loss = 0.02653050
Iteration 85, loss = 0.02606310
Iteration 86, loss = 0.02555505
Iteration 87, loss = 0.02510403
Iteration 88, loss = 0.02470862
Iteration 89, loss = 0.02412761
Iteration 90, loss = 0.02358707
Iteration 91, loss = 0.02325071
Iteration 92, loss = 0.02277021
Iteration 93, loss = 0.02233087
Iteration 94, loss = 0.02195235
Iteration 95, loss = 0.02154685
Iteration 96, loss = 0.02110599
Iteration 97, loss = 0.02063328
Iteration 98, loss = 0.02033450
Iteration 99, loss = 0.01986271
Iteration 100, loss = 0.01947753
Iteration 101, loss = 0.01917678
Iteration 102, loss = 0.01881287
Iteration 103, loss = 0.01847728
Iteration 104, loss = 0.01813533
Iteration 105, loss = 0.01779139
Iteration 106, loss = 0.01740457
Iteration 107, loss = 0.01712246
Iteration 108, loss = 0.01693738
Iteration 109, loss = 0.01665593
Iteration 110, loss = 0.01633219
Iteration 111, loss = 0.01583330
Iteration 112, loss = 0.01552582
Iteration 113, loss = 0.01531806
Iteration 114, loss = 0.01501369
Iteration 115, loss = 0.01470913
Iteration 116, loss = 0.01448653
Iteration 117, loss = 0.01416492
Iteration 118, loss = 0.01391016
Iteration 119, loss = 0.01374668
Iteration 120, loss = 0.01342382
Iteration 121, loss = 0.01319282
Iteration 122, loss = 0.01296591
Iteration 123, loss = 0.01272258
Iteration 124, loss = 0.01250369
Iteration 125, loss = 0.01226514
Iteration 126, loss = 0.01208876
Iteration 127, loss = 0.01185154
Iteration 128, loss = 0.01163831
Iteration 129, loss = 0.01145939
Iteration 130, loss = 0.01121703
Iteration 131, loss = 0.01105938
Iteration 132, loss = 0.01090477
Iteration 133, loss = 0.01066442
Iteration 134, loss = 0.01049387
Iteration 135, loss = 0.01032761
Iteration 136, loss = 0.01010974
Iteration 137, loss = 0.00996841
Iteration 138, loss = 0.00978943
Iteration 139, loss = 0.00963564
Iteration 140, loss = 0.00948649
Iteration 141, loss = 0.00928335
Iteration 142, loss = 0.00919581
Iteration 143, loss = 0.00899434
Iteration 144, loss = 0.00885695
Iteration 145, loss = 0.00868783
Iteration 146, loss = 0.00857194
Iteration 147, loss = 0.00844167
Iteration 148, loss = 0.00829535
Iteration 149, loss = 0.00816443
Iteration 150, loss = 0.00798471
Iteration 151, loss = 0.00789639
Iteration 152, loss = 0.00777131
Iteration 153, loss = 0.00764376
Iteration 154, loss = 0.00750086
Iteration 155, loss = 0.00740062
Iteration 156, loss = 0.00726272
Iteration 157, loss = 0.00714131
Iteration 158, loss = 0.00704501
Iteration 159, loss = 0.00693584
Iteration 160, loss = 0.00685938
Iteration 161, loss = 0.00673643
Iteration 162, loss = 0.00661996
Iteration 163, loss = 0.00650942
Iteration 164, loss = 0.00642924
Iteration 165, loss = 0.00631477
Iteration 166, loss = 0.00621823
Iteration 167, loss = 0.00612805
Iteration 168, loss = 0.00601150
Iteration 169, loss = 0.00590791
Iteration 170, loss = 0.00581431
Iteration 171, loss = 0.00571795
Iteration 172, loss = 0.00566766
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  30
Error rate: 11.20% ( 280/2500)
 - Class 1:  19, Class 2:  25, Class 3: 113, Class 4:  79, Class 5:  44
Iteration 1, loss = 1.38276491
Iteration 2, loss = 1.02146440
Iteration 3, loss = 0.77362046
Iteration 4, loss = 0.59943827
Iteration 5, loss = 0.47726101
Iteration 6, loss = 0.39172518
Iteration 7, loss = 0.33135305
Iteration 8, loss = 0.28675058
Iteration 9, loss = 0.25354444
Iteration 10, loss = 0.22760355
Iteration 11, loss = 0.20741643
Iteration 12, loss = 0.19079611
Iteration 13, loss = 0.17730612
Iteration 14, loss = 0.16598576
Iteration 15, loss = 0.15598978
Iteration 16, loss = 0.14735774
Iteration 17, loss = 0.13992325
Iteration 18, loss = 0.13346470
Iteration 19, loss = 0.12736257
Iteration 20, loss = 0.12185200
Iteration 21, loss = 0.11695290
Iteration 22, loss = 0.11243373
Iteration 23, loss = 0.10823787
Iteration 24, loss = 0.10447327
Iteration 25, loss = 0.10073584
Iteration 26, loss = 0.09741069
Iteration 27, loss = 0.09447021
Iteration 28, loss = 0.09123949
Iteration 29, loss = 0.08854092
Iteration 30, loss = 0.08600765
Iteration 31, loss = 0.08373783
Iteration 32, loss = 0.08067574
Iteration 33, loss = 0.07860108
Iteration 34, loss = 0.07667119
Iteration 35, loss = 0.07444866
Iteration 36, loss = 0.07229917
Iteration 37, loss = 0.07026906
Iteration 38, loss = 0.06871434
Iteration 39, loss = 0.06690746
Iteration 40, loss = 0.06494528
Iteration 41, loss = 0.06338515
Iteration 42, loss = 0.06169888
Iteration 43, loss = 0.06023556
Iteration 44, loss = 0.05849654
Iteration 45, loss = 0.05738516
Iteration 46, loss = 0.05597137
Iteration 47, loss = 0.05433715
Iteration 48, loss = 0.05302038
Iteration 49, loss = 0.05180537
Iteration 50, loss = 0.05068681
Iteration 51, loss = 0.04940289
Iteration 52, loss = 0.04833206
Iteration 53, loss = 0.04724552
Iteration 54, loss = 0.04608191
Iteration 55, loss = 0.04489553
Iteration 56, loss = 0.04397701
Iteration 57, loss = 0.04291097
Iteration 58, loss = 0.04222904
Iteration 59, loss = 0.04101079
Iteration 60, loss = 0.04003781
Iteration 61, loss = 0.03916929
Iteration 62, loss = 0.03837125
Iteration 63, loss = 0.03742252
Iteration 64, loss = 0.03665091
Iteration 65, loss = 0.03580261
Iteration 66, loss = 0.03518112
Iteration 67, loss = 0.03425449
Iteration 68, loss = 0.03360482
Iteration 69, loss = 0.03310223
Iteration 70, loss = 0.03214470
Iteration 71, loss = 0.03141033
Iteration 72, loss = 0.03078856
Iteration 73, loss = 0.02994675
Iteration 74, loss = 0.02962149
Iteration 75, loss = 0.02867553
Iteration 76, loss = 0.02806154
Iteration 77, loss = 0.02757611
Iteration 78, loss = 0.02701636
Iteration 79, loss = 0.02639562
Iteration 80, loss = 0.02581274
Iteration 81, loss = 0.02527535
Iteration 82, loss = 0.02468261
Iteration 83, loss = 0.02415194
Iteration 84, loss = 0.02359986
Iteration 85, loss = 0.02314998
Iteration 86, loss = 0.02263400
Iteration 87, loss = 0.02210294
Iteration 88, loss = 0.02166992
Iteration 89, loss = 0.02118486
Iteration 90, loss = 0.02074273
Iteration 91, loss = 0.02021004
Iteration 92, loss = 0.01981891
Iteration 93, loss = 0.01932562
Iteration 94, loss = 0.01891733
Iteration 95, loss = 0.01857355
Iteration 96, loss = 0.01816324
Iteration 97, loss = 0.01777517
Iteration 98, loss = 0.01737289
Iteration 99, loss = 0.01702451
Iteration 100, loss = 0.01668133
Iteration 101, loss = 0.01637888
Iteration 102, loss = 0.01599879
Iteration 103, loss = 0.01566239
Iteration 104, loss = 0.01535144
Iteration 105, loss = 0.01501824
Iteration 106, loss = 0.01468539
Iteration 107, loss = 0.01439985
Iteration 108, loss = 0.01409752
Iteration 109, loss = 0.01382223
Iteration 110, loss = 0.01347510
Iteration 111, loss = 0.01324082
Iteration 112, loss = 0.01294657
Iteration 113, loss = 0.01273884
Iteration 114, loss = 0.01247313
Iteration 115, loss = 0.01224952
Iteration 116, loss = 0.01198010
Iteration 117, loss = 0.01174791
Iteration 118, loss = 0.01143679
Iteration 119, loss = 0.01124604
Iteration 120, loss = 0.01103970
Iteration 121, loss = 0.01081456
Iteration 122, loss = 0.01065213
Iteration 123, loss = 0.01041136
Iteration 124, loss = 0.01025559
Iteration 125, loss = 0.01000909
Iteration 126, loss = 0.00982050
Iteration 127, loss = 0.00960042
Iteration 128, loss = 0.00944154
Iteration 129, loss = 0.00922274
Iteration 130, loss = 0.00907273
Iteration 131, loss = 0.00887397
Iteration 132, loss = 0.00872880
Iteration 133, loss = 0.00854850
Iteration 134, loss = 0.00839139
Iteration 135, loss = 0.00826864
Iteration 136, loss = 0.00813752
Iteration 137, loss = 0.00793202
Iteration 138, loss = 0.00781298
Iteration 139, loss = 0.00764359
Iteration 140, loss = 0.00752433
Iteration 141, loss = 0.00738963
Iteration 142, loss = 0.00725335
Iteration 143, loss = 0.00711993
Iteration 144, loss = 0.00696717
Iteration 145, loss = 0.00684734
Iteration 146, loss = 0.00673528
Iteration 147, loss = 0.00662919
Iteration 148, loss = 0.00652311
Iteration 149, loss = 0.00637802
Iteration 150, loss = 0.00624394
Iteration 151, loss = 0.00614055
Iteration 152, loss = 0.00604335
Iteration 153, loss = 0.00597057
Iteration 154, loss = 0.00585996
Iteration 155, loss = 0.00572955
Iteration 156, loss = 0.00562692
Iteration 157, loss = 0.00556514
Iteration 158, loss = 0.00545503
Iteration 159, loss = 0.00535511
Iteration 160, loss = 0.00527517
Iteration 161, loss = 0.00520158
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  35
Error rate: 24.44% ( 611/2500)
 - Class 1:  24, Class 2:  16, Class 3: 482, Class 4:  53, Class 5:  36
Iteration 1, loss = 1.37531639
Iteration 2, loss = 0.96073759
Iteration 3, loss = 0.70726302
Iteration 4, loss = 0.53994833
Iteration 5, loss = 0.42773500
Iteration 6, loss = 0.35145068
Iteration 7, loss = 0.29832026
Iteration 8, loss = 0.25916658
Iteration 9, loss = 0.22990647
Iteration 10, loss = 0.20778631
Iteration 11, loss = 0.18985832
Iteration 12, loss = 0.17528970
Iteration 13, loss = 0.16278437
Iteration 14, loss = 0.15269892
Iteration 15, loss = 0.14374585
Iteration 16, loss = 0.13636528
Iteration 17, loss = 0.12951580
Iteration 18, loss = 0.12367070
Iteration 19, loss = 0.11783035
Iteration 20, loss = 0.11309259
Iteration 21, loss = 0.10852477
Iteration 22, loss = 0.10453706
Iteration 23, loss = 0.10044268
Iteration 24, loss = 0.09709746
Iteration 25, loss = 0.09390343
Iteration 26, loss = 0.09053813
Iteration 27, loss = 0.08782939
Iteration 28, loss = 0.08492915
Iteration 29, loss = 0.08225082
Iteration 30, loss = 0.07975321
Iteration 31, loss = 0.07736339
Iteration 32, loss = 0.07531344
Iteration 33, loss = 0.07321040
Iteration 34, loss = 0.07087443
Iteration 35, loss = 0.06938649
Iteration 36, loss = 0.06745373
Iteration 37, loss = 0.06536743
Iteration 38, loss = 0.06366109
Iteration 39, loss = 0.06190397
Iteration 40, loss = 0.06020712
Iteration 41, loss = 0.05875442
Iteration 42, loss = 0.05711954
Iteration 43, loss = 0.05585555
Iteration 44, loss = 0.05455781
Iteration 45, loss = 0.05322663
Iteration 46, loss = 0.05202638
Iteration 47, loss = 0.05052421
Iteration 48, loss = 0.04927489
Iteration 49, loss = 0.04824982
Iteration 50, loss = 0.04741191
Iteration 51, loss = 0.04591761
Iteration 52, loss = 0.04483609
Iteration 53, loss = 0.04385362
Iteration 54, loss = 0.04283242
Iteration 55, loss = 0.04187138
Iteration 56, loss = 0.04084518
Iteration 57, loss = 0.03999279
Iteration 58, loss = 0.03912782
Iteration 59, loss = 0.03818368
Iteration 60, loss = 0.03739054
Iteration 61, loss = 0.03646226
Iteration 62, loss = 0.03557215
Iteration 63, loss = 0.03493696
Iteration 64, loss = 0.03415831
Iteration 65, loss = 0.03338554
Iteration 66, loss = 0.03266503
Iteration 67, loss = 0.03179932
Iteration 68, loss = 0.03123924
Iteration 69, loss = 0.03048991
Iteration 70, loss = 0.02986364
Iteration 71, loss = 0.02942809
Iteration 72, loss = 0.02854749
Iteration 73, loss = 0.02794063
Iteration 74, loss = 0.02738041
Iteration 75, loss = 0.02690239
Iteration 76, loss = 0.02616025
Iteration 77, loss = 0.02563294
Iteration 78, loss = 0.02535681
Iteration 79, loss = 0.02458635
Iteration 80, loss = 0.02402275
Iteration 81, loss = 0.02350250
Iteration 82, loss = 0.02305687
Iteration 83, loss = 0.02255229
Iteration 84, loss = 0.02206903
Iteration 85, loss = 0.02169329
Iteration 86, loss = 0.02117796
Iteration 87, loss = 0.02076552
Iteration 88, loss = 0.02028223
Iteration 89, loss = 0.01984286
Iteration 90, loss = 0.01948466
Iteration 91, loss = 0.01907680
Iteration 92, loss = 0.01861490
Iteration 93, loss = 0.01824978
Iteration 94, loss = 0.01794426
Iteration 95, loss = 0.01756677
Iteration 96, loss = 0.01717231
Iteration 97, loss = 0.01679531
Iteration 98, loss = 0.01641500
Iteration 99, loss = 0.01610868
Iteration 100, loss = 0.01573399
Iteration 101, loss = 0.01548636
Iteration 102, loss = 0.01511591
Iteration 103, loss = 0.01488414
Iteration 104, loss = 0.01457799
Iteration 105, loss = 0.01426216
Iteration 106, loss = 0.01399348
Iteration 107, loss = 0.01370036
Iteration 108, loss = 0.01344551
Iteration 109, loss = 0.01308920
Iteration 110, loss = 0.01284239
Iteration 111, loss = 0.01261892
Iteration 112, loss = 0.01239607
Iteration 113, loss = 0.01211411
Iteration 114, loss = 0.01186074
Iteration 115, loss = 0.01169699
Iteration 116, loss = 0.01140473
Iteration 117, loss = 0.01118507
Iteration 118, loss = 0.01099777
Iteration 119, loss = 0.01084698
Iteration 120, loss = 0.01057200
Iteration 121, loss = 0.01044755
Iteration 122, loss = 0.01013489
Iteration 123, loss = 0.01000515
Iteration 124, loss = 0.00973929
Iteration 125, loss = 0.00956235
Iteration 126, loss = 0.00941350
Iteration 127, loss = 0.00921037
Iteration 128, loss = 0.00907111
Iteration 129, loss = 0.00890035
Iteration 130, loss = 0.00870330
Iteration 131, loss = 0.00850804
Iteration 132, loss = 0.00836653
Iteration 133, loss = 0.00819121
Iteration 134, loss = 0.00807409
Iteration 135, loss = 0.00789338
Iteration 136, loss = 0.00776161
Iteration 137, loss = 0.00758761
Iteration 138, loss = 0.00741165
Iteration 139, loss = 0.00729543
Iteration 140, loss = 0.00714534
Iteration 141, loss = 0.00700529
Iteration 142, loss = 0.00689268
Iteration 143, loss = 0.00677876
Iteration 144, loss = 0.00664344
Iteration 145, loss = 0.00653397
Iteration 146, loss = 0.00638365
Iteration 147, loss = 0.00628572
Iteration 148, loss = 0.00616110
Iteration 149, loss = 0.00605034
Iteration 150, loss = 0.00592854
Iteration 151, loss = 0.00584193
Iteration 152, loss = 0.00574535
Iteration 153, loss = 0.00558859
Iteration 154, loss = 0.00546856
Iteration 155, loss = 0.00539486
Iteration 156, loss = 0.00530504
Iteration 157, loss = 0.00517549
Iteration 158, loss = 0.00509028
Iteration 159, loss = 0.00501776
Iteration 160, loss = 0.00492009
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  40
Error rate: 24.16% ( 604/2500)
 - Class 1:  35, Class 2:  18, Class 3: 490, Class 4:  28, Class 5:  33
Iteration 1, loss = 1.37916746
Iteration 2, loss = 0.96334606
Iteration 3, loss = 0.69321718
Iteration 4, loss = 0.51867862
Iteration 5, loss = 0.40515574
Iteration 6, loss = 0.33006350
Iteration 7, loss = 0.27920243
Iteration 8, loss = 0.24209109
Iteration 9, loss = 0.21481596
Iteration 10, loss = 0.19403866
Iteration 11, loss = 0.17735451
Iteration 12, loss = 0.16394863
Iteration 13, loss = 0.15301597
Iteration 14, loss = 0.14387866
Iteration 15, loss = 0.13546533
Iteration 16, loss = 0.12858662
Iteration 17, loss = 0.12214393
Iteration 18, loss = 0.11657084
Iteration 19, loss = 0.11162256
Iteration 20, loss = 0.10696038
Iteration 21, loss = 0.10269252
Iteration 22, loss = 0.09888614
Iteration 23, loss = 0.09526248
Iteration 24, loss = 0.09200324
Iteration 25, loss = 0.08864121
Iteration 26, loss = 0.08553352
Iteration 27, loss = 0.08264789
Iteration 28, loss = 0.08043234
Iteration 29, loss = 0.07783460
Iteration 30, loss = 0.07496991
Iteration 31, loss = 0.07278625
Iteration 32, loss = 0.07069347
Iteration 33, loss = 0.06864734
Iteration 34, loss = 0.06649697
Iteration 35, loss = 0.06466424
Iteration 36, loss = 0.06263662
Iteration 37, loss = 0.06110647
Iteration 38, loss = 0.05924829
Iteration 39, loss = 0.05760209
Iteration 40, loss = 0.05615282
Iteration 41, loss = 0.05444980
Iteration 42, loss = 0.05326838
Iteration 43, loss = 0.05158263
Iteration 44, loss = 0.05019495
Iteration 45, loss = 0.04874807
Iteration 46, loss = 0.04749064
Iteration 47, loss = 0.04629258
Iteration 48, loss = 0.04515096
Iteration 49, loss = 0.04387177
Iteration 50, loss = 0.04289513
Iteration 51, loss = 0.04182689
Iteration 52, loss = 0.04064252
Iteration 53, loss = 0.03957232
Iteration 54, loss = 0.03868709
Iteration 55, loss = 0.03776799
Iteration 56, loss = 0.03685530
Iteration 57, loss = 0.03579267
Iteration 58, loss = 0.03506839
Iteration 59, loss = 0.03436609
Iteration 60, loss = 0.03328897
Iteration 61, loss = 0.03276572
Iteration 62, loss = 0.03219020
Iteration 63, loss = 0.03112144
Iteration 64, loss = 0.03039650
Iteration 65, loss = 0.02974609
Iteration 66, loss = 0.02900433
Iteration 67, loss = 0.02833114
Iteration 68, loss = 0.02767190
Iteration 69, loss = 0.02707184
Iteration 70, loss = 0.02657800
Iteration 71, loss = 0.02583310
Iteration 72, loss = 0.02520366
Iteration 73, loss = 0.02468430
Iteration 74, loss = 0.02416151
Iteration 75, loss = 0.02363472
Iteration 76, loss = 0.02306132
Iteration 77, loss = 0.02246668
Iteration 78, loss = 0.02204054
Iteration 79, loss = 0.02151515
Iteration 80, loss = 0.02100931
Iteration 81, loss = 0.02064170
Iteration 82, loss = 0.02023321
Iteration 83, loss = 0.01970199
Iteration 84, loss = 0.01939104
Iteration 85, loss = 0.01900091
Iteration 86, loss = 0.01839342
Iteration 87, loss = 0.01806015
Iteration 88, loss = 0.01764745
Iteration 89, loss = 0.01718324
Iteration 90, loss = 0.01685776
Iteration 91, loss = 0.01641502
Iteration 92, loss = 0.01621936
Iteration 93, loss = 0.01586364
Iteration 94, loss = 0.01543417
Iteration 95, loss = 0.01514065
Iteration 96, loss = 0.01474605
Iteration 97, loss = 0.01445953
Iteration 98, loss = 0.01420989
Iteration 99, loss = 0.01379904
Iteration 100, loss = 0.01359375
Iteration 101, loss = 0.01341973
Iteration 102, loss = 0.01304229
Iteration 103, loss = 0.01277561
Iteration 104, loss = 0.01241199
Iteration 105, loss = 0.01217500
Iteration 106, loss = 0.01187621
Iteration 107, loss = 0.01169441
Iteration 108, loss = 0.01144465
Iteration 109, loss = 0.01121805
Iteration 110, loss = 0.01103264
Iteration 111, loss = 0.01075898
Iteration 112, loss = 0.01043707
Iteration 113, loss = 0.01024597
Iteration 114, loss = 0.01002377
Iteration 115, loss = 0.00991627
Iteration 116, loss = 0.00968388
Iteration 117, loss = 0.00947683
Iteration 118, loss = 0.00926182
Iteration 119, loss = 0.00912728
Iteration 120, loss = 0.00893968
Iteration 121, loss = 0.00874735
Iteration 122, loss = 0.00855203
Iteration 123, loss = 0.00838261
Iteration 124, loss = 0.00822218
Iteration 125, loss = 0.00805171
Iteration 126, loss = 0.00789826
Iteration 127, loss = 0.00774476
Iteration 128, loss = 0.00758946
Iteration 129, loss = 0.00746240
Iteration 130, loss = 0.00733810
Iteration 131, loss = 0.00719732
Iteration 132, loss = 0.00700896
Iteration 133, loss = 0.00687445
Iteration 134, loss = 0.00674275
Iteration 135, loss = 0.00659884
Iteration 136, loss = 0.00648789
Iteration 137, loss = 0.00636459
Iteration 138, loss = 0.00622344
Iteration 139, loss = 0.00612149
Iteration 140, loss = 0.00599259
Iteration 141, loss = 0.00586460
Iteration 142, loss = 0.00578740
Iteration 143, loss = 0.00566158
Iteration 144, loss = 0.00557037
Iteration 145, loss = 0.00549234
Iteration 146, loss = 0.00539178
Iteration 147, loss = 0.00526450
Iteration 148, loss = 0.00515068
Iteration 149, loss = 0.00509411
Iteration 150, loss = 0.00498534
Iteration 151, loss = 0.00488820
Iteration 152, loss = 0.00479890
Iteration 153, loss = 0.00471387
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  45
Error rate: 24.00% ( 600/2500)
 - Class 1:  44, Class 2:  17, Class 3: 496, Class 4:  32, Class 5:  11
Iteration 1, loss = 1.38043689
Iteration 2, loss = 0.93580312
Iteration 3, loss = 0.66508424
Iteration 4, loss = 0.49186244
Iteration 5, loss = 0.38200664
Iteration 6, loss = 0.31098217
Iteration 7, loss = 0.26257666
Iteration 8, loss = 0.22871729
Iteration 9, loss = 0.20350785
Iteration 10, loss = 0.18428644
Iteration 11, loss = 0.16898730
Iteration 12, loss = 0.15644892
Iteration 13, loss = 0.14602496
Iteration 14, loss = 0.13715811
Iteration 15, loss = 0.12975958
Iteration 16, loss = 0.12290058
Iteration 17, loss = 0.11697282
Iteration 18, loss = 0.11193192
Iteration 19, loss = 0.10682172
Iteration 20, loss = 0.10270477
Iteration 21, loss = 0.09868287
Iteration 22, loss = 0.09461589
Iteration 23, loss = 0.09092544
Iteration 24, loss = 0.08742675
Iteration 25, loss = 0.08474563
Iteration 26, loss = 0.08177208
Iteration 27, loss = 0.07871988
Iteration 28, loss = 0.07616047
Iteration 29, loss = 0.07369736
Iteration 30, loss = 0.07134503
Iteration 31, loss = 0.06968083
Iteration 32, loss = 0.06720052
Iteration 33, loss = 0.06508073
Iteration 34, loss = 0.06287412
Iteration 35, loss = 0.06098107
Iteration 36, loss = 0.05910755
Iteration 37, loss = 0.05743365
Iteration 38, loss = 0.05569449
Iteration 39, loss = 0.05442716
Iteration 40, loss = 0.05266906
Iteration 41, loss = 0.05148864
Iteration 42, loss = 0.04986066
Iteration 43, loss = 0.04846715
Iteration 44, loss = 0.04718290
Iteration 45, loss = 0.04588626
Iteration 46, loss = 0.04467298
Iteration 47, loss = 0.04334611
Iteration 48, loss = 0.04261100
Iteration 49, loss = 0.04147196
Iteration 50, loss = 0.04041966
Iteration 51, loss = 0.03933297
Iteration 52, loss = 0.03798394
Iteration 53, loss = 0.03706580
Iteration 54, loss = 0.03612163
Iteration 55, loss = 0.03519516
Iteration 56, loss = 0.03431318
Iteration 57, loss = 0.03347665
Iteration 58, loss = 0.03260400
Iteration 59, loss = 0.03187121
Iteration 60, loss = 0.03117223
Iteration 61, loss = 0.03035787
Iteration 62, loss = 0.02970493
Iteration 63, loss = 0.02872427
Iteration 64, loss = 0.02811145
Iteration 65, loss = 0.02740671
Iteration 66, loss = 0.02675103
Iteration 67, loss = 0.02596908
Iteration 68, loss = 0.02540892
Iteration 69, loss = 0.02482741
Iteration 70, loss = 0.02416855
Iteration 71, loss = 0.02362959
Iteration 72, loss = 0.02301688
Iteration 73, loss = 0.02249897
Iteration 74, loss = 0.02193835
Iteration 75, loss = 0.02145614
Iteration 76, loss = 0.02091078
Iteration 77, loss = 0.02037718
Iteration 78, loss = 0.01987442
Iteration 79, loss = 0.01941908
Iteration 80, loss = 0.01891182
Iteration 81, loss = 0.01852710
Iteration 82, loss = 0.01805099
Iteration 83, loss = 0.01755001
Iteration 84, loss = 0.01715816
Iteration 85, loss = 0.01686128
Iteration 86, loss = 0.01640266
Iteration 87, loss = 0.01595056
Iteration 88, loss = 0.01576320
Iteration 89, loss = 0.01520900
Iteration 90, loss = 0.01487935
Iteration 91, loss = 0.01452034
Iteration 92, loss = 0.01422579
Iteration 93, loss = 0.01388480
Iteration 94, loss = 0.01368441
Iteration 95, loss = 0.01333957
Iteration 96, loss = 0.01293421
Iteration 97, loss = 0.01260195
Iteration 98, loss = 0.01236806
Iteration 99, loss = 0.01204249
Iteration 100, loss = 0.01173463
Iteration 101, loss = 0.01146789
Iteration 102, loss = 0.01123588
Iteration 103, loss = 0.01096818
Iteration 104, loss = 0.01071972
Iteration 105, loss = 0.01044407
Iteration 106, loss = 0.01020896
Iteration 107, loss = 0.01003196
Iteration 108, loss = 0.00977563
Iteration 109, loss = 0.00952027
Iteration 110, loss = 0.00936204
Iteration 111, loss = 0.00915203
Iteration 112, loss = 0.00894296
Iteration 113, loss = 0.00868661
Iteration 114, loss = 0.00849517
Iteration 115, loss = 0.00831781
Iteration 116, loss = 0.00813483
Iteration 117, loss = 0.00794701
Iteration 118, loss = 0.00777965
Iteration 119, loss = 0.00761897
Iteration 120, loss = 0.00744906
Iteration 121, loss = 0.00723330
Iteration 122, loss = 0.00709897
Iteration 123, loss = 0.00690191
Iteration 124, loss = 0.00677291
Iteration 125, loss = 0.00666532
Iteration 126, loss = 0.00650783
Iteration 127, loss = 0.00634238
Iteration 128, loss = 0.00623948
Iteration 129, loss = 0.00610623
Iteration 130, loss = 0.00598591
Iteration 131, loss = 0.00588051
Iteration 132, loss = 0.00574089
Iteration 133, loss = 0.00561316
Iteration 134, loss = 0.00552836
Iteration 135, loss = 0.00541573
Iteration 136, loss = 0.00531570
Iteration 137, loss = 0.00521960
Iteration 138, loss = 0.00508791
Iteration 139, loss = 0.00503342
Iteration 140, loss = 0.00491614
Iteration 141, loss = 0.00485316
Iteration 142, loss = 0.00474494
Iteration 143, loss = 0.00465716
Iteration 144, loss = 0.00457389
Iteration 145, loss = 0.00450652
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  50
Error rate: 24.08% ( 602/2500)
 - Class 1:  40, Class 2:  19, Class 3: 486, Class 4:  39, Class 5:  18
Iteration 1, loss = 1.33908282
Iteration 2, loss = 0.88595841
Iteration 3, loss = 0.61166591
Iteration 4, loss = 0.44823790
Iteration 5, loss = 0.34958001
Iteration 6, loss = 0.28630734
Iteration 7, loss = 0.24357324
Iteration 8, loss = 0.21334167
Iteration 9, loss = 0.19016604
Iteration 10, loss = 0.17278051
Iteration 11, loss = 0.15864443
Iteration 12, loss = 0.14697906
Iteration 13, loss = 0.13727192
Iteration 14, loss = 0.12927156
Iteration 15, loss = 0.12187894
Iteration 16, loss = 0.11578793
Iteration 17, loss = 0.11012209
Iteration 18, loss = 0.10508985
Iteration 19, loss = 0.10064165
Iteration 20, loss = 0.09644196
Iteration 21, loss = 0.09269627
Iteration 22, loss = 0.08878833
Iteration 23, loss = 0.08562043
Iteration 24, loss = 0.08235261
Iteration 25, loss = 0.07933851
Iteration 26, loss = 0.07653576
Iteration 27, loss = 0.07364427
Iteration 28, loss = 0.07180146
Iteration 29, loss = 0.06942535
Iteration 30, loss = 0.06735133
Iteration 31, loss = 0.06484152
Iteration 32, loss = 0.06293361
Iteration 33, loss = 0.06078132
Iteration 34, loss = 0.05882580
Iteration 35, loss = 0.05717033
Iteration 36, loss = 0.05569051
Iteration 37, loss = 0.05377494
Iteration 38, loss = 0.05210297
Iteration 39, loss = 0.05053622
Iteration 40, loss = 0.04977263
Iteration 41, loss = 0.04771041
Iteration 42, loss = 0.04662609
Iteration 43, loss = 0.04511493
Iteration 44, loss = 0.04387673
Iteration 45, loss = 0.04240948
Iteration 46, loss = 0.04115416
Iteration 47, loss = 0.04029698
Iteration 48, loss = 0.03895610
Iteration 49, loss = 0.03807745
Iteration 50, loss = 0.03703906
Iteration 51, loss = 0.03587409
Iteration 52, loss = 0.03496455
Iteration 53, loss = 0.03408376
Iteration 54, loss = 0.03320559
Iteration 55, loss = 0.03226181
Iteration 56, loss = 0.03144030
Iteration 57, loss = 0.03073846
Iteration 58, loss = 0.02982227
Iteration 59, loss = 0.02899092
Iteration 60, loss = 0.02827147
Iteration 61, loss = 0.02763017
Iteration 62, loss = 0.02687930
Iteration 63, loss = 0.02609808
Iteration 64, loss = 0.02554759
Iteration 65, loss = 0.02466203
Iteration 66, loss = 0.02408442
Iteration 67, loss = 0.02347112
Iteration 68, loss = 0.02295601
Iteration 69, loss = 0.02235151
Iteration 70, loss = 0.02173946
Iteration 71, loss = 0.02121404
Iteration 72, loss = 0.02059807
Iteration 73, loss = 0.02011368
Iteration 74, loss = 0.01959608
Iteration 75, loss = 0.01916209
Iteration 76, loss = 0.01864346
Iteration 77, loss = 0.01815167
Iteration 78, loss = 0.01766754
Iteration 79, loss = 0.01724513
Iteration 80, loss = 0.01683711
Iteration 81, loss = 0.01645095
Iteration 82, loss = 0.01597579
Iteration 83, loss = 0.01554040
Iteration 84, loss = 0.01525810
Iteration 85, loss = 0.01480669
Iteration 86, loss = 0.01450320
Iteration 87, loss = 0.01409137
Iteration 88, loss = 0.01379450
Iteration 89, loss = 0.01342146
Iteration 90, loss = 0.01313316
Iteration 91, loss = 0.01276753
Iteration 92, loss = 0.01248871
Iteration 93, loss = 0.01221290
Iteration 94, loss = 0.01191152
Iteration 95, loss = 0.01158978
Iteration 96, loss = 0.01134335
Iteration 97, loss = 0.01103631
Iteration 98, loss = 0.01073574
Iteration 99, loss = 0.01050156
Iteration 100, loss = 0.01034882
Iteration 101, loss = 0.01007876
Iteration 102, loss = 0.00983465
Iteration 103, loss = 0.00964445
Iteration 104, loss = 0.00941294
Iteration 105, loss = 0.00916672
Iteration 106, loss = 0.00894461
Iteration 107, loss = 0.00877423
Iteration 108, loss = 0.00859735
Iteration 109, loss = 0.00837923
Iteration 110, loss = 0.00819260
Iteration 111, loss = 0.00802222
Iteration 112, loss = 0.00783476
Iteration 113, loss = 0.00761443
Iteration 114, loss = 0.00746961
Iteration 115, loss = 0.00730191
Iteration 116, loss = 0.00712571
Iteration 117, loss = 0.00699167
Iteration 118, loss = 0.00682540
Iteration 119, loss = 0.00670512
Iteration 120, loss = 0.00654314
Iteration 121, loss = 0.00640630
Iteration 122, loss = 0.00628642
Iteration 123, loss = 0.00620149
Iteration 124, loss = 0.00607302
Iteration 125, loss = 0.00590677
Iteration 126, loss = 0.00583666
Iteration 127, loss = 0.00568037
Iteration 128, loss = 0.00557657
Iteration 129, loss = 0.00544907
Iteration 130, loss = 0.00536614
Iteration 131, loss = 0.00524311
Iteration 132, loss = 0.00516302
Iteration 133, loss = 0.00507225
Iteration 134, loss = 0.00495010
Iteration 135, loss = 0.00485594
Iteration 136, loss = 0.00476673
Iteration 137, loss = 0.00468828
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  55
Error rate: 11.60% ( 290/2500)
 - Class 1:  38, Class 2:  20, Class 3: 104, Class 4:  71, Class 5:  57
Iteration 1, loss = 1.29846901
Iteration 2, loss = 0.84983634
Iteration 3, loss = 0.58224590
Iteration 4, loss = 0.42271240
Iteration 5, loss = 0.32768284
Iteration 6, loss = 0.26824540
Iteration 7, loss = 0.22889050
Iteration 8, loss = 0.20168158
Iteration 9, loss = 0.18090280
Iteration 10, loss = 0.16507049
Iteration 11, loss = 0.15230212
Iteration 12, loss = 0.14144850
Iteration 13, loss = 0.13276761
Iteration 14, loss = 0.12547396
Iteration 15, loss = 0.11866068
Iteration 16, loss = 0.11302016
Iteration 17, loss = 0.10753561
Iteration 18, loss = 0.10312952
Iteration 19, loss = 0.09830707
Iteration 20, loss = 0.09439036
Iteration 21, loss = 0.09112808
Iteration 22, loss = 0.08743684
Iteration 23, loss = 0.08483393
Iteration 24, loss = 0.08162630
Iteration 25, loss = 0.07853902
Iteration 26, loss = 0.07598409
Iteration 27, loss = 0.07349251
Iteration 28, loss = 0.07143355
Iteration 29, loss = 0.06894525
Iteration 30, loss = 0.06665475
Iteration 31, loss = 0.06473985
Iteration 32, loss = 0.06233456
Iteration 33, loss = 0.06079914
Iteration 34, loss = 0.05944354
Iteration 35, loss = 0.05718081
Iteration 36, loss = 0.05551601
Iteration 37, loss = 0.05399400
Iteration 38, loss = 0.05254050
Iteration 39, loss = 0.05095505
Iteration 40, loss = 0.04990770
Iteration 41, loss = 0.04796890
Iteration 42, loss = 0.04690235
Iteration 43, loss = 0.04567735
Iteration 44, loss = 0.04450305
Iteration 45, loss = 0.04328590
Iteration 46, loss = 0.04201391
Iteration 47, loss = 0.04089232
Iteration 48, loss = 0.03971617
Iteration 49, loss = 0.03866422
Iteration 50, loss = 0.03769498
Iteration 51, loss = 0.03660354
Iteration 52, loss = 0.03541178
Iteration 53, loss = 0.03473843
Iteration 54, loss = 0.03365459
Iteration 55, loss = 0.03273765
Iteration 56, loss = 0.03201815
Iteration 57, loss = 0.03111596
Iteration 58, loss = 0.03049138
Iteration 59, loss = 0.02963150
Iteration 60, loss = 0.02888774
Iteration 61, loss = 0.02801365
Iteration 62, loss = 0.02724432
Iteration 63, loss = 0.02659810
Iteration 64, loss = 0.02577335
Iteration 65, loss = 0.02529304
Iteration 66, loss = 0.02468628
Iteration 67, loss = 0.02400393
Iteration 68, loss = 0.02331838
Iteration 69, loss = 0.02263611
Iteration 70, loss = 0.02205649
Iteration 71, loss = 0.02151811
Iteration 72, loss = 0.02110270
Iteration 73, loss = 0.02037449
Iteration 74, loss = 0.01986037
Iteration 75, loss = 0.01921193
Iteration 76, loss = 0.01892522
Iteration 77, loss = 0.01838389
Iteration 78, loss = 0.01768694
Iteration 79, loss = 0.01721487
Iteration 80, loss = 0.01684664
Iteration 81, loss = 0.01650555
Iteration 82, loss = 0.01603466
Iteration 83, loss = 0.01554954
Iteration 84, loss = 0.01523311
Iteration 85, loss = 0.01485568
Iteration 86, loss = 0.01451544
Iteration 87, loss = 0.01396877
Iteration 88, loss = 0.01369665
Iteration 89, loss = 0.01336520
Iteration 90, loss = 0.01295796
Iteration 91, loss = 0.01261141
Iteration 92, loss = 0.01233262
Iteration 93, loss = 0.01206492
Iteration 94, loss = 0.01165590
Iteration 95, loss = 0.01143638
Iteration 96, loss = 0.01116389
Iteration 97, loss = 0.01081907
Iteration 98, loss = 0.01052641
Iteration 99, loss = 0.01024302
Iteration 100, loss = 0.00999942
Iteration 101, loss = 0.00981217
Iteration 102, loss = 0.00955937
Iteration 103, loss = 0.00926320
Iteration 104, loss = 0.00909631
Iteration 105, loss = 0.00883107
Iteration 106, loss = 0.00862742
Iteration 107, loss = 0.00843697
Iteration 108, loss = 0.00828819
Iteration 109, loss = 0.00804943
Iteration 110, loss = 0.00785702
Iteration 111, loss = 0.00764019
Iteration 112, loss = 0.00748497
Iteration 113, loss = 0.00730502
Iteration 114, loss = 0.00719616
Iteration 115, loss = 0.00700253
Iteration 116, loss = 0.00686485
Iteration 117, loss = 0.00670769
Iteration 118, loss = 0.00648342
Iteration 119, loss = 0.00639788
Iteration 120, loss = 0.00621704
Iteration 121, loss = 0.00611029
Iteration 122, loss = 0.00595742
Iteration 123, loss = 0.00582517
Iteration 124, loss = 0.00570208
Iteration 125, loss = 0.00556044
Iteration 126, loss = 0.00546822
Iteration 127, loss = 0.00533934
Iteration 128, loss = 0.00529990
Iteration 129, loss = 0.00514342
Iteration 130, loss = 0.00502109
Iteration 131, loss = 0.00491785
Iteration 132, loss = 0.00481630
Iteration 133, loss = 0.00474710
Iteration 134, loss = 0.00464213
Iteration 135, loss = 0.00455041
Iteration 136, loss = 0.00445894
Iteration 137, loss = 0.00439467
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  60
Error rate: 24.56% ( 614/2500)
 - Class 1:  29, Class 2:  24, Class 3: 500, Class 4:  36, Class 5:  25
Iteration 1, loss = 1.30249815
Iteration 2, loss = 0.83072604
Iteration 3, loss = 0.55371796
Iteration 4, loss = 0.39702127
Iteration 5, loss = 0.30763418
Iteration 6, loss = 0.25265930
Iteration 7, loss = 0.21692237
Iteration 8, loss = 0.19132043
Iteration 9, loss = 0.17254829
Iteration 10, loss = 0.15758689
Iteration 11, loss = 0.14628196
Iteration 12, loss = 0.13593711
Iteration 13, loss = 0.12799799
Iteration 14, loss = 0.12124058
Iteration 15, loss = 0.11488842
Iteration 16, loss = 0.10923825
Iteration 17, loss = 0.10480010
Iteration 18, loss = 0.10015102
Iteration 19, loss = 0.09587990
Iteration 20, loss = 0.09220260
Iteration 21, loss = 0.08914241
Iteration 22, loss = 0.08576165
Iteration 23, loss = 0.08372225
Iteration 24, loss = 0.07922159
Iteration 25, loss = 0.07659338
Iteration 26, loss = 0.07378815
Iteration 27, loss = 0.07122526
Iteration 28, loss = 0.06936133
Iteration 29, loss = 0.06699748
Iteration 30, loss = 0.06477550
Iteration 31, loss = 0.06333062
Iteration 32, loss = 0.06086225
Iteration 33, loss = 0.05924428
Iteration 34, loss = 0.05726455
Iteration 35, loss = 0.05524687
Iteration 36, loss = 0.05331296
Iteration 37, loss = 0.05219214
Iteration 38, loss = 0.05096080
Iteration 39, loss = 0.04916365
Iteration 40, loss = 0.04785034
Iteration 41, loss = 0.04607024
Iteration 42, loss = 0.04481932
Iteration 43, loss = 0.04350940
Iteration 44, loss = 0.04216792
Iteration 45, loss = 0.04120434
Iteration 46, loss = 0.04021108
Iteration 47, loss = 0.03883436
Iteration 48, loss = 0.03763870
Iteration 49, loss = 0.03668877
Iteration 50, loss = 0.03588752
Iteration 51, loss = 0.03467934
Iteration 52, loss = 0.03382407
Iteration 53, loss = 0.03296275
Iteration 54, loss = 0.03224853
Iteration 55, loss = 0.03135241
Iteration 56, loss = 0.03040433
Iteration 57, loss = 0.02942721
Iteration 58, loss = 0.02877761
Iteration 59, loss = 0.02815691
Iteration 60, loss = 0.02745407
Iteration 61, loss = 0.02659514
Iteration 62, loss = 0.02593866
Iteration 63, loss = 0.02528680
Iteration 64, loss = 0.02457956
Iteration 65, loss = 0.02425026
Iteration 66, loss = 0.02327483
Iteration 67, loss = 0.02286321
Iteration 68, loss = 0.02192737
Iteration 69, loss = 0.02155464
Iteration 70, loss = 0.02102099
Iteration 71, loss = 0.02050033
Iteration 72, loss = 0.01974982
Iteration 73, loss = 0.01930489
Iteration 74, loss = 0.01874757
Iteration 75, loss = 0.01825710
Iteration 76, loss = 0.01781794
Iteration 77, loss = 0.01731401
Iteration 78, loss = 0.01689553
Iteration 79, loss = 0.01637541
Iteration 80, loss = 0.01593221
Iteration 81, loss = 0.01561803
Iteration 82, loss = 0.01516019
Iteration 83, loss = 0.01475233
Iteration 84, loss = 0.01433965
Iteration 85, loss = 0.01400667
Iteration 86, loss = 0.01360251
Iteration 87, loss = 0.01325354
Iteration 88, loss = 0.01286696
Iteration 89, loss = 0.01263973
Iteration 90, loss = 0.01231779
Iteration 91, loss = 0.01194718
Iteration 92, loss = 0.01166371
Iteration 93, loss = 0.01139221
Iteration 94, loss = 0.01097289
Iteration 95, loss = 0.01075684
Iteration 96, loss = 0.01048404
Iteration 97, loss = 0.01022862
Iteration 98, loss = 0.00995989
Iteration 99, loss = 0.00975477
Iteration 100, loss = 0.00947821
Iteration 101, loss = 0.00933308
Iteration 102, loss = 0.00908774
Iteration 103, loss = 0.00881754
Iteration 104, loss = 0.00858238
Iteration 105, loss = 0.00837330
Iteration 106, loss = 0.00820700
Iteration 107, loss = 0.00795870
Iteration 108, loss = 0.00778031
Iteration 109, loss = 0.00761875
Iteration 110, loss = 0.00742171
Iteration 111, loss = 0.00727450
Iteration 112, loss = 0.00707588
Iteration 113, loss = 0.00690921
Iteration 114, loss = 0.00679906
Iteration 115, loss = 0.00663928
Iteration 116, loss = 0.00647866
Iteration 117, loss = 0.00639597
Iteration 118, loss = 0.00618764
Iteration 119, loss = 0.00606347
Iteration 120, loss = 0.00592525
Iteration 121, loss = 0.00579984
Iteration 122, loss = 0.00568832
Iteration 123, loss = 0.00558533
Iteration 124, loss = 0.00548664
Iteration 125, loss = 0.00534054
Iteration 126, loss = 0.00522381
Iteration 127, loss = 0.00510031
Iteration 128, loss = 0.00498951
Iteration 129, loss = 0.00488919
Iteration 130, loss = 0.00480133
Iteration 131, loss = 0.00470689
Iteration 132, loss = 0.00459612
Iteration 133, loss = 0.00452983
Iteration 134, loss = 0.00446258
Iteration 135, loss = 0.00436826
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  65
Error rate: 24.32% ( 608/2500)
 - Class 1:  30, Class 2:  23, Class 3: 498, Class 4:  41, Class 5:  16
Iteration 1, loss = 1.27171290
Iteration 2, loss = 0.76904198
Iteration 3, loss = 0.50043341
Iteration 4, loss = 0.35810274
Iteration 5, loss = 0.27957370
Iteration 6, loss = 0.23135239
Iteration 7, loss = 0.20059844
Iteration 8, loss = 0.17708324
Iteration 9, loss = 0.16054649
Iteration 10, loss = 0.14702765
Iteration 11, loss = 0.13622000
Iteration 12, loss = 0.12754131
Iteration 13, loss = 0.11981156
Iteration 14, loss = 0.11322217
Iteration 15, loss = 0.10745236
Iteration 16, loss = 0.10264978
Iteration 17, loss = 0.09750307
Iteration 18, loss = 0.09341406
Iteration 19, loss = 0.08949071
Iteration 20, loss = 0.08569224
Iteration 21, loss = 0.08305692
Iteration 22, loss = 0.07929311
Iteration 23, loss = 0.07646942
Iteration 24, loss = 0.07353125
Iteration 25, loss = 0.07106040
Iteration 26, loss = 0.06844415
Iteration 27, loss = 0.06631030
Iteration 28, loss = 0.06386431
Iteration 29, loss = 0.06208537
Iteration 30, loss = 0.05988163
Iteration 31, loss = 0.05792052
Iteration 32, loss = 0.05586537
Iteration 33, loss = 0.05413383
Iteration 34, loss = 0.05264308
Iteration 35, loss = 0.05126773
Iteration 36, loss = 0.04935348
Iteration 37, loss = 0.04796813
Iteration 38, loss = 0.04671540
Iteration 39, loss = 0.04494679
Iteration 40, loss = 0.04373059
Iteration 41, loss = 0.04261037
Iteration 42, loss = 0.04146442
Iteration 43, loss = 0.04023880
Iteration 44, loss = 0.03889693
Iteration 45, loss = 0.03773819
Iteration 46, loss = 0.03662202
Iteration 47, loss = 0.03581966
Iteration 48, loss = 0.03492824
Iteration 49, loss = 0.03386723
Iteration 50, loss = 0.03297112
Iteration 51, loss = 0.03194695
Iteration 52, loss = 0.03136539
Iteration 53, loss = 0.03030358
Iteration 54, loss = 0.02953111
Iteration 55, loss = 0.02874103
Iteration 56, loss = 0.02784581
Iteration 57, loss = 0.02715299
Iteration 58, loss = 0.02672960
Iteration 59, loss = 0.02596246
Iteration 60, loss = 0.02514777
Iteration 61, loss = 0.02451715
Iteration 62, loss = 0.02362866
Iteration 63, loss = 0.02312669
Iteration 64, loss = 0.02240064
Iteration 65, loss = 0.02173153
Iteration 66, loss = 0.02123442
Iteration 67, loss = 0.02058427
Iteration 68, loss = 0.02006102
Iteration 69, loss = 0.01964571
Iteration 70, loss = 0.01909259
Iteration 71, loss = 0.01855960
Iteration 72, loss = 0.01802934
Iteration 73, loss = 0.01756536
Iteration 74, loss = 0.01704887
Iteration 75, loss = 0.01666434
Iteration 76, loss = 0.01624690
Iteration 77, loss = 0.01592619
Iteration 78, loss = 0.01549768
Iteration 79, loss = 0.01512063
Iteration 80, loss = 0.01442900
Iteration 81, loss = 0.01409678
Iteration 82, loss = 0.01379655
Iteration 83, loss = 0.01329815
Iteration 84, loss = 0.01299827
Iteration 85, loss = 0.01275101
Iteration 86, loss = 0.01237878
Iteration 87, loss = 0.01212920
Iteration 88, loss = 0.01182513
Iteration 89, loss = 0.01135780
Iteration 90, loss = 0.01104394
Iteration 91, loss = 0.01071560
Iteration 92, loss = 0.01060642
Iteration 93, loss = 0.01020053
Iteration 94, loss = 0.01003601
Iteration 95, loss = 0.00968972
Iteration 96, loss = 0.00954583
Iteration 97, loss = 0.00921450
Iteration 98, loss = 0.00897083
Iteration 99, loss = 0.00878479
Iteration 100, loss = 0.00855786
Iteration 101, loss = 0.00827309
Iteration 102, loss = 0.00809562
Iteration 103, loss = 0.00787845
Iteration 104, loss = 0.00772652
Iteration 105, loss = 0.00753440
Iteration 106, loss = 0.00733134
Iteration 107, loss = 0.00716439
Iteration 108, loss = 0.00699011
Iteration 109, loss = 0.00683015
Iteration 110, loss = 0.00662421
Iteration 111, loss = 0.00651295
Iteration 112, loss = 0.00634682
Iteration 113, loss = 0.00619378
Iteration 114, loss = 0.00606302
Iteration 115, loss = 0.00593899
Iteration 116, loss = 0.00575348
Iteration 117, loss = 0.00564433
Iteration 118, loss = 0.00552644
Iteration 119, loss = 0.00540504
Iteration 120, loss = 0.00533528
Iteration 121, loss = 0.00517137
Iteration 122, loss = 0.00503940
Iteration 123, loss = 0.00493436
Iteration 124, loss = 0.00488275
Iteration 125, loss = 0.00475333
Iteration 126, loss = 0.00466339
Iteration 127, loss = 0.00455359
Iteration 128, loss = 0.00443391
Iteration 129, loss = 0.00436851
Iteration 130, loss = 0.00427586
Iteration 131, loss = 0.00420502
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  70
Error rate: 11.20% ( 280/2500)
 - Class 1:  35, Class 2:  20, Class 3: 131, Class 4:  53, Class 5:  41
Iteration 1, loss = 1.25030383
Iteration 2, loss = 0.73362222
Iteration 3, loss = 0.46683773
Iteration 4, loss = 0.33517783
Iteration 5, loss = 0.26325316
Iteration 6, loss = 0.21999331
Iteration 7, loss = 0.19080194
Iteration 8, loss = 0.17029772
Iteration 9, loss = 0.15440611
Iteration 10, loss = 0.14210754
Iteration 11, loss = 0.13243021
Iteration 12, loss = 0.12395246
Iteration 13, loss = 0.11628457
Iteration 14, loss = 0.11017459
Iteration 15, loss = 0.10539515
Iteration 16, loss = 0.09977491
Iteration 17, loss = 0.09513274
Iteration 18, loss = 0.09120534
Iteration 19, loss = 0.08745723
Iteration 20, loss = 0.08418066
Iteration 21, loss = 0.08059685
Iteration 22, loss = 0.07722770
Iteration 23, loss = 0.07455970
Iteration 24, loss = 0.07255002
Iteration 25, loss = 0.06964678
Iteration 26, loss = 0.06651007
Iteration 27, loss = 0.06461384
Iteration 28, loss = 0.06225197
Iteration 29, loss = 0.06019038
Iteration 30, loss = 0.05817689
Iteration 31, loss = 0.05636521
Iteration 32, loss = 0.05479356
Iteration 33, loss = 0.05367732
Iteration 34, loss = 0.05125709
Iteration 35, loss = 0.04949363
Iteration 36, loss = 0.04766885
Iteration 37, loss = 0.04616095
Iteration 38, loss = 0.04493081
Iteration 39, loss = 0.04349815
Iteration 40, loss = 0.04228414
Iteration 41, loss = 0.04129854
Iteration 42, loss = 0.03999263
Iteration 43, loss = 0.03858053
Iteration 44, loss = 0.03731290
Iteration 45, loss = 0.03624393
Iteration 46, loss = 0.03529714
Iteration 47, loss = 0.03420408
Iteration 48, loss = 0.03304666
Iteration 49, loss = 0.03213189
Iteration 50, loss = 0.03120451
Iteration 51, loss = 0.03034374
Iteration 52, loss = 0.02957965
Iteration 53, loss = 0.02851644
Iteration 54, loss = 0.02776328
Iteration 55, loss = 0.02699109
Iteration 56, loss = 0.02619027
Iteration 57, loss = 0.02536007
Iteration 58, loss = 0.02470957
Iteration 59, loss = 0.02402479
Iteration 60, loss = 0.02311931
Iteration 61, loss = 0.02275285
Iteration 62, loss = 0.02187555
Iteration 63, loss = 0.02137498
Iteration 64, loss = 0.02072284
Iteration 65, loss = 0.02020553
Iteration 66, loss = 0.01950580
Iteration 67, loss = 0.01903082
Iteration 68, loss = 0.01824490
Iteration 69, loss = 0.01780497
Iteration 70, loss = 0.01736417
Iteration 71, loss = 0.01666772
Iteration 72, loss = 0.01631060
Iteration 73, loss = 0.01594422
Iteration 74, loss = 0.01535102
Iteration 75, loss = 0.01493276
Iteration 76, loss = 0.01448579
Iteration 77, loss = 0.01401532
Iteration 78, loss = 0.01362094
Iteration 79, loss = 0.01321881
Iteration 80, loss = 0.01292883
Iteration 81, loss = 0.01259765
Iteration 82, loss = 0.01218540
Iteration 83, loss = 0.01195373
Iteration 84, loss = 0.01162303
Iteration 85, loss = 0.01120165
Iteration 86, loss = 0.01094269
Iteration 87, loss = 0.01060557
Iteration 88, loss = 0.01036389
Iteration 89, loss = 0.00999955
Iteration 90, loss = 0.00974609
Iteration 91, loss = 0.00951578
Iteration 92, loss = 0.00929502
Iteration 93, loss = 0.00900654
Iteration 94, loss = 0.00878707
Iteration 95, loss = 0.00860906
Iteration 96, loss = 0.00836033
Iteration 97, loss = 0.00813065
Iteration 98, loss = 0.00789934
Iteration 99, loss = 0.00766815
Iteration 100, loss = 0.00748460
Iteration 101, loss = 0.00735400
Iteration 102, loss = 0.00713489
Iteration 103, loss = 0.00696660
Iteration 104, loss = 0.00680080
Iteration 105, loss = 0.00665537
Iteration 106, loss = 0.00648177
Iteration 107, loss = 0.00632753
Iteration 108, loss = 0.00619173
Iteration 109, loss = 0.00602376
Iteration 110, loss = 0.00587924
Iteration 111, loss = 0.00574135
Iteration 112, loss = 0.00563869
Iteration 113, loss = 0.00547430
Iteration 114, loss = 0.00534528
Iteration 115, loss = 0.00523617
Iteration 116, loss = 0.00514065
Iteration 117, loss = 0.00500348
Iteration 118, loss = 0.00490815
Iteration 119, loss = 0.00481008
Iteration 120, loss = 0.00469503
Iteration 121, loss = 0.00461197
Iteration 122, loss = 0.00450306
Iteration 123, loss = 0.00441746
Iteration 124, loss = 0.00433018
Iteration 125, loss = 0.00422729
Iteration 126, loss = 0.00418055
Iteration 127, loss = 0.00406702
Iteration 128, loss = 0.00400052
Iteration 129, loss = 0.00392208
Iteration 130, loss = 0.00383390
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  75
Error rate: 25.36% ( 634/2500)
 - Class 1:  43, Class 2:  28, Class 3: 483, Class 4:  46, Class 5:  34
Iteration 1, loss = 1.26523222
Iteration 2, loss = 0.73191354
Iteration 3, loss = 0.47236716
Iteration 4, loss = 0.34038790
Iteration 5, loss = 0.26753563
Iteration 6, loss = 0.22383532
Iteration 7, loss = 0.19416257
Iteration 8, loss = 0.17324906
Iteration 9, loss = 0.15738383
Iteration 10, loss = 0.14451880
Iteration 11, loss = 0.13448551
Iteration 12, loss = 0.12575675
Iteration 13, loss = 0.11866224
Iteration 14, loss = 0.11211560
Iteration 15, loss = 0.10656346
Iteration 16, loss = 0.10130491
Iteration 17, loss = 0.09662792
Iteration 18, loss = 0.09216305
Iteration 19, loss = 0.08861077
Iteration 20, loss = 0.08504554
Iteration 21, loss = 0.08169254
Iteration 22, loss = 0.07857589
Iteration 23, loss = 0.07609949
Iteration 24, loss = 0.07328384
Iteration 25, loss = 0.07032847
Iteration 26, loss = 0.06738350
Iteration 27, loss = 0.06556365
Iteration 28, loss = 0.06341726
Iteration 29, loss = 0.06102017
Iteration 30, loss = 0.05895963
Iteration 31, loss = 0.05690875
Iteration 32, loss = 0.05553975
Iteration 33, loss = 0.05331047
Iteration 34, loss = 0.05188249
Iteration 35, loss = 0.05007271
Iteration 36, loss = 0.04851865
Iteration 37, loss = 0.04722854
Iteration 38, loss = 0.04589600
Iteration 39, loss = 0.04436210
Iteration 40, loss = 0.04281521
Iteration 41, loss = 0.04136708
Iteration 42, loss = 0.04036791
Iteration 43, loss = 0.03900404
Iteration 44, loss = 0.03801861
Iteration 45, loss = 0.03700218
Iteration 46, loss = 0.03573659
Iteration 47, loss = 0.03468959
Iteration 48, loss = 0.03392827
Iteration 49, loss = 0.03298991
Iteration 50, loss = 0.03190292
Iteration 51, loss = 0.03082935
Iteration 52, loss = 0.03012402
Iteration 53, loss = 0.02919180
Iteration 54, loss = 0.02831265
Iteration 55, loss = 0.02754489
Iteration 56, loss = 0.02670324
Iteration 57, loss = 0.02593838
Iteration 58, loss = 0.02513220
Iteration 59, loss = 0.02458245
Iteration 60, loss = 0.02372420
Iteration 61, loss = 0.02311815
Iteration 62, loss = 0.02257109
Iteration 63, loss = 0.02171024
Iteration 64, loss = 0.02103887
Iteration 65, loss = 0.02053304
Iteration 66, loss = 0.02008653
Iteration 67, loss = 0.01922970
Iteration 68, loss = 0.01869678
Iteration 69, loss = 0.01827019
Iteration 70, loss = 0.01771458
Iteration 71, loss = 0.01724270
Iteration 72, loss = 0.01658639
Iteration 73, loss = 0.01611371
Iteration 74, loss = 0.01578832
Iteration 75, loss = 0.01526910
Iteration 76, loss = 0.01481506
Iteration 77, loss = 0.01435316
Iteration 78, loss = 0.01398130
Iteration 79, loss = 0.01357149
Iteration 80, loss = 0.01321981
Iteration 81, loss = 0.01287241
Iteration 82, loss = 0.01257583
Iteration 83, loss = 0.01216907
Iteration 84, loss = 0.01180788
Iteration 85, loss = 0.01135487
Iteration 86, loss = 0.01114887
Iteration 87, loss = 0.01071921
Iteration 88, loss = 0.01052872
Iteration 89, loss = 0.01033225
Iteration 90, loss = 0.00994824
Iteration 91, loss = 0.00966346
Iteration 92, loss = 0.00936252
Iteration 93, loss = 0.00914296
Iteration 94, loss = 0.00887403
Iteration 95, loss = 0.00869592
Iteration 96, loss = 0.00844250
Iteration 97, loss = 0.00823728
Iteration 98, loss = 0.00801419
Iteration 99, loss = 0.00777132
Iteration 100, loss = 0.00756427
Iteration 101, loss = 0.00736851
Iteration 102, loss = 0.00719415
Iteration 103, loss = 0.00701310
Iteration 104, loss = 0.00682266
Iteration 105, loss = 0.00668115
Iteration 106, loss = 0.00655344
Iteration 107, loss = 0.00635002
Iteration 108, loss = 0.00617771
Iteration 109, loss = 0.00605007
Iteration 110, loss = 0.00587691
Iteration 111, loss = 0.00575819
Iteration 112, loss = 0.00563226
Iteration 113, loss = 0.00548472
Iteration 114, loss = 0.00536394
Iteration 115, loss = 0.00524873
Iteration 116, loss = 0.00510121
Iteration 117, loss = 0.00503830
Iteration 118, loss = 0.00492961
Iteration 119, loss = 0.00480369
Iteration 120, loss = 0.00468557
Iteration 121, loss = 0.00458320
Iteration 122, loss = 0.00449914
Iteration 123, loss = 0.00444070
Iteration 124, loss = 0.00428992
Iteration 125, loss = 0.00424742
Iteration 126, loss = 0.00413068
Iteration 127, loss = 0.00406648
Iteration 128, loss = 0.00397176
Iteration 129, loss = 0.00389152
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  80
Error rate: 24.64% ( 616/2500)
 - Class 1:  12, Class 2:  14, Class 3: 467, Class 4:  38, Class 5:  85
Iteration 1, loss = 1.25161762
Iteration 2, loss = 0.72291638
Iteration 3, loss = 0.46333663
Iteration 4, loss = 0.33320636
Iteration 5, loss = 0.26154486
Iteration 6, loss = 0.21880271
Iteration 7, loss = 0.19003148
Iteration 8, loss = 0.16934130
Iteration 9, loss = 0.15389117
Iteration 10, loss = 0.14146407
Iteration 11, loss = 0.13122777
Iteration 12, loss = 0.12258167
Iteration 13, loss = 0.11518423
Iteration 14, loss = 0.10900284
Iteration 15, loss = 0.10307056
Iteration 16, loss = 0.09854332
Iteration 17, loss = 0.09463131
Iteration 18, loss = 0.08948590
Iteration 19, loss = 0.08586753
Iteration 20, loss = 0.08285184
Iteration 21, loss = 0.08023627
Iteration 22, loss = 0.07617475
Iteration 23, loss = 0.07341468
Iteration 24, loss = 0.07040163
Iteration 25, loss = 0.06809399
Iteration 26, loss = 0.06576852
Iteration 27, loss = 0.06323482
Iteration 28, loss = 0.06079824
Iteration 29, loss = 0.05922390
Iteration 30, loss = 0.05720689
Iteration 31, loss = 0.05510458
Iteration 32, loss = 0.05312121
Iteration 33, loss = 0.05130776
Iteration 34, loss = 0.04991000
Iteration 35, loss = 0.04854390
Iteration 36, loss = 0.04671076
Iteration 37, loss = 0.04512479
Iteration 38, loss = 0.04365648
Iteration 39, loss = 0.04259958
Iteration 40, loss = 0.04149708
Iteration 41, loss = 0.04000472
Iteration 42, loss = 0.03879953
Iteration 43, loss = 0.03716287
Iteration 44, loss = 0.03627145
Iteration 45, loss = 0.03518570
Iteration 46, loss = 0.03396979
Iteration 47, loss = 0.03272488
Iteration 48, loss = 0.03182445
Iteration 49, loss = 0.03105159
Iteration 50, loss = 0.02984867
Iteration 51, loss = 0.02916141
Iteration 52, loss = 0.02817178
Iteration 53, loss = 0.02767027
Iteration 54, loss = 0.02686021
Iteration 55, loss = 0.02593150
Iteration 56, loss = 0.02510394
Iteration 57, loss = 0.02418121
Iteration 58, loss = 0.02350616
Iteration 59, loss = 0.02274261
Iteration 60, loss = 0.02207250
Iteration 61, loss = 0.02147363
Iteration 62, loss = 0.02081365
Iteration 63, loss = 0.02021101
Iteration 64, loss = 0.01952601
Iteration 65, loss = 0.01889473
Iteration 66, loss = 0.01836399
Iteration 67, loss = 0.01779290
Iteration 68, loss = 0.01734807
Iteration 69, loss = 0.01684493
Iteration 70, loss = 0.01630024
Iteration 71, loss = 0.01591586
Iteration 72, loss = 0.01560379
Iteration 73, loss = 0.01519857
Iteration 74, loss = 0.01450306
Iteration 75, loss = 0.01415155
Iteration 76, loss = 0.01365000
Iteration 77, loss = 0.01331843
Iteration 78, loss = 0.01284433
Iteration 79, loss = 0.01261425
Iteration 80, loss = 0.01216696
Iteration 81, loss = 0.01177967
Iteration 82, loss = 0.01145091
Iteration 83, loss = 0.01116723
Iteration 84, loss = 0.01081153
Iteration 85, loss = 0.01051273
Iteration 86, loss = 0.01019520
Iteration 87, loss = 0.01004991
Iteration 88, loss = 0.00965200
Iteration 89, loss = 0.00948013
Iteration 90, loss = 0.00925049
Iteration 91, loss = 0.00898892
Iteration 92, loss = 0.00884196
Iteration 93, loss = 0.00842270
Iteration 94, loss = 0.00819463
Iteration 95, loss = 0.00796606
Iteration 96, loss = 0.00778585
Iteration 97, loss = 0.00763905
Iteration 98, loss = 0.00741558
Iteration 99, loss = 0.00717231
Iteration 100, loss = 0.00700876
Iteration 101, loss = 0.00681721
Iteration 102, loss = 0.00665238
Iteration 103, loss = 0.00650491
Iteration 104, loss = 0.00635406
Iteration 105, loss = 0.00616061
Iteration 106, loss = 0.00604875
Iteration 107, loss = 0.00592010
Iteration 108, loss = 0.00573957
Iteration 109, loss = 0.00559288
Iteration 110, loss = 0.00546447
Iteration 111, loss = 0.00533023
Iteration 112, loss = 0.00519514
Iteration 113, loss = 0.00516253
Iteration 114, loss = 0.00503832
Iteration 115, loss = 0.00488026
Iteration 116, loss = 0.00476214
Iteration 117, loss = 0.00468458
Iteration 118, loss = 0.00454273
Iteration 119, loss = 0.00449071
Iteration 120, loss = 0.00439022
Iteration 121, loss = 0.00428285
Iteration 122, loss = 0.00422838
Iteration 123, loss = 0.00412122
Iteration 124, loss = 0.00402258
Iteration 125, loss = 0.00394226
Iteration 126, loss = 0.00385099
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  85
Error rate: 10.76% ( 269/2500)
 - Class 1:  24, Class 2:  28, Class 3: 122, Class 4:  64, Class 5:  31
Iteration 1, loss = 1.22646022
Iteration 2, loss = 0.69925664
Iteration 3, loss = 0.43956649
Iteration 4, loss = 0.31462804
Iteration 5, loss = 0.24841199
Iteration 6, loss = 0.20818092
Iteration 7, loss = 0.18131182
Iteration 8, loss = 0.16163198
Iteration 9, loss = 0.14685683
Iteration 10, loss = 0.13594119
Iteration 11, loss = 0.12615488
Iteration 12, loss = 0.11951939
Iteration 13, loss = 0.11149106
Iteration 14, loss = 0.10526408
Iteration 15, loss = 0.09966089
Iteration 16, loss = 0.09515341
Iteration 17, loss = 0.09079499
Iteration 18, loss = 0.08666870
Iteration 19, loss = 0.08312618
Iteration 20, loss = 0.07966544
Iteration 21, loss = 0.07704282
Iteration 22, loss = 0.07380858
Iteration 23, loss = 0.07073620
Iteration 24, loss = 0.06825028
Iteration 25, loss = 0.06620570
Iteration 26, loss = 0.06330330
Iteration 27, loss = 0.06042206
Iteration 28, loss = 0.05888621
Iteration 29, loss = 0.05681186
Iteration 30, loss = 0.05466705
Iteration 31, loss = 0.05287053
Iteration 32, loss = 0.05148268
Iteration 33, loss = 0.04928238
Iteration 34, loss = 0.04820276
Iteration 35, loss = 0.04633202
Iteration 36, loss = 0.04498065
Iteration 37, loss = 0.04373740
Iteration 38, loss = 0.04210380
Iteration 39, loss = 0.04080497
Iteration 40, loss = 0.03957935
Iteration 41, loss = 0.03841138
Iteration 42, loss = 0.03684856
Iteration 43, loss = 0.03593899
Iteration 44, loss = 0.03485335
Iteration 45, loss = 0.03377599
Iteration 46, loss = 0.03283587
Iteration 47, loss = 0.03171433
Iteration 48, loss = 0.03101229
Iteration 49, loss = 0.03002232
Iteration 50, loss = 0.02901218
Iteration 51, loss = 0.02824764
Iteration 52, loss = 0.02763464
Iteration 53, loss = 0.02643508
Iteration 54, loss = 0.02550162
Iteration 55, loss = 0.02499282
Iteration 56, loss = 0.02413842
Iteration 57, loss = 0.02357469
Iteration 58, loss = 0.02281108
Iteration 59, loss = 0.02207079
Iteration 60, loss = 0.02125829
Iteration 61, loss = 0.02075978
Iteration 62, loss = 0.02014884
Iteration 63, loss = 0.01950326
Iteration 64, loss = 0.01884313
Iteration 65, loss = 0.01827896
Iteration 66, loss = 0.01771816
Iteration 67, loss = 0.01716656
Iteration 68, loss = 0.01669144
Iteration 69, loss = 0.01608414
Iteration 70, loss = 0.01566539
Iteration 71, loss = 0.01515147
Iteration 72, loss = 0.01498419
Iteration 73, loss = 0.01434946
Iteration 74, loss = 0.01396103
Iteration 75, loss = 0.01363972
Iteration 76, loss = 0.01308227
Iteration 77, loss = 0.01277864
Iteration 78, loss = 0.01237564
Iteration 79, loss = 0.01210594
Iteration 80, loss = 0.01157859
Iteration 81, loss = 0.01132228
Iteration 82, loss = 0.01104466
Iteration 83, loss = 0.01074808
Iteration 84, loss = 0.01051680
Iteration 85, loss = 0.01012109
Iteration 86, loss = 0.00977238
Iteration 87, loss = 0.00950539
Iteration 88, loss = 0.00924121
Iteration 89, loss = 0.00896485
Iteration 90, loss = 0.00873523
Iteration 91, loss = 0.00856623
Iteration 92, loss = 0.00831856
Iteration 93, loss = 0.00810482
Iteration 94, loss = 0.00782221
Iteration 95, loss = 0.00764062
Iteration 96, loss = 0.00744492
Iteration 97, loss = 0.00727696
Iteration 98, loss = 0.00704975
Iteration 99, loss = 0.00687965
Iteration 100, loss = 0.00671231
Iteration 101, loss = 0.00651180
Iteration 102, loss = 0.00634581
Iteration 103, loss = 0.00618965
Iteration 104, loss = 0.00603667
Iteration 105, loss = 0.00590047
Iteration 106, loss = 0.00572112
Iteration 107, loss = 0.00561138
Iteration 108, loss = 0.00551912
Iteration 109, loss = 0.00534240
Iteration 110, loss = 0.00523871
Iteration 111, loss = 0.00511281
Iteration 112, loss = 0.00498815
Iteration 113, loss = 0.00483664
Iteration 114, loss = 0.00473951
Iteration 115, loss = 0.00464786
Iteration 116, loss = 0.00452862
Iteration 117, loss = 0.00442255
Iteration 118, loss = 0.00431394
Iteration 119, loss = 0.00425170
Iteration 120, loss = 0.00416974
Iteration 121, loss = 0.00406707
Iteration 122, loss = 0.00395926
Iteration 123, loss = 0.00387500
Iteration 124, loss = 0.00380914
Iteration 125, loss = 0.00371904
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  90
Error rate: 25.40% ( 635/2500)
 - Class 1:  43, Class 2:  18, Class 3: 449, Class 4:  46, Class 5:  79
Iteration 1, loss = 1.22305747
Iteration 2, loss = 0.69070773
Iteration 3, loss = 0.43068699
Iteration 4, loss = 0.30685502
Iteration 5, loss = 0.24195960
Iteration 6, loss = 0.20327518
Iteration 7, loss = 0.17729367
Iteration 8, loss = 0.15853694
Iteration 9, loss = 0.14482959
Iteration 10, loss = 0.13290099
Iteration 11, loss = 0.12413108
Iteration 12, loss = 0.11613895
Iteration 13, loss = 0.10951885
Iteration 14, loss = 0.10388873
Iteration 15, loss = 0.09897389
Iteration 16, loss = 0.09417251
Iteration 17, loss = 0.08984448
Iteration 18, loss = 0.08583010
Iteration 19, loss = 0.08218350
Iteration 20, loss = 0.07855923
Iteration 21, loss = 0.07506877
Iteration 22, loss = 0.07227862
Iteration 23, loss = 0.07022216
Iteration 24, loss = 0.06743641
Iteration 25, loss = 0.06431682
Iteration 26, loss = 0.06215775
Iteration 27, loss = 0.06002469
Iteration 28, loss = 0.05815150
Iteration 29, loss = 0.05580542
Iteration 30, loss = 0.05382833
Iteration 31, loss = 0.05222873
Iteration 32, loss = 0.05044495
Iteration 33, loss = 0.04926010
Iteration 34, loss = 0.04679893
Iteration 35, loss = 0.04548462
Iteration 36, loss = 0.04387938
Iteration 37, loss = 0.04238917
Iteration 38, loss = 0.04125485
Iteration 39, loss = 0.03992195
Iteration 40, loss = 0.03895723
Iteration 41, loss = 0.03733119
Iteration 42, loss = 0.03626185
Iteration 43, loss = 0.03533378
Iteration 44, loss = 0.03410184
Iteration 45, loss = 0.03327272
Iteration 46, loss = 0.03233548
Iteration 47, loss = 0.03106883
Iteration 48, loss = 0.03018663
Iteration 49, loss = 0.02928217
Iteration 50, loss = 0.02831460
Iteration 51, loss = 0.02732616
Iteration 52, loss = 0.02644080
Iteration 53, loss = 0.02560185
Iteration 54, loss = 0.02484569
Iteration 55, loss = 0.02418549
Iteration 56, loss = 0.02340517
Iteration 57, loss = 0.02272919
Iteration 58, loss = 0.02203297
Iteration 59, loss = 0.02112043
Iteration 60, loss = 0.02063046
Iteration 61, loss = 0.01994016
Iteration 62, loss = 0.01924887
Iteration 63, loss = 0.01910325
Iteration 64, loss = 0.01831934
Iteration 65, loss = 0.01767805
Iteration 66, loss = 0.01691636
Iteration 67, loss = 0.01647005
Iteration 68, loss = 0.01620384
Iteration 69, loss = 0.01526220
Iteration 70, loss = 0.01483198
Iteration 71, loss = 0.01436496
Iteration 72, loss = 0.01389096
Iteration 73, loss = 0.01353862
Iteration 74, loss = 0.01308904
Iteration 75, loss = 0.01268391
Iteration 76, loss = 0.01249162
Iteration 77, loss = 0.01204060
Iteration 78, loss = 0.01161766
Iteration 79, loss = 0.01127671
Iteration 80, loss = 0.01098681
Iteration 81, loss = 0.01060422
Iteration 82, loss = 0.01024993
Iteration 83, loss = 0.00992861
Iteration 84, loss = 0.00960921
Iteration 85, loss = 0.00937860
Iteration 86, loss = 0.00901369
Iteration 87, loss = 0.00884907
Iteration 88, loss = 0.00861511
Iteration 89, loss = 0.00833517
Iteration 90, loss = 0.00814979
Iteration 91, loss = 0.00790860
Iteration 92, loss = 0.00763528
Iteration 93, loss = 0.00743009
Iteration 94, loss = 0.00724759
Iteration 95, loss = 0.00718556
Iteration 96, loss = 0.00684780
Iteration 97, loss = 0.00667958
Iteration 98, loss = 0.00652334
Iteration 99, loss = 0.00640447
Iteration 100, loss = 0.00616664
Iteration 101, loss = 0.00601258
Iteration 102, loss = 0.00586360
Iteration 103, loss = 0.00568910
Iteration 104, loss = 0.00558375
Iteration 105, loss = 0.00541229
Iteration 106, loss = 0.00527389
Iteration 107, loss = 0.00513992
Iteration 108, loss = 0.00505067
Iteration 109, loss = 0.00492769
Iteration 110, loss = 0.00481731
Iteration 111, loss = 0.00465749
Iteration 112, loss = 0.00461138
Iteration 113, loss = 0.00447296
Iteration 114, loss = 0.00439295
Iteration 115, loss = 0.00426818
Iteration 116, loss = 0.00417182
Iteration 117, loss = 0.00409175
Iteration 118, loss = 0.00402705
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  95
Error rate: 23.64% ( 591/2500)
 - Class 1:  52, Class 2:  23, Class 3: 460, Class 4:  45, Class 5:  11
Iteration 1, loss = 1.23425126
Iteration 2, loss = 0.69229383
Iteration 3, loss = 0.42479654
Iteration 4, loss = 0.30018130
Iteration 5, loss = 0.23696801
Iteration 6, loss = 0.19822253
Iteration 7, loss = 0.17359560
Iteration 8, loss = 0.15469750
Iteration 9, loss = 0.14199084
Iteration 10, loss = 0.13031386
Iteration 11, loss = 0.12113033
Iteration 12, loss = 0.11376886
Iteration 13, loss = 0.10728175
Iteration 14, loss = 0.10163003
Iteration 15, loss = 0.09704451
Iteration 16, loss = 0.09186777
Iteration 17, loss = 0.08793592
Iteration 18, loss = 0.08387106
Iteration 19, loss = 0.08073123
Iteration 20, loss = 0.07711068
Iteration 21, loss = 0.07400187
Iteration 22, loss = 0.07138803
Iteration 23, loss = 0.06831902
Iteration 24, loss = 0.06586496
Iteration 25, loss = 0.06378461
Iteration 26, loss = 0.06136458
Iteration 27, loss = 0.05925605
Iteration 28, loss = 0.05692004
Iteration 29, loss = 0.05476746
Iteration 30, loss = 0.05339215
Iteration 31, loss = 0.05106330
Iteration 32, loss = 0.04956131
Iteration 33, loss = 0.04768840
Iteration 34, loss = 0.04628421
Iteration 35, loss = 0.04480253
Iteration 36, loss = 0.04340852
Iteration 37, loss = 0.04208460
Iteration 38, loss = 0.04099138
Iteration 39, loss = 0.03933013
Iteration 40, loss = 0.03809950
Iteration 41, loss = 0.03693009
Iteration 42, loss = 0.03575890
Iteration 43, loss = 0.03458703
Iteration 44, loss = 0.03380445
Iteration 45, loss = 0.03322504
Iteration 46, loss = 0.03147171
Iteration 47, loss = 0.03064848
Iteration 48, loss = 0.02975269
Iteration 49, loss = 0.02874324
Iteration 50, loss = 0.02787879
Iteration 51, loss = 0.02698627
Iteration 52, loss = 0.02618498
Iteration 53, loss = 0.02542784
Iteration 54, loss = 0.02471128
Iteration 55, loss = 0.02391375
Iteration 56, loss = 0.02318392
Iteration 57, loss = 0.02274874
Iteration 58, loss = 0.02193683
Iteration 59, loss = 0.02106160
Iteration 60, loss = 0.02043200
Iteration 61, loss = 0.01984691
Iteration 62, loss = 0.01924132
Iteration 63, loss = 0.01855990
Iteration 64, loss = 0.01794041
Iteration 65, loss = 0.01739902
Iteration 66, loss = 0.01686026
Iteration 67, loss = 0.01643296
Iteration 68, loss = 0.01586345
Iteration 69, loss = 0.01558529
Iteration 70, loss = 0.01487724
Iteration 71, loss = 0.01456940
Iteration 72, loss = 0.01409856
Iteration 73, loss = 0.01349242
Iteration 74, loss = 0.01306669
Iteration 75, loss = 0.01280555
Iteration 76, loss = 0.01240052
Iteration 77, loss = 0.01201599
Iteration 78, loss = 0.01179894
Iteration 79, loss = 0.01136401
Iteration 80, loss = 0.01113477
Iteration 81, loss = 0.01067191
Iteration 82, loss = 0.01043089
Iteration 83, loss = 0.01026545
Iteration 84, loss = 0.00989906
Iteration 85, loss = 0.00952953
Iteration 86, loss = 0.00932571
Iteration 87, loss = 0.00901402
Iteration 88, loss = 0.00877810
Iteration 89, loss = 0.00846710
Iteration 90, loss = 0.00826172
Iteration 91, loss = 0.00802726
Iteration 92, loss = 0.00777671
Iteration 93, loss = 0.00758839
Iteration 94, loss = 0.00747542
Iteration 95, loss = 0.00719601
Iteration 96, loss = 0.00706605
Iteration 97, loss = 0.00684274
Iteration 98, loss = 0.00668236
Iteration 99, loss = 0.00653803
Iteration 100, loss = 0.00630094
Iteration 101, loss = 0.00615027
Iteration 102, loss = 0.00598603
Iteration 103, loss = 0.00582431
Iteration 104, loss = 0.00567605
Iteration 105, loss = 0.00551245
Iteration 106, loss = 0.00545456
Iteration 107, loss = 0.00531805
Iteration 108, loss = 0.00516180
Iteration 109, loss = 0.00503690
Iteration 110, loss = 0.00494422
Iteration 111, loss = 0.00477636
Iteration 112, loss = 0.00468881
Iteration 113, loss = 0.00457635
Iteration 114, loss = 0.00445576
Iteration 115, loss = 0.00437824
Iteration 116, loss = 0.00428949
Iteration 117, loss = 0.00423987
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted: 100
Error rate: 24.56% ( 614/2500)
 - Class 1:  21, Class 2:  21, Class 3: 500, Class 4:  36, Class 5:  36
Iteration 1, loss = 1.59601434
Iteration 2, loss = 1.48699273
Iteration 3, loss = 1.40799085
Iteration 4, loss = 1.34249357
Iteration 5, loss = 1.28592090
Iteration 6, loss = 1.23549192
Iteration 7, loss = 1.18967945
Iteration 8, loss = 1.14773455
Iteration 9, loss = 1.10869899
Iteration 10, loss = 1.07227832
Iteration 11, loss = 1.03802980
Iteration 12, loss = 1.00576131
Iteration 13, loss = 0.97493819
Iteration 14, loss = 0.94557247
Iteration 15, loss = 0.91722650
Iteration 16, loss = 0.88968717
Iteration 17, loss = 0.86267065
Iteration 18, loss = 0.83609559
Iteration 19, loss = 0.80968739
Iteration 20, loss = 0.78388415
Iteration 21, loss = 0.75841643
Iteration 22, loss = 0.73352050
Iteration 23, loss = 0.70971561
Iteration 24, loss = 0.68663411
Iteration 25, loss = 0.66491632
Iteration 26, loss = 0.64467050
Iteration 27, loss = 0.62540699
Iteration 28, loss = 0.60754772
Iteration 29, loss = 0.59051817
Iteration 30, loss = 0.57425521
Iteration 31, loss = 0.55880181
Iteration 32, loss = 0.54437764
Iteration 33, loss = 0.53041415
Iteration 34, loss = 0.51712549
Iteration 35, loss = 0.50441753
Iteration 36, loss = 0.49254106
Iteration 37, loss = 0.48066477
Iteration 38, loss = 0.46974506
Iteration 39, loss = 0.45898162
Iteration 40, loss = 0.44873067
Iteration 41, loss = 0.43883174
Iteration 42, loss = 0.42948208
Iteration 43, loss = 0.42039086
Iteration 44, loss = 0.41148556
Iteration 45, loss = 0.40300519
Iteration 46, loss = 0.39509481
Iteration 47, loss = 0.38711345
Iteration 48, loss = 0.37919484
Iteration 49, loss = 0.37192826
Iteration 50, loss = 0.36481065
Iteration 51, loss = 0.35786303
Iteration 52, loss = 0.35107796
Iteration 53, loss = 0.34441511
Iteration 54, loss = 0.33812497
Iteration 55, loss = 0.33180802
Iteration 56, loss = 0.32582442
Iteration 57, loss = 0.31983950
Iteration 58, loss = 0.31421839
Iteration 59, loss = 0.30864024
Iteration 60, loss = 0.30312486
Iteration 61, loss = 0.29770942
Iteration 62, loss = 0.29255238
Iteration 63, loss = 0.28736539
Iteration 64, loss = 0.28211520
Iteration 65, loss = 0.27708183
Iteration 66, loss = 0.27214181
Iteration 67, loss = 0.26715203
Iteration 68, loss = 0.26211245
Iteration 69, loss = 0.25713382
Iteration 70, loss = 0.25199431
Iteration 71, loss = 0.24699903
Iteration 72, loss = 0.24194693
Iteration 73, loss = 0.23684077
Iteration 74, loss = 0.23197453
Iteration 75, loss = 0.22721585
Iteration 76, loss = 0.22273063
Iteration 77, loss = 0.21807659
Iteration 78, loss = 0.21381147
Iteration 79, loss = 0.20950462
Iteration 80, loss = 0.20554944
Iteration 81, loss = 0.20176350
Iteration 82, loss = 0.19797623
Iteration 83, loss = 0.19427588
Iteration 84, loss = 0.19101828
Iteration 85, loss = 0.18752305
Iteration 86, loss = 0.18406426
Iteration 87, loss = 0.18104172
Iteration 88, loss = 0.17795945
Iteration 89, loss = 0.17510938
Iteration 90, loss = 0.17218096
Iteration 91, loss = 0.16950400
Iteration 92, loss = 0.16682985
Iteration 93, loss = 0.16433449
Iteration 94, loss = 0.16167811
Iteration 95, loss = 0.15927734
Iteration 96, loss = 0.15693625
Iteration 97, loss = 0.15467319
Iteration 98, loss = 0.15242866
Iteration 99, loss = 0.15022385
Iteration 100, loss = 0.14811034
Iteration 101, loss = 0.14625651
Iteration 102, loss = 0.14427643
Iteration 103, loss = 0.14219642
Iteration 104, loss = 0.14018635
Iteration 105, loss = 0.13844427
Iteration 106, loss = 0.13662266
Iteration 107, loss = 0.13494566
Iteration 108, loss = 0.13312256
Iteration 109, loss = 0.13148730
Iteration 110, loss = 0.12986854
Iteration 111, loss = 0.12812675
Iteration 112, loss = 0.12670167
Iteration 113, loss = 0.12510510
Iteration 114, loss = 0.12360017
Iteration 115, loss = 0.12208870
Iteration 116, loss = 0.12097608
Iteration 117, loss = 0.11935721
Iteration 118, loss = 0.11786884
Iteration 119, loss = 0.11672163
Iteration 120, loss = 0.11526830
Iteration 121, loss = 0.11434107
Iteration 122, loss = 0.11277227
Iteration 123, loss = 0.11141731
Iteration 124, loss = 0.11029094
Iteration 125, loss = 0.10904865
Iteration 126, loss = 0.10788994
Iteration 127, loss = 0.10661118
Iteration 128, loss = 0.10554242
Iteration 129, loss = 0.10440581
Iteration 130, loss = 0.10333751
Iteration 131, loss = 0.10231185
Iteration 132, loss = 0.10129410
Iteration 133, loss = 0.10020454
Iteration 134, loss = 0.09938206
Iteration 135, loss = 0.09809993
Iteration 136, loss = 0.09731037
Iteration 137, loss = 0.09619663
Iteration 138, loss = 0.09531026
Iteration 139, loss = 0.09445177
Iteration 140, loss = 0.09348990
Iteration 141, loss = 0.09263653
Iteration 142, loss = 0.09176273
Iteration 143, loss = 0.09085194
Iteration 144, loss = 0.08993684
Iteration 145, loss = 0.08906627
Iteration 146, loss = 0.08831655
Iteration 147, loss = 0.08747723
Iteration 148, loss = 0.08658601
Iteration 149, loss = 0.08578830
Iteration 150, loss = 0.08510195
Iteration 151, loss = 0.08425539
Iteration 152, loss = 0.08349429
Iteration 153, loss = 0.08274623
Iteration 154, loss = 0.08224578
Iteration 155, loss = 0.08133842
Iteration 156, loss = 0.08061709
Iteration 157, loss = 0.07995725
Iteration 158, loss = 0.07918473
Iteration 159, loss = 0.07846895
Iteration 160, loss = 0.07786106
Iteration 161, loss = 0.07712375
Iteration 162, loss = 0.07645387
Iteration 163, loss = 0.07590403
Iteration 164, loss = 0.07545464
Iteration 165, loss = 0.07466658
Iteration 166, loss = 0.07401761
Iteration 167, loss = 0.07336354
Iteration 168, loss = 0.07277672
Iteration 169, loss = 0.07223136
Iteration 170, loss = 0.07161105
Iteration 171, loss = 0.07105249
Iteration 172, loss = 0.07049601
Iteration 173, loss = 0.06993817
Iteration 174, loss = 0.06939260
Iteration 175, loss = 0.06884774
Iteration 176, loss = 0.06832470
Iteration 177, loss = 0.06769592
Iteration 178, loss = 0.06708799
Iteration 179, loss = 0.06647517
Iteration 180, loss = 0.06597522
Iteration 181, loss = 0.06544270
Iteration 182, loss = 0.06506514
Iteration 183, loss = 0.06444726
Iteration 184, loss = 0.06405648
Iteration 185, loss = 0.06358029
Iteration 186, loss = 0.06318534
Iteration 187, loss = 0.06266670
Iteration 188, loss = 0.06214967
Iteration 189, loss = 0.06168952
Iteration 190, loss = 0.06131433
Iteration 191, loss = 0.06111171
Iteration 192, loss = 0.06046679
Iteration 193, loss = 0.05997416
Iteration 194, loss = 0.05954950
Iteration 195, loss = 0.05925883
Iteration 196, loss = 0.05883585
Iteration 197, loss = 0.05838109
Iteration 198, loss = 0.05800936
Iteration 199, loss = 0.05756319
Iteration 200, loss = 0.05720773
Number of features extracted:   5
Error rate: 6.96% ( 174/2500)
 - Class 1:  23, Class 2:  16, Class 3:  63, Class 4:  55, Class 5:  17
Iteration 1, loss = 1.57409537
Iteration 2, loss = 1.40082917
Iteration 3, loss = 1.26527002
Iteration 4, loss = 1.15250789
Iteration 5, loss = 1.05421373
Iteration 6, loss = 0.96542868
Iteration 7, loss = 0.88453574
Iteration 8, loss = 0.81095796
Iteration 9, loss = 0.74453540
Iteration 10, loss = 0.68539446
Iteration 11, loss = 0.63262583
Iteration 12, loss = 0.58536804
Iteration 13, loss = 0.54305910
Iteration 14, loss = 0.50477503
Iteration 15, loss = 0.46978738
Iteration 16, loss = 0.43820898
Iteration 17, loss = 0.40963566
Iteration 18, loss = 0.38415039
Iteration 19, loss = 0.36122294
Iteration 20, loss = 0.34084238
Iteration 21, loss = 0.32264651
Iteration 22, loss = 0.30611467
Iteration 23, loss = 0.29136272
Iteration 24, loss = 0.27792138
Iteration 25, loss = 0.26574709
Iteration 26, loss = 0.25458491
Iteration 27, loss = 0.24445504
Iteration 28, loss = 0.23487374
Iteration 29, loss = 0.22609845
Iteration 30, loss = 0.21794475
Iteration 31, loss = 0.21047669
Iteration 32, loss = 0.20369850
Iteration 33, loss = 0.19689982
Iteration 34, loss = 0.19102024
Iteration 35, loss = 0.18521105
Iteration 36, loss = 0.17985757
Iteration 37, loss = 0.17457929
Iteration 38, loss = 0.16973200
Iteration 39, loss = 0.16524743
Iteration 40, loss = 0.16086315
Iteration 41, loss = 0.15702013
Iteration 42, loss = 0.15313266
Iteration 43, loss = 0.14906848
Iteration 44, loss = 0.14563097
Iteration 45, loss = 0.14220494
Iteration 46, loss = 0.13887069
Iteration 47, loss = 0.13596470
Iteration 48, loss = 0.13317608
Iteration 49, loss = 0.12982127
Iteration 50, loss = 0.12718399
Iteration 51, loss = 0.12434247
Iteration 52, loss = 0.12174683
Iteration 53, loss = 0.11938407
Iteration 54, loss = 0.11688751
Iteration 55, loss = 0.11461978
Iteration 56, loss = 0.11241045
Iteration 57, loss = 0.11035063
Iteration 58, loss = 0.10830261
Iteration 59, loss = 0.10611807
Iteration 60, loss = 0.10439542
Iteration 61, loss = 0.10229525
Iteration 62, loss = 0.10047992
Iteration 63, loss = 0.09874729
Iteration 64, loss = 0.09703295
Iteration 65, loss = 0.09524345
Iteration 66, loss = 0.09364690
Iteration 67, loss = 0.09197051
Iteration 68, loss = 0.09078344
Iteration 69, loss = 0.08900990
Iteration 70, loss = 0.08757742
Iteration 71, loss = 0.08612292
Iteration 72, loss = 0.08468237
Iteration 73, loss = 0.08330541
Iteration 74, loss = 0.08195966
Iteration 75, loss = 0.08070464
Iteration 76, loss = 0.07930918
Iteration 77, loss = 0.07810237
Iteration 78, loss = 0.07693642
Iteration 79, loss = 0.07585214
Iteration 80, loss = 0.07466921
Iteration 81, loss = 0.07357256
Iteration 82, loss = 0.07263418
Iteration 83, loss = 0.07151997
Iteration 84, loss = 0.07040202
Iteration 85, loss = 0.06931347
Iteration 86, loss = 0.06833180
Iteration 87, loss = 0.06748444
Iteration 88, loss = 0.06638182
Iteration 89, loss = 0.06544838
Iteration 90, loss = 0.06450774
Iteration 91, loss = 0.06356790
Iteration 92, loss = 0.06281578
Iteration 93, loss = 0.06200337
Iteration 94, loss = 0.06115792
Iteration 95, loss = 0.06034025
Iteration 96, loss = 0.05955237
Iteration 97, loss = 0.05877181
Iteration 98, loss = 0.05802643
Iteration 99, loss = 0.05724222
Iteration 100, loss = 0.05648284
Iteration 101, loss = 0.05579518
Iteration 102, loss = 0.05501171
Iteration 103, loss = 0.05429402
Iteration 104, loss = 0.05386355
Iteration 105, loss = 0.05299392
Iteration 106, loss = 0.05240895
Iteration 107, loss = 0.05180750
Iteration 108, loss = 0.05123351
Iteration 109, loss = 0.05041284
Iteration 110, loss = 0.04975118
Iteration 111, loss = 0.04923875
Iteration 112, loss = 0.04862855
Iteration 113, loss = 0.04801315
Iteration 114, loss = 0.04745650
Iteration 115, loss = 0.04686825
Iteration 116, loss = 0.04638051
Iteration 117, loss = 0.04586399
Iteration 118, loss = 0.04529485
Iteration 119, loss = 0.04470980
Iteration 120, loss = 0.04424467
Iteration 121, loss = 0.04370165
Iteration 122, loss = 0.04332179
Iteration 123, loss = 0.04283206
Iteration 124, loss = 0.04226711
Iteration 125, loss = 0.04178531
Iteration 126, loss = 0.04130332
Iteration 127, loss = 0.04107315
Iteration 128, loss = 0.04046833
Iteration 129, loss = 0.04002035
Iteration 130, loss = 0.03953409
Iteration 131, loss = 0.03912560
Iteration 132, loss = 0.03869010
Iteration 133, loss = 0.03833599
Iteration 134, loss = 0.03794271
Iteration 135, loss = 0.03752218
Iteration 136, loss = 0.03714229
Iteration 137, loss = 0.03680080
Iteration 138, loss = 0.03640839
Iteration 139, loss = 0.03599367
Iteration 140, loss = 0.03564817
Iteration 141, loss = 0.03528266
Iteration 142, loss = 0.03481944
Iteration 143, loss = 0.03450083
Iteration 144, loss = 0.03411275
Iteration 145, loss = 0.03372964
Iteration 146, loss = 0.03348366
Iteration 147, loss = 0.03306311
Iteration 148, loss = 0.03277670
Iteration 149, loss = 0.03247880
Iteration 150, loss = 0.03207816
Iteration 151, loss = 0.03173881
Iteration 152, loss = 0.03143707
Iteration 153, loss = 0.03112774
Iteration 154, loss = 0.03090472
Iteration 155, loss = 0.03053161
Iteration 156, loss = 0.03016673
Iteration 157, loss = 0.02992232
Iteration 158, loss = 0.02959184
Iteration 159, loss = 0.02936182
Iteration 160, loss = 0.02904153
Iteration 161, loss = 0.02876028
Iteration 162, loss = 0.02846155
Iteration 163, loss = 0.02818364
Iteration 164, loss = 0.02798406
Iteration 165, loss = 0.02775245
Iteration 166, loss = 0.02743335
Iteration 167, loss = 0.02719821
Iteration 168, loss = 0.02697257
Iteration 169, loss = 0.02668137
Iteration 170, loss = 0.02643737
Iteration 171, loss = 0.02621628
Iteration 172, loss = 0.02597731
Iteration 173, loss = 0.02576831
Iteration 174, loss = 0.02551627
Iteration 175, loss = 0.02533272
Iteration 176, loss = 0.02511543
Iteration 177, loss = 0.02483563
Iteration 178, loss = 0.02467110
Iteration 179, loss = 0.02443037
Iteration 180, loss = 0.02425329
Iteration 181, loss = 0.02397053
Iteration 182, loss = 0.02381560
Iteration 183, loss = 0.02365169
Iteration 184, loss = 0.02339361
Iteration 185, loss = 0.02318341
Iteration 186, loss = 0.02298821
Iteration 187, loss = 0.02280252
Iteration 188, loss = 0.02264334
Iteration 189, loss = 0.02241519
Iteration 190, loss = 0.02224716
Iteration 191, loss = 0.02207682
Iteration 192, loss = 0.02190318
Iteration 193, loss = 0.02167745
Iteration 194, loss = 0.02150888
Iteration 195, loss = 0.02138110
Iteration 196, loss = 0.02114438
Iteration 197, loss = 0.02091863
Iteration 198, loss = 0.02079860
Iteration 199, loss = 0.02059577
Iteration 200, loss = 0.02047657
Number of features extracted:  10
Error rate: 23.04% ( 576/2500)
 - Class 1:  31, Class 2:  15, Class 3: 495, Class 4:  21, Class 5:  14
Iteration 1, loss = 1.49815973
Iteration 2, loss = 1.25885696
Iteration 3, loss = 1.08696528
Iteration 4, loss = 0.95063867
Iteration 5, loss = 0.83804617
Iteration 6, loss = 0.74405619
Iteration 7, loss = 0.66448703
Iteration 8, loss = 0.59677888
Iteration 9, loss = 0.53942210
Iteration 10, loss = 0.49059794
Iteration 11, loss = 0.44891880
Iteration 12, loss = 0.41318273
Iteration 13, loss = 0.38235001
Iteration 14, loss = 0.35553304
Iteration 15, loss = 0.33228823
Iteration 16, loss = 0.31165060
Iteration 17, loss = 0.29348073
Iteration 18, loss = 0.27712589
Iteration 19, loss = 0.26263766
Iteration 20, loss = 0.24960814
Iteration 21, loss = 0.23774302
Iteration 22, loss = 0.22706392
Iteration 23, loss = 0.21716956
Iteration 24, loss = 0.20841916
Iteration 25, loss = 0.20001812
Iteration 26, loss = 0.19232273
Iteration 27, loss = 0.18535336
Iteration 28, loss = 0.17872496
Iteration 29, loss = 0.17273341
Iteration 30, loss = 0.16732811
Iteration 31, loss = 0.16171010
Iteration 32, loss = 0.15652713
Iteration 33, loss = 0.15191827
Iteration 34, loss = 0.14760742
Iteration 35, loss = 0.14350021
Iteration 36, loss = 0.13933519
Iteration 37, loss = 0.13574421
Iteration 38, loss = 0.13207545
Iteration 39, loss = 0.12890338
Iteration 40, loss = 0.12553642
Iteration 41, loss = 0.12254936
Iteration 42, loss = 0.11964148
Iteration 43, loss = 0.11660847
Iteration 44, loss = 0.11405048
Iteration 45, loss = 0.11156979
Iteration 46, loss = 0.10891890
Iteration 47, loss = 0.10670603
Iteration 48, loss = 0.10408762
Iteration 49, loss = 0.10215157
Iteration 50, loss = 0.10015334
Iteration 51, loss = 0.09793963
Iteration 52, loss = 0.09586842
Iteration 53, loss = 0.09370443
Iteration 54, loss = 0.09190217
Iteration 55, loss = 0.09002111
Iteration 56, loss = 0.08816042
Iteration 57, loss = 0.08650476
Iteration 58, loss = 0.08477556
Iteration 59, loss = 0.08341007
Iteration 60, loss = 0.08187786
Iteration 61, loss = 0.08017446
Iteration 62, loss = 0.07857587
Iteration 63, loss = 0.07716864
Iteration 64, loss = 0.07582837
Iteration 65, loss = 0.07440151
Iteration 66, loss = 0.07312628
Iteration 67, loss = 0.07170900
Iteration 68, loss = 0.07055395
Iteration 69, loss = 0.06926334
Iteration 70, loss = 0.06796423
Iteration 71, loss = 0.06666930
Iteration 72, loss = 0.06553664
Iteration 73, loss = 0.06454616
Iteration 74, loss = 0.06342346
Iteration 75, loss = 0.06222707
Iteration 76, loss = 0.06119929
Iteration 77, loss = 0.06030994
Iteration 78, loss = 0.05922603
Iteration 79, loss = 0.05824358
Iteration 80, loss = 0.05744851
Iteration 81, loss = 0.05648587
Iteration 82, loss = 0.05550908
Iteration 83, loss = 0.05505567
Iteration 84, loss = 0.05413624
Iteration 85, loss = 0.05287335
Iteration 86, loss = 0.05210433
Iteration 87, loss = 0.05133680
Iteration 88, loss = 0.05060350
Iteration 89, loss = 0.04981547
Iteration 90, loss = 0.04897931
Iteration 91, loss = 0.04822814
Iteration 92, loss = 0.04741381
Iteration 93, loss = 0.04668815
Iteration 94, loss = 0.04599437
Iteration 95, loss = 0.04538065
Iteration 96, loss = 0.04461156
Iteration 97, loss = 0.04397590
Iteration 98, loss = 0.04329405
Iteration 99, loss = 0.04266502
Iteration 100, loss = 0.04192351
Iteration 101, loss = 0.04148978
Iteration 102, loss = 0.04080337
Iteration 103, loss = 0.04013441
Iteration 104, loss = 0.03956001
Iteration 105, loss = 0.03885622
Iteration 106, loss = 0.03845075
Iteration 107, loss = 0.03778420
Iteration 108, loss = 0.03724310
Iteration 109, loss = 0.03666466
Iteration 110, loss = 0.03615124
Iteration 111, loss = 0.03554731
Iteration 112, loss = 0.03505339
Iteration 113, loss = 0.03457635
Iteration 114, loss = 0.03410704
Iteration 115, loss = 0.03367898
Iteration 116, loss = 0.03322915
Iteration 117, loss = 0.03272223
Iteration 118, loss = 0.03225694
Iteration 119, loss = 0.03188804
Iteration 120, loss = 0.03130644
Iteration 121, loss = 0.03093394
Iteration 122, loss = 0.03055396
Iteration 123, loss = 0.03006954
Iteration 124, loss = 0.02967505
Iteration 125, loss = 0.02932600
Iteration 126, loss = 0.02894420
Iteration 127, loss = 0.02853678
Iteration 128, loss = 0.02814348
Iteration 129, loss = 0.02775173
Iteration 130, loss = 0.02748003
Iteration 131, loss = 0.02712532
Iteration 132, loss = 0.02673878
Iteration 133, loss = 0.02640999
Iteration 134, loss = 0.02616197
Iteration 135, loss = 0.02577929
Iteration 136, loss = 0.02539674
Iteration 137, loss = 0.02511357
Iteration 138, loss = 0.02479014
Iteration 139, loss = 0.02442350
Iteration 140, loss = 0.02407680
Iteration 141, loss = 0.02377382
Iteration 142, loss = 0.02350204
Iteration 143, loss = 0.02317982
Iteration 144, loss = 0.02292824
Iteration 145, loss = 0.02256404
Iteration 146, loss = 0.02232516
Iteration 147, loss = 0.02204259
Iteration 148, loss = 0.02178306
Iteration 149, loss = 0.02153080
Iteration 150, loss = 0.02124658
Iteration 151, loss = 0.02093469
Iteration 152, loss = 0.02075194
Iteration 153, loss = 0.02044587
Iteration 154, loss = 0.02018102
Iteration 155, loss = 0.01987529
Iteration 156, loss = 0.01964638
Iteration 157, loss = 0.01942960
Iteration 158, loss = 0.01918000
Iteration 159, loss = 0.01896472
Iteration 160, loss = 0.01870512
Iteration 161, loss = 0.01847190
Iteration 162, loss = 0.01827868
Iteration 163, loss = 0.01800041
Iteration 164, loss = 0.01784508
Iteration 165, loss = 0.01759534
Iteration 166, loss = 0.01735202
Iteration 167, loss = 0.01718095
Iteration 168, loss = 0.01695687
Iteration 169, loss = 0.01682362
Iteration 170, loss = 0.01660270
Iteration 171, loss = 0.01632798
Iteration 172, loss = 0.01614666
Iteration 173, loss = 0.01594899
Iteration 174, loss = 0.01576138
Iteration 175, loss = 0.01560572
Iteration 176, loss = 0.01540177
Iteration 177, loss = 0.01517974
Iteration 178, loss = 0.01500895
Iteration 179, loss = 0.01485951
Iteration 180, loss = 0.01463302
Iteration 181, loss = 0.01445356
Iteration 182, loss = 0.01424695
Iteration 183, loss = 0.01408579
Iteration 184, loss = 0.01387187
Iteration 185, loss = 0.01369518
Iteration 186, loss = 0.01352665
Iteration 187, loss = 0.01333429
Iteration 188, loss = 0.01316963
Iteration 189, loss = 0.01299694
Iteration 190, loss = 0.01285833
Iteration 191, loss = 0.01268908
Iteration 192, loss = 0.01253342
Iteration 193, loss = 0.01236952
Iteration 194, loss = 0.01225716
Iteration 195, loss = 0.01207862
Iteration 196, loss = 0.01194106
Iteration 197, loss = 0.01181593
Iteration 198, loss = 0.01163824
Iteration 199, loss = 0.01150672
Iteration 200, loss = 0.01138000
Number of features extracted:  15
Error rate: 8.96% ( 224/2500)
 - Class 1:  17, Class 2:  17, Class 3:  93, Class 4:  66, Class 5:  31
Iteration 1, loss = 1.43928722
Iteration 2, loss = 1.15582492
Iteration 3, loss = 0.95232893
Iteration 4, loss = 0.79370624
Iteration 5, loss = 0.66638585
Iteration 6, loss = 0.56462196
Iteration 7, loss = 0.48300965
Iteration 8, loss = 0.41954648
Iteration 9, loss = 0.36908135
Iteration 10, loss = 0.32910713
Iteration 11, loss = 0.29707631
Iteration 12, loss = 0.27064670
Iteration 13, loss = 0.24894293
Iteration 14, loss = 0.23068590
Iteration 15, loss = 0.21499521
Iteration 16, loss = 0.20177699
Iteration 17, loss = 0.18999875
Iteration 18, loss = 0.17969238
Iteration 19, loss = 0.17097844
Iteration 20, loss = 0.16284244
Iteration 21, loss = 0.15561692
Iteration 22, loss = 0.14918746
Iteration 23, loss = 0.14312488
Iteration 24, loss = 0.13783848
Iteration 25, loss = 0.13302490
Iteration 26, loss = 0.12871055
Iteration 27, loss = 0.12400762
Iteration 28, loss = 0.12023095
Iteration 29, loss = 0.11669928
Iteration 30, loss = 0.11309947
Iteration 31, loss = 0.10980280
Iteration 32, loss = 0.10678794
Iteration 33, loss = 0.10405210
Iteration 34, loss = 0.10106596
Iteration 35, loss = 0.09853737
Iteration 36, loss = 0.09597566
Iteration 37, loss = 0.09386962
Iteration 38, loss = 0.09167079
Iteration 39, loss = 0.08946723
Iteration 40, loss = 0.08722418
Iteration 41, loss = 0.08508227
Iteration 42, loss = 0.08309459
Iteration 43, loss = 0.08129298
Iteration 44, loss = 0.07951154
Iteration 45, loss = 0.07781578
Iteration 46, loss = 0.07618559
Iteration 47, loss = 0.07461490
Iteration 48, loss = 0.07307759
Iteration 49, loss = 0.07154743
Iteration 50, loss = 0.06986678
Iteration 51, loss = 0.06858340
Iteration 52, loss = 0.06719957
Iteration 53, loss = 0.06595400
Iteration 54, loss = 0.06453515
Iteration 55, loss = 0.06327191
Iteration 56, loss = 0.06219351
Iteration 57, loss = 0.06120840
Iteration 58, loss = 0.05980099
Iteration 59, loss = 0.05861296
Iteration 60, loss = 0.05737290
Iteration 61, loss = 0.05635186
Iteration 62, loss = 0.05518371
Iteration 63, loss = 0.05427072
Iteration 64, loss = 0.05314568
Iteration 65, loss = 0.05219782
Iteration 66, loss = 0.05140121
Iteration 67, loss = 0.05030954
Iteration 68, loss = 0.04981284
Iteration 69, loss = 0.04870630
Iteration 70, loss = 0.04801029
Iteration 71, loss = 0.04696998
Iteration 72, loss = 0.04604806
Iteration 73, loss = 0.04511302
Iteration 74, loss = 0.04440315
Iteration 75, loss = 0.04355966
Iteration 76, loss = 0.04293741
Iteration 77, loss = 0.04212129
Iteration 78, loss = 0.04133295
Iteration 79, loss = 0.04060281
Iteration 80, loss = 0.03999278
Iteration 81, loss = 0.03932541
Iteration 82, loss = 0.03861313
Iteration 83, loss = 0.03805441
Iteration 84, loss = 0.03739424
Iteration 85, loss = 0.03670576
Iteration 86, loss = 0.03606042
Iteration 87, loss = 0.03556937
Iteration 88, loss = 0.03493700
Iteration 89, loss = 0.03439354
Iteration 90, loss = 0.03377715
Iteration 91, loss = 0.03323670
Iteration 92, loss = 0.03263850
Iteration 93, loss = 0.03213575
Iteration 94, loss = 0.03170342
Iteration 95, loss = 0.03113628
Iteration 96, loss = 0.03053467
Iteration 97, loss = 0.03028889
Iteration 98, loss = 0.02959730
Iteration 99, loss = 0.02922182
Iteration 100, loss = 0.02866887
Iteration 101, loss = 0.02833520
Iteration 102, loss = 0.02783521
Iteration 103, loss = 0.02730823
Iteration 104, loss = 0.02687068
Iteration 105, loss = 0.02646363
Iteration 106, loss = 0.02613454
Iteration 107, loss = 0.02561247
Iteration 108, loss = 0.02518824
Iteration 109, loss = 0.02492053
Iteration 110, loss = 0.02447692
Iteration 111, loss = 0.02406486
Iteration 112, loss = 0.02366618
Iteration 113, loss = 0.02338355
Iteration 114, loss = 0.02291827
Iteration 115, loss = 0.02264339
Iteration 116, loss = 0.02226067
Iteration 117, loss = 0.02197048
Iteration 118, loss = 0.02160805
Iteration 119, loss = 0.02129078
Iteration 120, loss = 0.02093491
Iteration 121, loss = 0.02066729
Iteration 122, loss = 0.02026430
Iteration 123, loss = 0.01992709
Iteration 124, loss = 0.01964878
Iteration 125, loss = 0.01939856
Iteration 126, loss = 0.01902314
Iteration 127, loss = 0.01870020
Iteration 128, loss = 0.01843584
Iteration 129, loss = 0.01814619
Iteration 130, loss = 0.01790541
Iteration 131, loss = 0.01764294
Iteration 132, loss = 0.01738802
Iteration 133, loss = 0.01712577
Iteration 134, loss = 0.01683618
Iteration 135, loss = 0.01659919
Iteration 136, loss = 0.01634110
Iteration 137, loss = 0.01612585
Iteration 138, loss = 0.01593048
Iteration 139, loss = 0.01571251
Iteration 140, loss = 0.01550076
Iteration 141, loss = 0.01523487
Iteration 142, loss = 0.01503074
Iteration 143, loss = 0.01479325
Iteration 144, loss = 0.01461192
Iteration 145, loss = 0.01439931
Iteration 146, loss = 0.01422728
Iteration 147, loss = 0.01398740
Iteration 148, loss = 0.01379372
Iteration 149, loss = 0.01364672
Iteration 150, loss = 0.01340662
Iteration 151, loss = 0.01330880
Iteration 152, loss = 0.01303921
Iteration 153, loss = 0.01286146
Iteration 154, loss = 0.01274127
Iteration 155, loss = 0.01251233
Iteration 156, loss = 0.01234431
Iteration 157, loss = 0.01217450
Iteration 158, loss = 0.01204032
Iteration 159, loss = 0.01184479
Iteration 160, loss = 0.01172826
Iteration 161, loss = 0.01156216
Iteration 162, loss = 0.01137955
Iteration 163, loss = 0.01126133
Iteration 164, loss = 0.01108436
Iteration 165, loss = 0.01092310
Iteration 166, loss = 0.01081128
Iteration 167, loss = 0.01063299
Iteration 168, loss = 0.01048952
Iteration 169, loss = 0.01037951
Iteration 170, loss = 0.01025599
Iteration 171, loss = 0.01007479
Iteration 172, loss = 0.00995421
Iteration 173, loss = 0.00977431
Iteration 174, loss = 0.00965247
Iteration 175, loss = 0.00955485
Iteration 176, loss = 0.00939718
Iteration 177, loss = 0.00925869
Iteration 178, loss = 0.00916824
Iteration 179, loss = 0.00896515
Iteration 180, loss = 0.00884883
Iteration 181, loss = 0.00874022
Iteration 182, loss = 0.00859508
Iteration 183, loss = 0.00848592
Iteration 184, loss = 0.00836876
Iteration 185, loss = 0.00824745
Iteration 186, loss = 0.00812569
Iteration 187, loss = 0.00801793
Iteration 188, loss = 0.00794910
Iteration 189, loss = 0.00782706
Iteration 190, loss = 0.00770946
Iteration 191, loss = 0.00761704
Iteration 192, loss = 0.00751905
Iteration 193, loss = 0.00742431
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  20
Error rate: 24.32% ( 608/2500)
 - Class 1:  31, Class 2:  15, Class 3: 500, Class 4:  25, Class 5:  37
Iteration 1, loss = 1.40555889
Iteration 2, loss = 1.08131224
Iteration 3, loss = 0.85556848
Iteration 4, loss = 0.69260488
Iteration 5, loss = 0.57067217
Iteration 6, loss = 0.47732613
Iteration 7, loss = 0.40635005
Iteration 8, loss = 0.35191275
Iteration 9, loss = 0.30986905
Iteration 10, loss = 0.27670857
Iteration 11, loss = 0.25031820
Iteration 12, loss = 0.22867886
Iteration 13, loss = 0.21102986
Iteration 14, loss = 0.19620488
Iteration 15, loss = 0.18360751
Iteration 16, loss = 0.17301330
Iteration 17, loss = 0.16297168
Iteration 18, loss = 0.15501484
Iteration 19, loss = 0.14755567
Iteration 20, loss = 0.14085607
Iteration 21, loss = 0.13492915
Iteration 22, loss = 0.12960056
Iteration 23, loss = 0.12456546
Iteration 24, loss = 0.12014938
Iteration 25, loss = 0.11596527
Iteration 26, loss = 0.11192651
Iteration 27, loss = 0.10856985
Iteration 28, loss = 0.10493484
Iteration 29, loss = 0.10157425
Iteration 30, loss = 0.09839013
Iteration 31, loss = 0.09574179
Iteration 32, loss = 0.09310170
Iteration 33, loss = 0.09057448
Iteration 34, loss = 0.08827730
Iteration 35, loss = 0.08574193
Iteration 36, loss = 0.08346063
Iteration 37, loss = 0.08146023
Iteration 38, loss = 0.07917204
Iteration 39, loss = 0.07722211
Iteration 40, loss = 0.07555121
Iteration 41, loss = 0.07385967
Iteration 42, loss = 0.07190819
Iteration 43, loss = 0.07031518
Iteration 44, loss = 0.06877307
Iteration 45, loss = 0.06693010
Iteration 46, loss = 0.06560341
Iteration 47, loss = 0.06410887
Iteration 48, loss = 0.06264921
Iteration 49, loss = 0.06144132
Iteration 50, loss = 0.05989801
Iteration 51, loss = 0.05863545
Iteration 52, loss = 0.05735527
Iteration 53, loss = 0.05613133
Iteration 54, loss = 0.05512896
Iteration 55, loss = 0.05378743
Iteration 56, loss = 0.05265341
Iteration 57, loss = 0.05150895
Iteration 58, loss = 0.05081744
Iteration 59, loss = 0.05010951
Iteration 60, loss = 0.04867391
Iteration 61, loss = 0.04765572
Iteration 62, loss = 0.04665917
Iteration 63, loss = 0.04558035
Iteration 64, loss = 0.04484007
Iteration 65, loss = 0.04399312
Iteration 66, loss = 0.04304843
Iteration 67, loss = 0.04209950
Iteration 68, loss = 0.04133828
Iteration 69, loss = 0.04060366
Iteration 70, loss = 0.03973163
Iteration 71, loss = 0.03903817
Iteration 72, loss = 0.03830727
Iteration 73, loss = 0.03771428
Iteration 74, loss = 0.03742231
Iteration 75, loss = 0.03650535
Iteration 76, loss = 0.03563416
Iteration 77, loss = 0.03485112
Iteration 78, loss = 0.03426634
Iteration 79, loss = 0.03366713
Iteration 80, loss = 0.03291712
Iteration 81, loss = 0.03239262
Iteration 82, loss = 0.03177572
Iteration 83, loss = 0.03110920
Iteration 84, loss = 0.03063718
Iteration 85, loss = 0.03015336
Iteration 86, loss = 0.02956009
Iteration 87, loss = 0.02908990
Iteration 88, loss = 0.02846381
Iteration 89, loss = 0.02805053
Iteration 90, loss = 0.02757200
Iteration 91, loss = 0.02697962
Iteration 92, loss = 0.02650740
Iteration 93, loss = 0.02603175
Iteration 94, loss = 0.02557698
Iteration 95, loss = 0.02512654
Iteration 96, loss = 0.02478776
Iteration 97, loss = 0.02426355
Iteration 98, loss = 0.02395620
Iteration 99, loss = 0.02344432
Iteration 100, loss = 0.02302597
Iteration 101, loss = 0.02267331
Iteration 102, loss = 0.02226673
Iteration 103, loss = 0.02193581
Iteration 104, loss = 0.02152188
Iteration 105, loss = 0.02115136
Iteration 106, loss = 0.02080171
Iteration 107, loss = 0.02053922
Iteration 108, loss = 0.02011659
Iteration 109, loss = 0.01987480
Iteration 110, loss = 0.01941331
Iteration 111, loss = 0.01915546
Iteration 112, loss = 0.01882885
Iteration 113, loss = 0.01849569
Iteration 114, loss = 0.01828214
Iteration 115, loss = 0.01800368
Iteration 116, loss = 0.01764402
Iteration 117, loss = 0.01738698
Iteration 118, loss = 0.01708339
Iteration 119, loss = 0.01683047
Iteration 120, loss = 0.01658574
Iteration 121, loss = 0.01624412
Iteration 122, loss = 0.01602458
Iteration 123, loss = 0.01576828
Iteration 124, loss = 0.01560484
Iteration 125, loss = 0.01528913
Iteration 126, loss = 0.01501170
Iteration 127, loss = 0.01482727
Iteration 128, loss = 0.01462059
Iteration 129, loss = 0.01434162
Iteration 130, loss = 0.01409078
Iteration 131, loss = 0.01390680
Iteration 132, loss = 0.01370792
Iteration 133, loss = 0.01359157
Iteration 134, loss = 0.01324618
Iteration 135, loss = 0.01308538
Iteration 136, loss = 0.01295193
Iteration 137, loss = 0.01268593
Iteration 138, loss = 0.01247706
Iteration 139, loss = 0.01225824
Iteration 140, loss = 0.01204350
Iteration 141, loss = 0.01187344
Iteration 142, loss = 0.01165461
Iteration 143, loss = 0.01151917
Iteration 144, loss = 0.01132837
Iteration 145, loss = 0.01113731
Iteration 146, loss = 0.01098231
Iteration 147, loss = 0.01084459
Iteration 148, loss = 0.01073650
Iteration 149, loss = 0.01049022
Iteration 150, loss = 0.01037754
Iteration 151, loss = 0.01020137
Iteration 152, loss = 0.01005276
Iteration 153, loss = 0.00990834
Iteration 154, loss = 0.00976306
Iteration 155, loss = 0.00964156
Iteration 156, loss = 0.00950874
Iteration 157, loss = 0.00937875
Iteration 158, loss = 0.00926748
Iteration 159, loss = 0.00908934
Iteration 160, loss = 0.00896596
Iteration 161, loss = 0.00885342
Iteration 162, loss = 0.00874729
Iteration 163, loss = 0.00861893
Iteration 164, loss = 0.00849902
Iteration 165, loss = 0.00838950
Iteration 166, loss = 0.00827195
Iteration 167, loss = 0.00816077
Iteration 168, loss = 0.00805021
Iteration 169, loss = 0.00793520
Iteration 170, loss = 0.00782928
Iteration 171, loss = 0.00773594
Iteration 172, loss = 0.00763982
Iteration 173, loss = 0.00754720
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  25
Error rate: 24.52% ( 613/2500)
 - Class 1:  23, Class 2:  14, Class 3: 500, Class 4:  26, Class 5:  50
Iteration 1, loss = 1.44574673
Iteration 2, loss = 1.08175742
Iteration 3, loss = 0.85371745
Iteration 4, loss = 0.68821348
Iteration 5, loss = 0.56393863
Iteration 6, loss = 0.47094665
Iteration 7, loss = 0.40010488
Iteration 8, loss = 0.34669956
Iteration 9, loss = 0.30491118
Iteration 10, loss = 0.27206477
Iteration 11, loss = 0.24587900
Iteration 12, loss = 0.22443931
Iteration 13, loss = 0.20657671
Iteration 14, loss = 0.19159300
Iteration 15, loss = 0.17913249
Iteration 16, loss = 0.16844063
Iteration 17, loss = 0.15898464
Iteration 18, loss = 0.15063736
Iteration 19, loss = 0.14324956
Iteration 20, loss = 0.13678891
Iteration 21, loss = 0.13063213
Iteration 22, loss = 0.12494537
Iteration 23, loss = 0.12052959
Iteration 24, loss = 0.11534212
Iteration 25, loss = 0.11136403
Iteration 26, loss = 0.10776092
Iteration 27, loss = 0.10402753
Iteration 28, loss = 0.10028164
Iteration 29, loss = 0.09716285
Iteration 30, loss = 0.09400213
Iteration 31, loss = 0.09108490
Iteration 32, loss = 0.08837523
Iteration 33, loss = 0.08571247
Iteration 34, loss = 0.08338299
Iteration 35, loss = 0.08119734
Iteration 36, loss = 0.07872714
Iteration 37, loss = 0.07671451
Iteration 38, loss = 0.07469496
Iteration 39, loss = 0.07299010
Iteration 40, loss = 0.07074284
Iteration 41, loss = 0.06910982
Iteration 42, loss = 0.06731514
Iteration 43, loss = 0.06597157
Iteration 44, loss = 0.06393537
Iteration 45, loss = 0.06238687
Iteration 46, loss = 0.06086714
Iteration 47, loss = 0.05940414
Iteration 48, loss = 0.05816462
Iteration 49, loss = 0.05682604
Iteration 50, loss = 0.05541717
Iteration 51, loss = 0.05425755
Iteration 52, loss = 0.05295532
Iteration 53, loss = 0.05193516
Iteration 54, loss = 0.05068116
Iteration 55, loss = 0.04953277
Iteration 56, loss = 0.04843313
Iteration 57, loss = 0.04733308
Iteration 58, loss = 0.04631306
Iteration 59, loss = 0.04535973
Iteration 60, loss = 0.04458335
Iteration 61, loss = 0.04324492
Iteration 62, loss = 0.04249878
Iteration 63, loss = 0.04153467
Iteration 64, loss = 0.04067286
Iteration 65, loss = 0.03972578
Iteration 66, loss = 0.03892286
Iteration 67, loss = 0.03816794
Iteration 68, loss = 0.03735275
Iteration 69, loss = 0.03652458
Iteration 70, loss = 0.03574623
Iteration 71, loss = 0.03510806
Iteration 72, loss = 0.03436102
Iteration 73, loss = 0.03359604
Iteration 74, loss = 0.03290999
Iteration 75, loss = 0.03233920
Iteration 76, loss = 0.03172508
Iteration 77, loss = 0.03105092
Iteration 78, loss = 0.03040865
Iteration 79, loss = 0.02987514
Iteration 80, loss = 0.02918524
Iteration 81, loss = 0.02870264
Iteration 82, loss = 0.02814440
Iteration 83, loss = 0.02745315
Iteration 84, loss = 0.02701591
Iteration 85, loss = 0.02650603
Iteration 86, loss = 0.02599838
Iteration 87, loss = 0.02540343
Iteration 88, loss = 0.02489582
Iteration 89, loss = 0.02437730
Iteration 90, loss = 0.02396039
Iteration 91, loss = 0.02368820
Iteration 92, loss = 0.02312882
Iteration 93, loss = 0.02266076
Iteration 94, loss = 0.02214906
Iteration 95, loss = 0.02172946
Iteration 96, loss = 0.02142679
Iteration 97, loss = 0.02096713
Iteration 98, loss = 0.02058027
Iteration 99, loss = 0.02011782
Iteration 100, loss = 0.01977469
Iteration 101, loss = 0.01939250
Iteration 102, loss = 0.01907574
Iteration 103, loss = 0.01873668
Iteration 104, loss = 0.01832552
Iteration 105, loss = 0.01808543
Iteration 106, loss = 0.01782068
Iteration 107, loss = 0.01746746
Iteration 108, loss = 0.01702113
Iteration 109, loss = 0.01675178
Iteration 110, loss = 0.01639525
Iteration 111, loss = 0.01614968
Iteration 112, loss = 0.01594542
Iteration 113, loss = 0.01565164
Iteration 114, loss = 0.01538040
Iteration 115, loss = 0.01504082
Iteration 116, loss = 0.01484859
Iteration 117, loss = 0.01453710
Iteration 118, loss = 0.01426235
Iteration 119, loss = 0.01400958
Iteration 120, loss = 0.01377525
Iteration 121, loss = 0.01357068
Iteration 122, loss = 0.01335524
Iteration 123, loss = 0.01307318
Iteration 124, loss = 0.01281692
Iteration 125, loss = 0.01264663
Iteration 126, loss = 0.01240844
Iteration 127, loss = 0.01221094
Iteration 128, loss = 0.01199642
Iteration 129, loss = 0.01182514
Iteration 130, loss = 0.01165031
Iteration 131, loss = 0.01141937
Iteration 132, loss = 0.01121786
Iteration 133, loss = 0.01102256
Iteration 134, loss = 0.01082697
Iteration 135, loss = 0.01063999
Iteration 136, loss = 0.01053023
Iteration 137, loss = 0.01033298
Iteration 138, loss = 0.01015139
Iteration 139, loss = 0.00999485
Iteration 140, loss = 0.00983176
Iteration 141, loss = 0.00963566
Iteration 142, loss = 0.00951815
Iteration 143, loss = 0.00936144
Iteration 144, loss = 0.00922662
Iteration 145, loss = 0.00903751
Iteration 146, loss = 0.00896116
Iteration 147, loss = 0.00875102
Iteration 148, loss = 0.00859030
Iteration 149, loss = 0.00848975
Iteration 150, loss = 0.00833866
Iteration 151, loss = 0.00817946
Iteration 152, loss = 0.00803688
Iteration 153, loss = 0.00789278
Iteration 154, loss = 0.00776963
Iteration 155, loss = 0.00763667
Iteration 156, loss = 0.00750117
Iteration 157, loss = 0.00738980
Iteration 158, loss = 0.00727429
Iteration 159, loss = 0.00711485
Iteration 160, loss = 0.00699487
Iteration 161, loss = 0.00689246
Iteration 162, loss = 0.00674137
Iteration 163, loss = 0.00663514
Iteration 164, loss = 0.00652703
Iteration 165, loss = 0.00640938
Iteration 166, loss = 0.00631528
Iteration 167, loss = 0.00623569
Iteration 168, loss = 0.00613683
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  30
Error rate: 24.32% ( 608/2500)
 - Class 1:  18, Class 2:  18, Class 3: 498, Class 4:  23, Class 5:  51
Iteration 1, loss = 1.39415084
Iteration 2, loss = 1.01560110
Iteration 3, loss = 0.76920085
Iteration 4, loss = 0.59898520
Iteration 5, loss = 0.47949888
Iteration 6, loss = 0.39471736
Iteration 7, loss = 0.33382625
Iteration 8, loss = 0.28849395
Iteration 9, loss = 0.25448879
Iteration 10, loss = 0.22800948
Iteration 11, loss = 0.20711324
Iteration 12, loss = 0.19034053
Iteration 13, loss = 0.17637516
Iteration 14, loss = 0.16451536
Iteration 15, loss = 0.15425290
Iteration 16, loss = 0.14565146
Iteration 17, loss = 0.13774808
Iteration 18, loss = 0.13131399
Iteration 19, loss = 0.12478207
Iteration 20, loss = 0.11952031
Iteration 21, loss = 0.11467413
Iteration 22, loss = 0.10992638
Iteration 23, loss = 0.10584078
Iteration 24, loss = 0.10197408
Iteration 25, loss = 0.09834526
Iteration 26, loss = 0.09513647
Iteration 27, loss = 0.09190480
Iteration 28, loss = 0.08866587
Iteration 29, loss = 0.08608950
Iteration 30, loss = 0.08341991
Iteration 31, loss = 0.08054324
Iteration 32, loss = 0.07841880
Iteration 33, loss = 0.07594107
Iteration 34, loss = 0.07373813
Iteration 35, loss = 0.07167332
Iteration 36, loss = 0.06988201
Iteration 37, loss = 0.06796177
Iteration 38, loss = 0.06590350
Iteration 39, loss = 0.06415074
Iteration 40, loss = 0.06253171
Iteration 41, loss = 0.06078918
Iteration 42, loss = 0.05923671
Iteration 43, loss = 0.05792115
Iteration 44, loss = 0.05662793
Iteration 45, loss = 0.05476076
Iteration 46, loss = 0.05348372
Iteration 47, loss = 0.05220445
Iteration 48, loss = 0.05079806
Iteration 49, loss = 0.04953600
Iteration 50, loss = 0.04836050
Iteration 51, loss = 0.04726884
Iteration 52, loss = 0.04605977
Iteration 53, loss = 0.04521224
Iteration 54, loss = 0.04394052
Iteration 55, loss = 0.04312977
Iteration 56, loss = 0.04211825
Iteration 57, loss = 0.04126736
Iteration 58, loss = 0.04021420
Iteration 59, loss = 0.03913464
Iteration 60, loss = 0.03826077
Iteration 61, loss = 0.03769888
Iteration 62, loss = 0.03664633
Iteration 63, loss = 0.03588083
Iteration 64, loss = 0.03522815
Iteration 65, loss = 0.03436427
Iteration 66, loss = 0.03353295
Iteration 67, loss = 0.03299085
Iteration 68, loss = 0.03216760
Iteration 69, loss = 0.03144097
Iteration 70, loss = 0.03095815
Iteration 71, loss = 0.03015664
Iteration 72, loss = 0.02961157
Iteration 73, loss = 0.02899563
Iteration 74, loss = 0.02827310
Iteration 75, loss = 0.02772266
Iteration 76, loss = 0.02712414
Iteration 77, loss = 0.02659386
Iteration 78, loss = 0.02609087
Iteration 79, loss = 0.02553016
Iteration 80, loss = 0.02496858
Iteration 81, loss = 0.02448842
Iteration 82, loss = 0.02413454
Iteration 83, loss = 0.02346952
Iteration 84, loss = 0.02298398
Iteration 85, loss = 0.02255532
Iteration 86, loss = 0.02210858
Iteration 87, loss = 0.02163278
Iteration 88, loss = 0.02113088
Iteration 89, loss = 0.02075717
Iteration 90, loss = 0.02040451
Iteration 91, loss = 0.01995032
Iteration 92, loss = 0.01951489
Iteration 93, loss = 0.01911556
Iteration 94, loss = 0.01874421
Iteration 95, loss = 0.01834463
Iteration 96, loss = 0.01794075
Iteration 97, loss = 0.01765812
Iteration 98, loss = 0.01726959
Iteration 99, loss = 0.01686411
Iteration 100, loss = 0.01663614
Iteration 101, loss = 0.01630975
Iteration 102, loss = 0.01598328
Iteration 103, loss = 0.01557157
Iteration 104, loss = 0.01526038
Iteration 105, loss = 0.01494398
Iteration 106, loss = 0.01468720
Iteration 107, loss = 0.01434105
Iteration 108, loss = 0.01411674
Iteration 109, loss = 0.01380222
Iteration 110, loss = 0.01344674
Iteration 111, loss = 0.01324685
Iteration 112, loss = 0.01300645
Iteration 113, loss = 0.01270995
Iteration 114, loss = 0.01244786
Iteration 115, loss = 0.01224614
Iteration 116, loss = 0.01202383
Iteration 117, loss = 0.01176297
Iteration 118, loss = 0.01154800
Iteration 119, loss = 0.01128015
Iteration 120, loss = 0.01107308
Iteration 121, loss = 0.01086380
Iteration 122, loss = 0.01067693
Iteration 123, loss = 0.01049552
Iteration 124, loss = 0.01030915
Iteration 125, loss = 0.01010563
Iteration 126, loss = 0.00998633
Iteration 127, loss = 0.00978634
Iteration 128, loss = 0.00958848
Iteration 129, loss = 0.00938283
Iteration 130, loss = 0.00928487
Iteration 131, loss = 0.00912056
Iteration 132, loss = 0.00893534
Iteration 133, loss = 0.00880584
Iteration 134, loss = 0.00859541
Iteration 135, loss = 0.00848130
Iteration 136, loss = 0.00834109
Iteration 137, loss = 0.00818625
Iteration 138, loss = 0.00805736
Iteration 139, loss = 0.00793031
Iteration 140, loss = 0.00775831
Iteration 141, loss = 0.00761504
Iteration 142, loss = 0.00754796
Iteration 143, loss = 0.00743115
Iteration 144, loss = 0.00728691
Iteration 145, loss = 0.00718689
Iteration 146, loss = 0.00703676
Iteration 147, loss = 0.00689620
Iteration 148, loss = 0.00682734
Iteration 149, loss = 0.00673495
Iteration 150, loss = 0.00663749
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  35
Error rate: 23.76% ( 594/2500)
 - Class 1:  23, Class 2:  18, Class 3: 500, Class 4:  30, Class 5:  23
Iteration 1, loss = 1.39941972
Iteration 2, loss = 1.00868539
Iteration 3, loss = 0.75486944
Iteration 4, loss = 0.57586806
Iteration 5, loss = 0.45182994
Iteration 6, loss = 0.36704206
Iteration 7, loss = 0.30853837
Iteration 8, loss = 0.26572702
Iteration 9, loss = 0.23498412
Iteration 10, loss = 0.21111431
Iteration 11, loss = 0.19293010
Iteration 12, loss = 0.17735915
Iteration 13, loss = 0.16489564
Iteration 14, loss = 0.15483936
Iteration 15, loss = 0.14578189
Iteration 16, loss = 0.13815795
Iteration 17, loss = 0.13129362
Iteration 18, loss = 0.12540251
Iteration 19, loss = 0.11997323
Iteration 20, loss = 0.11519009
Iteration 21, loss = 0.11079801
Iteration 22, loss = 0.10671118
Iteration 23, loss = 0.10287282
Iteration 24, loss = 0.09966705
Iteration 25, loss = 0.09638443
Iteration 26, loss = 0.09342442
Iteration 27, loss = 0.09043630
Iteration 28, loss = 0.08782913
Iteration 29, loss = 0.08489795
Iteration 30, loss = 0.08256215
Iteration 31, loss = 0.08009453
Iteration 32, loss = 0.07791686
Iteration 33, loss = 0.07557990
Iteration 34, loss = 0.07331917
Iteration 35, loss = 0.07162420
Iteration 36, loss = 0.06987425
Iteration 37, loss = 0.06799200
Iteration 38, loss = 0.06591258
Iteration 39, loss = 0.06444270
Iteration 40, loss = 0.06275135
Iteration 41, loss = 0.06113010
Iteration 42, loss = 0.05963826
Iteration 43, loss = 0.05794470
Iteration 44, loss = 0.05668928
Iteration 45, loss = 0.05528847
Iteration 46, loss = 0.05392259
Iteration 47, loss = 0.05295677
Iteration 48, loss = 0.05162023
Iteration 49, loss = 0.05040774
Iteration 50, loss = 0.04898092
Iteration 51, loss = 0.04791698
Iteration 52, loss = 0.04674279
Iteration 53, loss = 0.04573234
Iteration 54, loss = 0.04465480
Iteration 55, loss = 0.04354882
Iteration 56, loss = 0.04257843
Iteration 57, loss = 0.04163830
Iteration 58, loss = 0.04067318
Iteration 59, loss = 0.03988435
Iteration 60, loss = 0.03927444
Iteration 61, loss = 0.03808266
Iteration 62, loss = 0.03725781
Iteration 63, loss = 0.03660208
Iteration 64, loss = 0.03580954
Iteration 65, loss = 0.03493955
Iteration 66, loss = 0.03405959
Iteration 67, loss = 0.03334681
Iteration 68, loss = 0.03283851
Iteration 69, loss = 0.03192457
Iteration 70, loss = 0.03155150
Iteration 71, loss = 0.03072669
Iteration 72, loss = 0.03002668
Iteration 73, loss = 0.02926526
Iteration 74, loss = 0.02865384
Iteration 75, loss = 0.02804337
Iteration 76, loss = 0.02749481
Iteration 77, loss = 0.02696046
Iteration 78, loss = 0.02638660
Iteration 79, loss = 0.02592012
Iteration 80, loss = 0.02522572
Iteration 81, loss = 0.02485416
Iteration 82, loss = 0.02420893
Iteration 83, loss = 0.02371534
Iteration 84, loss = 0.02309354
Iteration 85, loss = 0.02274416
Iteration 86, loss = 0.02223709
Iteration 87, loss = 0.02181381
Iteration 88, loss = 0.02151578
Iteration 89, loss = 0.02095316
Iteration 90, loss = 0.02047221
Iteration 91, loss = 0.02002704
Iteration 92, loss = 0.01958726
Iteration 93, loss = 0.01927652
Iteration 94, loss = 0.01879411
Iteration 95, loss = 0.01855531
Iteration 96, loss = 0.01811576
Iteration 97, loss = 0.01774104
Iteration 98, loss = 0.01740486
Iteration 99, loss = 0.01703674
Iteration 100, loss = 0.01665843
Iteration 101, loss = 0.01632850
Iteration 102, loss = 0.01608046
Iteration 103, loss = 0.01568479
Iteration 104, loss = 0.01543297
Iteration 105, loss = 0.01501757
Iteration 106, loss = 0.01474255
Iteration 107, loss = 0.01444032
Iteration 108, loss = 0.01417516
Iteration 109, loss = 0.01389003
Iteration 110, loss = 0.01358546
Iteration 111, loss = 0.01338028
Iteration 112, loss = 0.01305074
Iteration 113, loss = 0.01284358
Iteration 114, loss = 0.01250588
Iteration 115, loss = 0.01222461
Iteration 116, loss = 0.01204385
Iteration 117, loss = 0.01184960
Iteration 118, loss = 0.01157277
Iteration 119, loss = 0.01132489
Iteration 120, loss = 0.01113580
Iteration 121, loss = 0.01086791
Iteration 122, loss = 0.01067472
Iteration 123, loss = 0.01048890
Iteration 124, loss = 0.01028466
Iteration 125, loss = 0.01001173
Iteration 126, loss = 0.00983729
Iteration 127, loss = 0.00964927
Iteration 128, loss = 0.00943883
Iteration 129, loss = 0.00925151
Iteration 130, loss = 0.00906109
Iteration 131, loss = 0.00892303
Iteration 132, loss = 0.00869572
Iteration 133, loss = 0.00851642
Iteration 134, loss = 0.00834598
Iteration 135, loss = 0.00818094
Iteration 136, loss = 0.00801443
Iteration 137, loss = 0.00785872
Iteration 138, loss = 0.00769057
Iteration 139, loss = 0.00756456
Iteration 140, loss = 0.00743865
Iteration 141, loss = 0.00728380
Iteration 142, loss = 0.00716087
Iteration 143, loss = 0.00702284
Iteration 144, loss = 0.00692924
Iteration 145, loss = 0.00677415
Iteration 146, loss = 0.00664219
Iteration 147, loss = 0.00650839
Iteration 148, loss = 0.00642607
Iteration 149, loss = 0.00628670
Iteration 150, loss = 0.00613928
Iteration 151, loss = 0.00604128
Iteration 152, loss = 0.00595304
Iteration 153, loss = 0.00583825
Iteration 154, loss = 0.00569940
Iteration 155, loss = 0.00561705
Iteration 156, loss = 0.00551924
Iteration 157, loss = 0.00542806
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  40
Error rate: 24.56% ( 614/2500)
 - Class 1:  42, Class 2:  17, Class 3: 494, Class 4:  40, Class 5:  21
Iteration 1, loss = 1.37012576
Iteration 2, loss = 0.96994594
Iteration 3, loss = 0.70154950
Iteration 4, loss = 0.52410119
Iteration 5, loss = 0.40749936
Iteration 6, loss = 0.33063128
Iteration 7, loss = 0.27824756
Iteration 8, loss = 0.24153693
Iteration 9, loss = 0.21375077
Iteration 10, loss = 0.19298062
Iteration 11, loss = 0.17682049
Iteration 12, loss = 0.16323499
Iteration 13, loss = 0.15230767
Iteration 14, loss = 0.14281866
Iteration 15, loss = 0.13493514
Iteration 16, loss = 0.12820440
Iteration 17, loss = 0.12191935
Iteration 18, loss = 0.11613690
Iteration 19, loss = 0.11191468
Iteration 20, loss = 0.10661366
Iteration 21, loss = 0.10254495
Iteration 22, loss = 0.09870736
Iteration 23, loss = 0.09545167
Iteration 24, loss = 0.09185878
Iteration 25, loss = 0.08860561
Iteration 26, loss = 0.08582073
Iteration 27, loss = 0.08286618
Iteration 28, loss = 0.08019570
Iteration 29, loss = 0.07753886
Iteration 30, loss = 0.07512129
Iteration 31, loss = 0.07301722
Iteration 32, loss = 0.07093725
Iteration 33, loss = 0.06874582
Iteration 34, loss = 0.06666354
Iteration 35, loss = 0.06520214
Iteration 36, loss = 0.06312402
Iteration 37, loss = 0.06121820
Iteration 38, loss = 0.05947196
Iteration 39, loss = 0.05809395
Iteration 40, loss = 0.05653658
Iteration 41, loss = 0.05522849
Iteration 42, loss = 0.05364862
Iteration 43, loss = 0.05223586
Iteration 44, loss = 0.05089970
Iteration 45, loss = 0.04944945
Iteration 46, loss = 0.04825248
Iteration 47, loss = 0.04709901
Iteration 48, loss = 0.04577633
Iteration 49, loss = 0.04476590
Iteration 50, loss = 0.04366773
Iteration 51, loss = 0.04295989
Iteration 52, loss = 0.04163857
Iteration 53, loss = 0.04043385
Iteration 54, loss = 0.03942798
Iteration 55, loss = 0.03839520
Iteration 56, loss = 0.03764115
Iteration 57, loss = 0.03673989
Iteration 58, loss = 0.03568281
Iteration 59, loss = 0.03499468
Iteration 60, loss = 0.03416892
Iteration 61, loss = 0.03336676
Iteration 62, loss = 0.03237649
Iteration 63, loss = 0.03171778
Iteration 64, loss = 0.03090199
Iteration 65, loss = 0.03017801
Iteration 66, loss = 0.02954193
Iteration 67, loss = 0.02877870
Iteration 68, loss = 0.02809745
Iteration 69, loss = 0.02751553
Iteration 70, loss = 0.02679259
Iteration 71, loss = 0.02658433
Iteration 72, loss = 0.02556459
Iteration 73, loss = 0.02504660
Iteration 74, loss = 0.02446086
Iteration 75, loss = 0.02386704
Iteration 76, loss = 0.02332316
Iteration 77, loss = 0.02276416
Iteration 78, loss = 0.02222614
Iteration 79, loss = 0.02184365
Iteration 80, loss = 0.02113645
Iteration 81, loss = 0.02060776
Iteration 82, loss = 0.02020917
Iteration 83, loss = 0.01975750
Iteration 84, loss = 0.01921790
Iteration 85, loss = 0.01885268
Iteration 86, loss = 0.01840844
Iteration 87, loss = 0.01797200
Iteration 88, loss = 0.01763932
Iteration 89, loss = 0.01733799
Iteration 90, loss = 0.01699327
Iteration 91, loss = 0.01633960
Iteration 92, loss = 0.01607086
Iteration 93, loss = 0.01564492
Iteration 94, loss = 0.01534583
Iteration 95, loss = 0.01497011
Iteration 96, loss = 0.01458047
Iteration 97, loss = 0.01425564
Iteration 98, loss = 0.01396794
Iteration 99, loss = 0.01362706
Iteration 100, loss = 0.01339084
Iteration 101, loss = 0.01308347
Iteration 102, loss = 0.01278961
Iteration 103, loss = 0.01248035
Iteration 104, loss = 0.01217594
Iteration 105, loss = 0.01194400
Iteration 106, loss = 0.01171206
Iteration 107, loss = 0.01151324
Iteration 108, loss = 0.01126279
Iteration 109, loss = 0.01102080
Iteration 110, loss = 0.01074693
Iteration 111, loss = 0.01045022
Iteration 112, loss = 0.01026232
Iteration 113, loss = 0.01006922
Iteration 114, loss = 0.00984291
Iteration 115, loss = 0.00966714
Iteration 116, loss = 0.00950300
Iteration 117, loss = 0.00926365
Iteration 118, loss = 0.00905874
Iteration 119, loss = 0.00884649
Iteration 120, loss = 0.00864852
Iteration 121, loss = 0.00855523
Iteration 122, loss = 0.00838565
Iteration 123, loss = 0.00820736
Iteration 124, loss = 0.00796213
Iteration 125, loss = 0.00784995
Iteration 126, loss = 0.00768312
Iteration 127, loss = 0.00760064
Iteration 128, loss = 0.00739668
Iteration 129, loss = 0.00725276
Iteration 130, loss = 0.00709577
Iteration 131, loss = 0.00693516
Iteration 132, loss = 0.00682622
Iteration 133, loss = 0.00670553
Iteration 134, loss = 0.00656450
Iteration 135, loss = 0.00646676
Iteration 136, loss = 0.00634714
Iteration 137, loss = 0.00621535
Iteration 138, loss = 0.00608099
Iteration 139, loss = 0.00597223
Iteration 140, loss = 0.00586954
Iteration 141, loss = 0.00574987
Iteration 142, loss = 0.00568092
Iteration 143, loss = 0.00553088
Iteration 144, loss = 0.00544323
Iteration 145, loss = 0.00536255
Iteration 146, loss = 0.00527196
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  45
Error rate: 24.68% ( 617/2500)
 - Class 1:  36, Class 2:  16, Class 3: 500, Class 4:  38, Class 5:  27
Iteration 1, loss = 1.35903250
Iteration 2, loss = 0.90820574
Iteration 3, loss = 0.64010737
Iteration 4, loss = 0.47461783
Iteration 5, loss = 0.37071951
Iteration 6, loss = 0.30295701
Iteration 7, loss = 0.25757068
Iteration 8, loss = 0.22453237
Iteration 9, loss = 0.20046152
Iteration 10, loss = 0.18191989
Iteration 11, loss = 0.16629336
Iteration 12, loss = 0.15452506
Iteration 13, loss = 0.14418618
Iteration 14, loss = 0.13561066
Iteration 15, loss = 0.12795883
Iteration 16, loss = 0.12253704
Iteration 17, loss = 0.11591794
Iteration 18, loss = 0.11077861
Iteration 19, loss = 0.10583059
Iteration 20, loss = 0.10158801
Iteration 21, loss = 0.09798260
Iteration 22, loss = 0.09400071
Iteration 23, loss = 0.09049356
Iteration 24, loss = 0.08770715
Iteration 25, loss = 0.08440136
Iteration 26, loss = 0.08157018
Iteration 27, loss = 0.07902834
Iteration 28, loss = 0.07612194
Iteration 29, loss = 0.07396427
Iteration 30, loss = 0.07170226
Iteration 31, loss = 0.06967712
Iteration 32, loss = 0.06735901
Iteration 33, loss = 0.06528862
Iteration 34, loss = 0.06366017
Iteration 35, loss = 0.06170717
Iteration 36, loss = 0.05991105
Iteration 37, loss = 0.05819343
Iteration 38, loss = 0.05669607
Iteration 39, loss = 0.05526634
Iteration 40, loss = 0.05352947
Iteration 41, loss = 0.05217361
Iteration 42, loss = 0.05055862
Iteration 43, loss = 0.04948460
Iteration 44, loss = 0.04811767
Iteration 45, loss = 0.04672822
Iteration 46, loss = 0.04552797
Iteration 47, loss = 0.04436577
Iteration 48, loss = 0.04335349
Iteration 49, loss = 0.04256832
Iteration 50, loss = 0.04110794
Iteration 51, loss = 0.03996073
Iteration 52, loss = 0.03898564
Iteration 53, loss = 0.03800295
Iteration 54, loss = 0.03693587
Iteration 55, loss = 0.03621655
Iteration 56, loss = 0.03522338
Iteration 57, loss = 0.03437805
Iteration 58, loss = 0.03341417
Iteration 59, loss = 0.03265247
Iteration 60, loss = 0.03186543
Iteration 61, loss = 0.03099636
Iteration 62, loss = 0.03014665
Iteration 63, loss = 0.02931527
Iteration 64, loss = 0.02875154
Iteration 65, loss = 0.02807768
Iteration 66, loss = 0.02738948
Iteration 67, loss = 0.02656910
Iteration 68, loss = 0.02601164
Iteration 69, loss = 0.02536730
Iteration 70, loss = 0.02467229
Iteration 71, loss = 0.02410233
Iteration 72, loss = 0.02344406
Iteration 73, loss = 0.02303013
Iteration 74, loss = 0.02243492
Iteration 75, loss = 0.02175432
Iteration 76, loss = 0.02116142
Iteration 77, loss = 0.02062322
Iteration 78, loss = 0.02025885
Iteration 79, loss = 0.01969675
Iteration 80, loss = 0.01918476
Iteration 81, loss = 0.01881303
Iteration 82, loss = 0.01828824
Iteration 83, loss = 0.01777849
Iteration 84, loss = 0.01738348
Iteration 85, loss = 0.01697123
Iteration 86, loss = 0.01652696
Iteration 87, loss = 0.01627338
Iteration 88, loss = 0.01583942
Iteration 89, loss = 0.01543763
Iteration 90, loss = 0.01505374
Iteration 91, loss = 0.01467910
Iteration 92, loss = 0.01427280
Iteration 93, loss = 0.01400776
Iteration 94, loss = 0.01365191
Iteration 95, loss = 0.01332931
Iteration 96, loss = 0.01308797
Iteration 97, loss = 0.01275065
Iteration 98, loss = 0.01249054
Iteration 99, loss = 0.01213475
Iteration 100, loss = 0.01185932
Iteration 101, loss = 0.01160068
Iteration 102, loss = 0.01131504
Iteration 103, loss = 0.01109002
Iteration 104, loss = 0.01083868
Iteration 105, loss = 0.01062452
Iteration 106, loss = 0.01038354
Iteration 107, loss = 0.01005374
Iteration 108, loss = 0.00992295
Iteration 109, loss = 0.00966572
Iteration 110, loss = 0.00948734
Iteration 111, loss = 0.00928863
Iteration 112, loss = 0.00906133
Iteration 113, loss = 0.00888181
Iteration 114, loss = 0.00868832
Iteration 115, loss = 0.00852168
Iteration 116, loss = 0.00832469
Iteration 117, loss = 0.00814889
Iteration 118, loss = 0.00794142
Iteration 119, loss = 0.00784797
Iteration 120, loss = 0.00765765
Iteration 121, loss = 0.00745538
Iteration 122, loss = 0.00731533
Iteration 123, loss = 0.00714846
Iteration 124, loss = 0.00700860
Iteration 125, loss = 0.00685946
Iteration 126, loss = 0.00670996
Iteration 127, loss = 0.00658069
Iteration 128, loss = 0.00644871
Iteration 129, loss = 0.00625903
Iteration 130, loss = 0.00619746
Iteration 131, loss = 0.00605187
Iteration 132, loss = 0.00591923
Iteration 133, loss = 0.00580369
Iteration 134, loss = 0.00569718
Iteration 135, loss = 0.00557197
Iteration 136, loss = 0.00547691
Iteration 137, loss = 0.00533659
Iteration 138, loss = 0.00527529
Iteration 139, loss = 0.00517544
Iteration 140, loss = 0.00505999
Iteration 141, loss = 0.00496237
Iteration 142, loss = 0.00487228
Iteration 143, loss = 0.00479472
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  50
Error rate: 25.04% ( 626/2500)
 - Class 1:  20, Class 2:  21, Class 3: 497, Class 4:  24, Class 5:  64
Iteration 1, loss = 1.30677226
Iteration 2, loss = 0.82753594
Iteration 3, loss = 0.56005183
Iteration 4, loss = 0.40800791
Iteration 5, loss = 0.31820676
Iteration 6, loss = 0.26205432
Iteration 7, loss = 0.22424163
Iteration 8, loss = 0.19781765
Iteration 9, loss = 0.17789040
Iteration 10, loss = 0.16243152
Iteration 11, loss = 0.15023379
Iteration 12, loss = 0.13988203
Iteration 13, loss = 0.13111559
Iteration 14, loss = 0.12374551
Iteration 15, loss = 0.11770031
Iteration 16, loss = 0.11185446
Iteration 17, loss = 0.10675351
Iteration 18, loss = 0.10222440
Iteration 19, loss = 0.09810506
Iteration 20, loss = 0.09419393
Iteration 21, loss = 0.09057891
Iteration 22, loss = 0.08695312
Iteration 23, loss = 0.08398292
Iteration 24, loss = 0.08124756
Iteration 25, loss = 0.07868414
Iteration 26, loss = 0.07608176
Iteration 27, loss = 0.07332497
Iteration 28, loss = 0.07078522
Iteration 29, loss = 0.06850371
Iteration 30, loss = 0.06622268
Iteration 31, loss = 0.06420592
Iteration 32, loss = 0.06229690
Iteration 33, loss = 0.06031358
Iteration 34, loss = 0.05855219
Iteration 35, loss = 0.05678769
Iteration 36, loss = 0.05560448
Iteration 37, loss = 0.05358761
Iteration 38, loss = 0.05196903
Iteration 39, loss = 0.05047035
Iteration 40, loss = 0.04904136
Iteration 41, loss = 0.04773475
Iteration 42, loss = 0.04652319
Iteration 43, loss = 0.04521968
Iteration 44, loss = 0.04366900
Iteration 45, loss = 0.04280836
Iteration 46, loss = 0.04158932
Iteration 47, loss = 0.04046317
Iteration 48, loss = 0.03921026
Iteration 49, loss = 0.03844070
Iteration 50, loss = 0.03729332
Iteration 51, loss = 0.03623710
Iteration 52, loss = 0.03521493
Iteration 53, loss = 0.03414264
Iteration 54, loss = 0.03340705
Iteration 55, loss = 0.03259042
Iteration 56, loss = 0.03193533
Iteration 57, loss = 0.03069420
Iteration 58, loss = 0.03011617
Iteration 59, loss = 0.02945169
Iteration 60, loss = 0.02834897
Iteration 61, loss = 0.02781301
Iteration 62, loss = 0.02702557
Iteration 63, loss = 0.02625621
Iteration 64, loss = 0.02560086
Iteration 65, loss = 0.02491299
Iteration 66, loss = 0.02439035
Iteration 67, loss = 0.02371507
Iteration 68, loss = 0.02307833
Iteration 69, loss = 0.02264740
Iteration 70, loss = 0.02197619
Iteration 71, loss = 0.02137391
Iteration 72, loss = 0.02080468
Iteration 73, loss = 0.02024467
Iteration 74, loss = 0.01997313
Iteration 75, loss = 0.01941426
Iteration 76, loss = 0.01873406
Iteration 77, loss = 0.01828512
Iteration 78, loss = 0.01791355
Iteration 79, loss = 0.01740655
Iteration 80, loss = 0.01717369
Iteration 81, loss = 0.01669030
Iteration 82, loss = 0.01616501
Iteration 83, loss = 0.01582447
Iteration 84, loss = 0.01551079
Iteration 85, loss = 0.01495785
Iteration 86, loss = 0.01459010
Iteration 87, loss = 0.01414311
Iteration 88, loss = 0.01381571
Iteration 89, loss = 0.01348439
Iteration 90, loss = 0.01315703
Iteration 91, loss = 0.01288777
Iteration 92, loss = 0.01257405
Iteration 93, loss = 0.01219238
Iteration 94, loss = 0.01192791
Iteration 95, loss = 0.01161325
Iteration 96, loss = 0.01126858
Iteration 97, loss = 0.01103021
Iteration 98, loss = 0.01077704
Iteration 99, loss = 0.01050350
Iteration 100, loss = 0.01028740
Iteration 101, loss = 0.01006854
Iteration 102, loss = 0.00974311
Iteration 103, loss = 0.00954811
Iteration 104, loss = 0.00933673
Iteration 105, loss = 0.00914597
Iteration 106, loss = 0.00890572
Iteration 107, loss = 0.00868955
Iteration 108, loss = 0.00848165
Iteration 109, loss = 0.00832510
Iteration 110, loss = 0.00816520
Iteration 111, loss = 0.00791061
Iteration 112, loss = 0.00776124
Iteration 113, loss = 0.00755255
Iteration 114, loss = 0.00744272
Iteration 115, loss = 0.00723496
Iteration 116, loss = 0.00707924
Iteration 117, loss = 0.00692853
Iteration 118, loss = 0.00680589
Iteration 119, loss = 0.00663943
Iteration 120, loss = 0.00646973
Iteration 121, loss = 0.00637565
Iteration 122, loss = 0.00623437
Iteration 123, loss = 0.00608814
Iteration 124, loss = 0.00595671
Iteration 125, loss = 0.00584884
Iteration 126, loss = 0.00571090
Iteration 127, loss = 0.00559311
Iteration 128, loss = 0.00548484
Iteration 129, loss = 0.00537996
Iteration 130, loss = 0.00528514
Iteration 131, loss = 0.00520794
Iteration 132, loss = 0.00506939
Iteration 133, loss = 0.00495741
Iteration 134, loss = 0.00486783
Iteration 135, loss = 0.00478277
Iteration 136, loss = 0.00470507
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  55
Error rate: 14.12% ( 353/2500)
 - Class 1:  36, Class 2:  17, Class 3: 223, Class 4:  46, Class 5:  31
Iteration 1, loss = 1.30955606
Iteration 2, loss = 0.84496597
Iteration 3, loss = 0.56652078
Iteration 4, loss = 0.40710898
Iteration 5, loss = 0.31547140
Iteration 6, loss = 0.25873472
Iteration 7, loss = 0.22151461
Iteration 8, loss = 0.19556891
Iteration 9, loss = 0.17594465
Iteration 10, loss = 0.16061820
Iteration 11, loss = 0.14845211
Iteration 12, loss = 0.13857027
Iteration 13, loss = 0.13055582
Iteration 14, loss = 0.12300447
Iteration 15, loss = 0.11667201
Iteration 16, loss = 0.11146114
Iteration 17, loss = 0.10612292
Iteration 18, loss = 0.10187986
Iteration 19, loss = 0.09730703
Iteration 20, loss = 0.09362644
Iteration 21, loss = 0.08994355
Iteration 22, loss = 0.08696614
Iteration 23, loss = 0.08348798
Iteration 24, loss = 0.08047319
Iteration 25, loss = 0.07788298
Iteration 26, loss = 0.07532765
Iteration 27, loss = 0.07245803
Iteration 28, loss = 0.07012809
Iteration 29, loss = 0.06816988
Iteration 30, loss = 0.06570366
Iteration 31, loss = 0.06396688
Iteration 32, loss = 0.06206906
Iteration 33, loss = 0.06004170
Iteration 34, loss = 0.05793094
Iteration 35, loss = 0.05620076
Iteration 36, loss = 0.05456587
Iteration 37, loss = 0.05312241
Iteration 38, loss = 0.05148935
Iteration 39, loss = 0.04993701
Iteration 40, loss = 0.04848228
Iteration 41, loss = 0.04721237
Iteration 42, loss = 0.04585672
Iteration 43, loss = 0.04448151
Iteration 44, loss = 0.04334320
Iteration 45, loss = 0.04219295
Iteration 46, loss = 0.04109351
Iteration 47, loss = 0.04007161
Iteration 48, loss = 0.03868008
Iteration 49, loss = 0.03783942
Iteration 50, loss = 0.03671452
Iteration 51, loss = 0.03566907
Iteration 52, loss = 0.03470962
Iteration 53, loss = 0.03373832
Iteration 54, loss = 0.03296382
Iteration 55, loss = 0.03211992
Iteration 56, loss = 0.03140968
Iteration 57, loss = 0.03050224
Iteration 58, loss = 0.02974187
Iteration 59, loss = 0.02928150
Iteration 60, loss = 0.02833002
Iteration 61, loss = 0.02724756
Iteration 62, loss = 0.02659264
Iteration 63, loss = 0.02590227
Iteration 64, loss = 0.02547675
Iteration 65, loss = 0.02452774
Iteration 66, loss = 0.02411610
Iteration 67, loss = 0.02344865
Iteration 68, loss = 0.02278839
Iteration 69, loss = 0.02205168
Iteration 70, loss = 0.02166680
Iteration 71, loss = 0.02114503
Iteration 72, loss = 0.02051723
Iteration 73, loss = 0.01997651
Iteration 74, loss = 0.01938123
Iteration 75, loss = 0.01891158
Iteration 76, loss = 0.01847422
Iteration 77, loss = 0.01795001
Iteration 78, loss = 0.01748265
Iteration 79, loss = 0.01703980
Iteration 80, loss = 0.01670930
Iteration 81, loss = 0.01621716
Iteration 82, loss = 0.01581355
Iteration 83, loss = 0.01541809
Iteration 84, loss = 0.01507763
Iteration 85, loss = 0.01464540
Iteration 86, loss = 0.01429708
Iteration 87, loss = 0.01417347
Iteration 88, loss = 0.01362260
Iteration 89, loss = 0.01324934
Iteration 90, loss = 0.01307418
Iteration 91, loss = 0.01264142
Iteration 92, loss = 0.01224749
Iteration 93, loss = 0.01198519
Iteration 94, loss = 0.01174880
Iteration 95, loss = 0.01138991
Iteration 96, loss = 0.01110918
Iteration 97, loss = 0.01086839
Iteration 98, loss = 0.01069072
Iteration 99, loss = 0.01032297
Iteration 100, loss = 0.01010383
Iteration 101, loss = 0.00987567
Iteration 102, loss = 0.00961356
Iteration 103, loss = 0.00950758
Iteration 104, loss = 0.00916400
Iteration 105, loss = 0.00898756
Iteration 106, loss = 0.00889263
Iteration 107, loss = 0.00857516
Iteration 108, loss = 0.00836369
Iteration 109, loss = 0.00820743
Iteration 110, loss = 0.00798173
Iteration 111, loss = 0.00786451
Iteration 112, loss = 0.00766308
Iteration 113, loss = 0.00751196
Iteration 114, loss = 0.00734001
Iteration 115, loss = 0.00726207
Iteration 116, loss = 0.00700742
Iteration 117, loss = 0.00684862
Iteration 118, loss = 0.00669217
Iteration 119, loss = 0.00655754
Iteration 120, loss = 0.00644960
Iteration 121, loss = 0.00629774
Iteration 122, loss = 0.00613854
Iteration 123, loss = 0.00600368
Iteration 124, loss = 0.00586351
Iteration 125, loss = 0.00574998
Iteration 126, loss = 0.00565006
Iteration 127, loss = 0.00552371
Iteration 128, loss = 0.00541186
Iteration 129, loss = 0.00529418
Iteration 130, loss = 0.00516235
Iteration 131, loss = 0.00508415
Iteration 132, loss = 0.00496918
Iteration 133, loss = 0.00486258
Iteration 134, loss = 0.00476293
Iteration 135, loss = 0.00470468
Iteration 136, loss = 0.00457917
Iteration 137, loss = 0.00448528
Iteration 138, loss = 0.00441051
Iteration 139, loss = 0.00431898
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  60
Error rate: 24.92% ( 623/2500)
 - Class 1:  39, Class 2:  23, Class 3: 500, Class 4:  34, Class 5:  27
Iteration 1, loss = 1.31500391
Iteration 2, loss = 0.83497391
Iteration 3, loss = 0.56251949
Iteration 4, loss = 0.40511788
Iteration 5, loss = 0.31412453
Iteration 6, loss = 0.25736326
Iteration 7, loss = 0.22000118
Iteration 8, loss = 0.19316641
Iteration 9, loss = 0.17342595
Iteration 10, loss = 0.15806378
Iteration 11, loss = 0.14606957
Iteration 12, loss = 0.13599244
Iteration 13, loss = 0.12770641
Iteration 14, loss = 0.12144983
Iteration 15, loss = 0.11421402
Iteration 16, loss = 0.10885236
Iteration 17, loss = 0.10413903
Iteration 18, loss = 0.09911444
Iteration 19, loss = 0.09557964
Iteration 20, loss = 0.09145436
Iteration 21, loss = 0.08775249
Iteration 22, loss = 0.08532507
Iteration 23, loss = 0.08132466
Iteration 24, loss = 0.07826523
Iteration 25, loss = 0.07548390
Iteration 26, loss = 0.07308284
Iteration 27, loss = 0.07067287
Iteration 28, loss = 0.06843107
Iteration 29, loss = 0.06633928
Iteration 30, loss = 0.06393731
Iteration 31, loss = 0.06192592
Iteration 32, loss = 0.05981295
Iteration 33, loss = 0.05806177
Iteration 34, loss = 0.05635111
Iteration 35, loss = 0.05470101
Iteration 36, loss = 0.05342446
Iteration 37, loss = 0.05143073
Iteration 38, loss = 0.04994660
Iteration 39, loss = 0.04848077
Iteration 40, loss = 0.04729993
Iteration 41, loss = 0.04567974
Iteration 42, loss = 0.04440660
Iteration 43, loss = 0.04325153
Iteration 44, loss = 0.04228517
Iteration 45, loss = 0.04093192
Iteration 46, loss = 0.03991562
Iteration 47, loss = 0.03856733
Iteration 48, loss = 0.03735088
Iteration 49, loss = 0.03656881
Iteration 50, loss = 0.03562526
Iteration 51, loss = 0.03484529
Iteration 52, loss = 0.03355441
Iteration 53, loss = 0.03269523
Iteration 54, loss = 0.03188030
Iteration 55, loss = 0.03103492
Iteration 56, loss = 0.03028388
Iteration 57, loss = 0.02942033
Iteration 58, loss = 0.02876172
Iteration 59, loss = 0.02804248
Iteration 60, loss = 0.02712148
Iteration 61, loss = 0.02638527
Iteration 62, loss = 0.02585214
Iteration 63, loss = 0.02512753
Iteration 64, loss = 0.02434855
Iteration 65, loss = 0.02380712
Iteration 66, loss = 0.02313576
Iteration 67, loss = 0.02254080
Iteration 68, loss = 0.02193547
Iteration 69, loss = 0.02146574
Iteration 70, loss = 0.02095713
Iteration 71, loss = 0.02048446
Iteration 72, loss = 0.01975817
Iteration 73, loss = 0.01920950
Iteration 74, loss = 0.01868504
Iteration 75, loss = 0.01825393
Iteration 76, loss = 0.01778379
Iteration 77, loss = 0.01737049
Iteration 78, loss = 0.01685823
Iteration 79, loss = 0.01638805
Iteration 80, loss = 0.01604379
Iteration 81, loss = 0.01553073
Iteration 82, loss = 0.01525098
Iteration 83, loss = 0.01484965
Iteration 84, loss = 0.01458905
Iteration 85, loss = 0.01417261
Iteration 86, loss = 0.01360705
Iteration 87, loss = 0.01333603
Iteration 88, loss = 0.01291687
Iteration 89, loss = 0.01255387
Iteration 90, loss = 0.01220478
Iteration 91, loss = 0.01197119
Iteration 92, loss = 0.01164963
Iteration 93, loss = 0.01134106
Iteration 94, loss = 0.01105994
Iteration 95, loss = 0.01083510
Iteration 96, loss = 0.01049373
Iteration 97, loss = 0.01018083
Iteration 98, loss = 0.00998401
Iteration 99, loss = 0.00973679
Iteration 100, loss = 0.00943203
Iteration 101, loss = 0.00928252
Iteration 102, loss = 0.00902757
Iteration 103, loss = 0.00884504
Iteration 104, loss = 0.00860109
Iteration 105, loss = 0.00835285
Iteration 106, loss = 0.00819754
Iteration 107, loss = 0.00800935
Iteration 108, loss = 0.00779384
Iteration 109, loss = 0.00755147
Iteration 110, loss = 0.00745725
Iteration 111, loss = 0.00723566
Iteration 112, loss = 0.00708313
Iteration 113, loss = 0.00689035
Iteration 114, loss = 0.00674675
Iteration 115, loss = 0.00659731
Iteration 116, loss = 0.00643428
Iteration 117, loss = 0.00627903
Iteration 118, loss = 0.00615626
Iteration 119, loss = 0.00600007
Iteration 120, loss = 0.00589137
Iteration 121, loss = 0.00577048
Iteration 122, loss = 0.00562838
Iteration 123, loss = 0.00549279
Iteration 124, loss = 0.00538301
Iteration 125, loss = 0.00528334
Iteration 126, loss = 0.00516731
Iteration 127, loss = 0.00504829
Iteration 128, loss = 0.00494390
Iteration 129, loss = 0.00486904
Iteration 130, loss = 0.00474883
Iteration 131, loss = 0.00465923
Iteration 132, loss = 0.00456176
Iteration 133, loss = 0.00447356
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  65
Error rate: 23.80% ( 595/2500)
 - Class 1:  32, Class 2:  18, Class 3: 499, Class 4:  24, Class 5:  22
Iteration 1, loss = 1.34724677
Iteration 2, loss = 0.84164184
Iteration 3, loss = 0.56292100
Iteration 4, loss = 0.40527150
Iteration 5, loss = 0.31322870
Iteration 6, loss = 0.25658250
Iteration 7, loss = 0.21946489
Iteration 8, loss = 0.19280007
Iteration 9, loss = 0.17355691
Iteration 10, loss = 0.15893247
Iteration 11, loss = 0.14587924
Iteration 12, loss = 0.13600279
Iteration 13, loss = 0.12753313
Iteration 14, loss = 0.12022614
Iteration 15, loss = 0.11379101
Iteration 16, loss = 0.10809041
Iteration 17, loss = 0.10330981
Iteration 18, loss = 0.09875910
Iteration 19, loss = 0.09460879
Iteration 20, loss = 0.09060418
Iteration 21, loss = 0.08717221
Iteration 22, loss = 0.08415429
Iteration 23, loss = 0.08137459
Iteration 24, loss = 0.07815449
Iteration 25, loss = 0.07529696
Iteration 26, loss = 0.07256725
Iteration 27, loss = 0.07059775
Iteration 28, loss = 0.06743376
Iteration 29, loss = 0.06564312
Iteration 30, loss = 0.06338939
Iteration 31, loss = 0.06150757
Iteration 32, loss = 0.05925090
Iteration 33, loss = 0.05766600
Iteration 34, loss = 0.05558631
Iteration 35, loss = 0.05421807
Iteration 36, loss = 0.05237298
Iteration 37, loss = 0.05085159
Iteration 38, loss = 0.04978772
Iteration 39, loss = 0.04792699
Iteration 40, loss = 0.04644122
Iteration 41, loss = 0.04526292
Iteration 42, loss = 0.04386865
Iteration 43, loss = 0.04247062
Iteration 44, loss = 0.04122275
Iteration 45, loss = 0.04012676
Iteration 46, loss = 0.03909794
Iteration 47, loss = 0.03792723
Iteration 48, loss = 0.03676016
Iteration 49, loss = 0.03593166
Iteration 50, loss = 0.03486664
Iteration 51, loss = 0.03373396
Iteration 52, loss = 0.03282168
Iteration 53, loss = 0.03204117
Iteration 54, loss = 0.03106846
Iteration 55, loss = 0.03019557
Iteration 56, loss = 0.02954102
Iteration 57, loss = 0.02853250
Iteration 58, loss = 0.02767479
Iteration 59, loss = 0.02698524
Iteration 60, loss = 0.02627828
Iteration 61, loss = 0.02561408
Iteration 62, loss = 0.02502048
Iteration 63, loss = 0.02415795
Iteration 64, loss = 0.02359233
Iteration 65, loss = 0.02277819
Iteration 66, loss = 0.02227494
Iteration 67, loss = 0.02180934
Iteration 68, loss = 0.02100273
Iteration 69, loss = 0.02046870
Iteration 70, loss = 0.01987029
Iteration 71, loss = 0.01931940
Iteration 72, loss = 0.01872261
Iteration 73, loss = 0.01825693
Iteration 74, loss = 0.01777327
Iteration 75, loss = 0.01736486
Iteration 76, loss = 0.01698982
Iteration 77, loss = 0.01638876
Iteration 78, loss = 0.01624699
Iteration 79, loss = 0.01559308
Iteration 80, loss = 0.01510920
Iteration 81, loss = 0.01474177
Iteration 82, loss = 0.01433621
Iteration 83, loss = 0.01396037
Iteration 84, loss = 0.01361655
Iteration 85, loss = 0.01322431
Iteration 86, loss = 0.01282088
Iteration 87, loss = 0.01251653
Iteration 88, loss = 0.01216841
Iteration 89, loss = 0.01188403
Iteration 90, loss = 0.01160897
Iteration 91, loss = 0.01123717
Iteration 92, loss = 0.01121439
Iteration 93, loss = 0.01080520
Iteration 94, loss = 0.01042258
Iteration 95, loss = 0.01017915
Iteration 96, loss = 0.00992016
Iteration 97, loss = 0.00969897
Iteration 98, loss = 0.00942440
Iteration 99, loss = 0.00919238
Iteration 100, loss = 0.00899806
Iteration 101, loss = 0.00886023
Iteration 102, loss = 0.00856936
Iteration 103, loss = 0.00840868
Iteration 104, loss = 0.00819969
Iteration 105, loss = 0.00806361
Iteration 106, loss = 0.00785587
Iteration 107, loss = 0.00769458
Iteration 108, loss = 0.00749349
Iteration 109, loss = 0.00727740
Iteration 110, loss = 0.00711807
Iteration 111, loss = 0.00693620
Iteration 112, loss = 0.00678436
Iteration 113, loss = 0.00662664
Iteration 114, loss = 0.00649578
Iteration 115, loss = 0.00635696
Iteration 116, loss = 0.00615929
Iteration 117, loss = 0.00605169
Iteration 118, loss = 0.00596008
Iteration 119, loss = 0.00579445
Iteration 120, loss = 0.00568887
Iteration 121, loss = 0.00553943
Iteration 122, loss = 0.00542125
Iteration 123, loss = 0.00532527
Iteration 124, loss = 0.00521609
Iteration 125, loss = 0.00511119
Iteration 126, loss = 0.00498691
Iteration 127, loss = 0.00487317
Iteration 128, loss = 0.00477400
Iteration 129, loss = 0.00468856
Iteration 130, loss = 0.00459175
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  70
Error rate: 11.24% ( 281/2500)
 - Class 1:  50, Class 2:  20, Class 3: 115, Class 4:  80, Class 5:  16
Iteration 1, loss = 1.27891212
Iteration 2, loss = 0.76841186
Iteration 3, loss = 0.49040924
Iteration 4, loss = 0.34818399
Iteration 5, loss = 0.27187932
Iteration 6, loss = 0.22535742
Iteration 7, loss = 0.19455520
Iteration 8, loss = 0.17272065
Iteration 9, loss = 0.15661807
Iteration 10, loss = 0.14351573
Iteration 11, loss = 0.13344369
Iteration 12, loss = 0.12471269
Iteration 13, loss = 0.11745969
Iteration 14, loss = 0.11108589
Iteration 15, loss = 0.10573289
Iteration 16, loss = 0.10111296
Iteration 17, loss = 0.09641438
Iteration 18, loss = 0.09167039
Iteration 19, loss = 0.08797712
Iteration 20, loss = 0.08416169
Iteration 21, loss = 0.08043591
Iteration 22, loss = 0.07763803
Iteration 23, loss = 0.07472623
Iteration 24, loss = 0.07217138
Iteration 25, loss = 0.06960684
Iteration 26, loss = 0.06664280
Iteration 27, loss = 0.06532023
Iteration 28, loss = 0.06279037
Iteration 29, loss = 0.06037603
Iteration 30, loss = 0.05830831
Iteration 31, loss = 0.05634126
Iteration 32, loss = 0.05428098
Iteration 33, loss = 0.05261297
Iteration 34, loss = 0.05085812
Iteration 35, loss = 0.04934747
Iteration 36, loss = 0.04771218
Iteration 37, loss = 0.04634698
Iteration 38, loss = 0.04492303
Iteration 39, loss = 0.04349064
Iteration 40, loss = 0.04220588
Iteration 41, loss = 0.04111223
Iteration 42, loss = 0.03950287
Iteration 43, loss = 0.03853077
Iteration 44, loss = 0.03746375
Iteration 45, loss = 0.03660214
Iteration 46, loss = 0.03551601
Iteration 47, loss = 0.03410544
Iteration 48, loss = 0.03320938
Iteration 49, loss = 0.03210191
Iteration 50, loss = 0.03103997
Iteration 51, loss = 0.03025910
Iteration 52, loss = 0.02949690
Iteration 53, loss = 0.02847651
Iteration 54, loss = 0.02803444
Iteration 55, loss = 0.02694516
Iteration 56, loss = 0.02623426
Iteration 57, loss = 0.02556003
Iteration 58, loss = 0.02470872
Iteration 59, loss = 0.02392493
Iteration 60, loss = 0.02359043
Iteration 61, loss = 0.02260088
Iteration 62, loss = 0.02196918
Iteration 63, loss = 0.02160917
Iteration 64, loss = 0.02074170
Iteration 65, loss = 0.02017983
Iteration 66, loss = 0.01969033
Iteration 67, loss = 0.01905788
Iteration 68, loss = 0.01871391
Iteration 69, loss = 0.01801458
Iteration 70, loss = 0.01743010
Iteration 71, loss = 0.01691223
Iteration 72, loss = 0.01638135
Iteration 73, loss = 0.01593451
Iteration 74, loss = 0.01550914
Iteration 75, loss = 0.01508496
Iteration 76, loss = 0.01479574
Iteration 77, loss = 0.01441900
Iteration 78, loss = 0.01395048
Iteration 79, loss = 0.01359727
Iteration 80, loss = 0.01306829
Iteration 81, loss = 0.01274525
Iteration 82, loss = 0.01235296
Iteration 83, loss = 0.01203469
Iteration 84, loss = 0.01167175
Iteration 85, loss = 0.01135869
Iteration 86, loss = 0.01113673
Iteration 87, loss = 0.01075082
Iteration 88, loss = 0.01052667
Iteration 89, loss = 0.01016789
Iteration 90, loss = 0.00992687
Iteration 91, loss = 0.00971213
Iteration 92, loss = 0.00935373
Iteration 93, loss = 0.00917342
Iteration 94, loss = 0.00892811
Iteration 95, loss = 0.00867287
Iteration 96, loss = 0.00847611
Iteration 97, loss = 0.00828829
Iteration 98, loss = 0.00805161
Iteration 99, loss = 0.00785617
Iteration 100, loss = 0.00763211
Iteration 101, loss = 0.00748937
Iteration 102, loss = 0.00729539
Iteration 103, loss = 0.00712403
Iteration 104, loss = 0.00693506
Iteration 105, loss = 0.00676361
Iteration 106, loss = 0.00660415
Iteration 107, loss = 0.00649082
Iteration 108, loss = 0.00635221
Iteration 109, loss = 0.00618182
Iteration 110, loss = 0.00604720
Iteration 111, loss = 0.00590919
Iteration 112, loss = 0.00578098
Iteration 113, loss = 0.00565325
Iteration 114, loss = 0.00550732
Iteration 115, loss = 0.00540623
Iteration 116, loss = 0.00525214
Iteration 117, loss = 0.00512527
Iteration 118, loss = 0.00499237
Iteration 119, loss = 0.00489035
Iteration 120, loss = 0.00479846
Iteration 121, loss = 0.00475000
Iteration 122, loss = 0.00465367
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  75
Error rate: 10.56% ( 264/2500)
 - Class 1:  22, Class 2:  23, Class 3: 112, Class 4:  90, Class 5:  17
Iteration 1, loss = 1.27670342
Iteration 2, loss = 0.75835642
Iteration 3, loss = 0.49409727
Iteration 4, loss = 0.35593641
Iteration 5, loss = 0.27852139
Iteration 6, loss = 0.23178079
Iteration 7, loss = 0.20044647
Iteration 8, loss = 0.17814149
Iteration 9, loss = 0.16056877
Iteration 10, loss = 0.14747567
Iteration 11, loss = 0.13682744
Iteration 12, loss = 0.12794522
Iteration 13, loss = 0.12070385
Iteration 14, loss = 0.11407194
Iteration 15, loss = 0.10867613
Iteration 16, loss = 0.10332370
Iteration 17, loss = 0.09916708
Iteration 18, loss = 0.09446352
Iteration 19, loss = 0.09040136
Iteration 20, loss = 0.08699833
Iteration 21, loss = 0.08376573
Iteration 22, loss = 0.08070640
Iteration 23, loss = 0.07763896
Iteration 24, loss = 0.07465289
Iteration 25, loss = 0.07207322
Iteration 26, loss = 0.06958264
Iteration 27, loss = 0.06745660
Iteration 28, loss = 0.06612680
Iteration 29, loss = 0.06266242
Iteration 30, loss = 0.06114438
Iteration 31, loss = 0.05917304
Iteration 32, loss = 0.05688249
Iteration 33, loss = 0.05521931
Iteration 34, loss = 0.05322819
Iteration 35, loss = 0.05170776
Iteration 36, loss = 0.05001939
Iteration 37, loss = 0.04874667
Iteration 38, loss = 0.04708838
Iteration 39, loss = 0.04581881
Iteration 40, loss = 0.04429104
Iteration 41, loss = 0.04316138
Iteration 42, loss = 0.04231822
Iteration 43, loss = 0.04121935
Iteration 44, loss = 0.03995068
Iteration 45, loss = 0.03846497
Iteration 46, loss = 0.03758622
Iteration 47, loss = 0.03606360
Iteration 48, loss = 0.03524028
Iteration 49, loss = 0.03422868
Iteration 50, loss = 0.03338862
Iteration 51, loss = 0.03213471
Iteration 52, loss = 0.03134477
Iteration 53, loss = 0.03044935
Iteration 54, loss = 0.02974201
Iteration 55, loss = 0.02871947
Iteration 56, loss = 0.02810513
Iteration 57, loss = 0.02717589
Iteration 58, loss = 0.02634938
Iteration 59, loss = 0.02571542
Iteration 60, loss = 0.02505208
Iteration 61, loss = 0.02438526
Iteration 62, loss = 0.02361438
Iteration 63, loss = 0.02302111
Iteration 64, loss = 0.02229391
Iteration 65, loss = 0.02160169
Iteration 66, loss = 0.02095869
Iteration 67, loss = 0.02046634
Iteration 68, loss = 0.02008690
Iteration 69, loss = 0.01922816
Iteration 70, loss = 0.01885967
Iteration 71, loss = 0.01818352
Iteration 72, loss = 0.01762166
Iteration 73, loss = 0.01715160
Iteration 74, loss = 0.01685925
Iteration 75, loss = 0.01608123
Iteration 76, loss = 0.01576207
Iteration 77, loss = 0.01540274
Iteration 78, loss = 0.01495540
Iteration 79, loss = 0.01433062
Iteration 80, loss = 0.01399611
Iteration 81, loss = 0.01361190
Iteration 82, loss = 0.01336254
Iteration 83, loss = 0.01298154
Iteration 84, loss = 0.01267006
Iteration 85, loss = 0.01220382
Iteration 86, loss = 0.01178544
Iteration 87, loss = 0.01158075
Iteration 88, loss = 0.01116476
Iteration 89, loss = 0.01089775
Iteration 90, loss = 0.01050534
Iteration 91, loss = 0.01034713
Iteration 92, loss = 0.01000229
Iteration 93, loss = 0.00972748
Iteration 94, loss = 0.00945109
Iteration 95, loss = 0.00928662
Iteration 96, loss = 0.00901136
Iteration 97, loss = 0.00879541
Iteration 98, loss = 0.00849060
Iteration 99, loss = 0.00828503
Iteration 100, loss = 0.00809333
Iteration 101, loss = 0.00784442
Iteration 102, loss = 0.00769957
Iteration 103, loss = 0.00749442
Iteration 104, loss = 0.00730002
Iteration 105, loss = 0.00707741
Iteration 106, loss = 0.00702780
Iteration 107, loss = 0.00673835
Iteration 108, loss = 0.00665317
Iteration 109, loss = 0.00649795
Iteration 110, loss = 0.00634531
Iteration 111, loss = 0.00615490
Iteration 112, loss = 0.00601202
Iteration 113, loss = 0.00586998
Iteration 114, loss = 0.00573652
Iteration 115, loss = 0.00556633
Iteration 116, loss = 0.00545810
Iteration 117, loss = 0.00532073
Iteration 118, loss = 0.00522343
Iteration 119, loss = 0.00511182
Iteration 120, loss = 0.00500539
Iteration 121, loss = 0.00489601
Iteration 122, loss = 0.00478985
Iteration 123, loss = 0.00467609
Iteration 124, loss = 0.00457259
Iteration 125, loss = 0.00454469
Iteration 126, loss = 0.00437266
Iteration 127, loss = 0.00430471
Iteration 128, loss = 0.00423138
Iteration 129, loss = 0.00411336
Iteration 130, loss = 0.00405384
Iteration 131, loss = 0.00397220
Iteration 132, loss = 0.00388025
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  80
Error rate: 24.84% ( 621/2500)
 - Class 1:  41, Class 2:  15, Class 3: 466, Class 4:  39, Class 5:  60
Iteration 1, loss = 1.25131811
Iteration 2, loss = 0.72853448
Iteration 3, loss = 0.46565933
Iteration 4, loss = 0.33321236
Iteration 5, loss = 0.26107045
Iteration 6, loss = 0.21754241
Iteration 7, loss = 0.18843100
Iteration 8, loss = 0.16815921
Iteration 9, loss = 0.15283542
Iteration 10, loss = 0.13949753
Iteration 11, loss = 0.12997907
Iteration 12, loss = 0.12151669
Iteration 13, loss = 0.11479222
Iteration 14, loss = 0.10856997
Iteration 15, loss = 0.10342769
Iteration 16, loss = 0.09807562
Iteration 17, loss = 0.09362273
Iteration 18, loss = 0.08989773
Iteration 19, loss = 0.08576296
Iteration 20, loss = 0.08265035
Iteration 21, loss = 0.07925629
Iteration 22, loss = 0.07616836
Iteration 23, loss = 0.07389480
Iteration 24, loss = 0.07125797
Iteration 25, loss = 0.06847239
Iteration 26, loss = 0.06600828
Iteration 27, loss = 0.06358380
Iteration 28, loss = 0.06158830
Iteration 29, loss = 0.05961866
Iteration 30, loss = 0.05737755
Iteration 31, loss = 0.05538659
Iteration 32, loss = 0.05405175
Iteration 33, loss = 0.05232194
Iteration 34, loss = 0.05099483
Iteration 35, loss = 0.04930253
Iteration 36, loss = 0.04739700
Iteration 37, loss = 0.04595128
Iteration 38, loss = 0.04458090
Iteration 39, loss = 0.04328225
Iteration 40, loss = 0.04185685
Iteration 41, loss = 0.04061670
Iteration 42, loss = 0.03964967
Iteration 43, loss = 0.03817856
Iteration 44, loss = 0.03751091
Iteration 45, loss = 0.03583517
Iteration 46, loss = 0.03493681
Iteration 47, loss = 0.03380575
Iteration 48, loss = 0.03321356
Iteration 49, loss = 0.03230433
Iteration 50, loss = 0.03130209
Iteration 51, loss = 0.03042673
Iteration 52, loss = 0.02983032
Iteration 53, loss = 0.02850091
Iteration 54, loss = 0.02773530
Iteration 55, loss = 0.02708992
Iteration 56, loss = 0.02624432
Iteration 57, loss = 0.02547032
Iteration 58, loss = 0.02461677
Iteration 59, loss = 0.02406868
Iteration 60, loss = 0.02331777
Iteration 61, loss = 0.02291973
Iteration 62, loss = 0.02196105
Iteration 63, loss = 0.02126507
Iteration 64, loss = 0.02086795
Iteration 65, loss = 0.02011065
Iteration 66, loss = 0.01948226
Iteration 67, loss = 0.01905415
Iteration 68, loss = 0.01838886
Iteration 69, loss = 0.01789320
Iteration 70, loss = 0.01746890
Iteration 71, loss = 0.01703362
Iteration 72, loss = 0.01630978
Iteration 73, loss = 0.01592246
Iteration 74, loss = 0.01548842
Iteration 75, loss = 0.01509722
Iteration 76, loss = 0.01457044
Iteration 77, loss = 0.01418021
Iteration 78, loss = 0.01369954
Iteration 79, loss = 0.01330446
Iteration 80, loss = 0.01291009
Iteration 81, loss = 0.01267178
Iteration 82, loss = 0.01222686
Iteration 83, loss = 0.01208205
Iteration 84, loss = 0.01159344
Iteration 85, loss = 0.01121954
Iteration 86, loss = 0.01088790
Iteration 87, loss = 0.01068246
Iteration 88, loss = 0.01028514
Iteration 89, loss = 0.01007740
Iteration 90, loss = 0.00973072
Iteration 91, loss = 0.00944905
Iteration 92, loss = 0.00930130
Iteration 93, loss = 0.00896286
Iteration 94, loss = 0.00881251
Iteration 95, loss = 0.00842816
Iteration 96, loss = 0.00819110
Iteration 97, loss = 0.00806007
Iteration 98, loss = 0.00784827
Iteration 99, loss = 0.00758855
Iteration 100, loss = 0.00736657
Iteration 101, loss = 0.00721293
Iteration 102, loss = 0.00698994
Iteration 103, loss = 0.00680452
Iteration 104, loss = 0.00666097
Iteration 105, loss = 0.00650207
Iteration 106, loss = 0.00634630
Iteration 107, loss = 0.00618082
Iteration 108, loss = 0.00604620
Iteration 109, loss = 0.00591501
Iteration 110, loss = 0.00573012
Iteration 111, loss = 0.00560181
Iteration 112, loss = 0.00547127
Iteration 113, loss = 0.00534867
Iteration 114, loss = 0.00520835
Iteration 115, loss = 0.00516054
Iteration 116, loss = 0.00497267
Iteration 117, loss = 0.00486640
Iteration 118, loss = 0.00473977
Iteration 119, loss = 0.00470829
Iteration 120, loss = 0.00454953
Iteration 121, loss = 0.00449037
Iteration 122, loss = 0.00438255
Iteration 123, loss = 0.00429058
Iteration 124, loss = 0.00419566
Iteration 125, loss = 0.00408899
Iteration 126, loss = 0.00403871
Iteration 127, loss = 0.00393954
Iteration 128, loss = 0.00386733
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  85
Error rate: 24.04% ( 601/2500)
 - Class 1:  22, Class 2:  24, Class 3: 500, Class 4:  35, Class 5:  20
Iteration 1, loss = 1.25550968
Iteration 2, loss = 0.71733541
Iteration 3, loss = 0.45284316
Iteration 4, loss = 0.32311851
Iteration 5, loss = 0.25389174
Iteration 6, loss = 0.21264253
Iteration 7, loss = 0.18521531
Iteration 8, loss = 0.16515852
Iteration 9, loss = 0.15007819
Iteration 10, loss = 0.13824023
Iteration 11, loss = 0.12899559
Iteration 12, loss = 0.12084244
Iteration 13, loss = 0.11438063
Iteration 14, loss = 0.10755969
Iteration 15, loss = 0.10242144
Iteration 16, loss = 0.09833842
Iteration 17, loss = 0.09319056
Iteration 18, loss = 0.08895651
Iteration 19, loss = 0.08569579
Iteration 20, loss = 0.08207317
Iteration 21, loss = 0.07887592
Iteration 22, loss = 0.07560501
Iteration 23, loss = 0.07248730
Iteration 24, loss = 0.06967982
Iteration 25, loss = 0.06722897
Iteration 26, loss = 0.06514342
Iteration 27, loss = 0.06254773
Iteration 28, loss = 0.06000944
Iteration 29, loss = 0.05837562
Iteration 30, loss = 0.05610552
Iteration 31, loss = 0.05423478
Iteration 32, loss = 0.05241929
Iteration 33, loss = 0.05064908
Iteration 34, loss = 0.04899498
Iteration 35, loss = 0.04769802
Iteration 36, loss = 0.04605713
Iteration 37, loss = 0.04500070
Iteration 38, loss = 0.04305552
Iteration 39, loss = 0.04183284
Iteration 40, loss = 0.04018653
Iteration 41, loss = 0.03901077
Iteration 42, loss = 0.03793874
Iteration 43, loss = 0.03665310
Iteration 44, loss = 0.03591829
Iteration 45, loss = 0.03489171
Iteration 46, loss = 0.03349132
Iteration 47, loss = 0.03270758
Iteration 48, loss = 0.03160325
Iteration 49, loss = 0.03053741
Iteration 50, loss = 0.02958885
Iteration 51, loss = 0.02889339
Iteration 52, loss = 0.02786662
Iteration 53, loss = 0.02722024
Iteration 54, loss = 0.02612558
Iteration 55, loss = 0.02547783
Iteration 56, loss = 0.02469417
Iteration 57, loss = 0.02382966
Iteration 58, loss = 0.02334218
Iteration 59, loss = 0.02271939
Iteration 60, loss = 0.02179254
Iteration 61, loss = 0.02125208
Iteration 62, loss = 0.02068068
Iteration 63, loss = 0.01997718
Iteration 64, loss = 0.01941745
Iteration 65, loss = 0.01877831
Iteration 66, loss = 0.01816365
Iteration 67, loss = 0.01766663
Iteration 68, loss = 0.01718120
Iteration 69, loss = 0.01659844
Iteration 70, loss = 0.01600229
Iteration 71, loss = 0.01553058
Iteration 72, loss = 0.01507613
Iteration 73, loss = 0.01463986
Iteration 74, loss = 0.01420455
Iteration 75, loss = 0.01380748
Iteration 76, loss = 0.01331783
Iteration 77, loss = 0.01299760
Iteration 78, loss = 0.01254870
Iteration 79, loss = 0.01220683
Iteration 80, loss = 0.01184728
Iteration 81, loss = 0.01152441
Iteration 82, loss = 0.01117381
Iteration 83, loss = 0.01089019
Iteration 84, loss = 0.01050506
Iteration 85, loss = 0.01020546
Iteration 86, loss = 0.00989115
Iteration 87, loss = 0.00970465
Iteration 88, loss = 0.00954597
Iteration 89, loss = 0.00912764
Iteration 90, loss = 0.00892462
Iteration 91, loss = 0.00872338
Iteration 92, loss = 0.00839084
Iteration 93, loss = 0.00817482
Iteration 94, loss = 0.00802578
Iteration 95, loss = 0.00773800
Iteration 96, loss = 0.00752791
Iteration 97, loss = 0.00740059
Iteration 98, loss = 0.00709056
Iteration 99, loss = 0.00693953
Iteration 100, loss = 0.00676504
Iteration 101, loss = 0.00655601
Iteration 102, loss = 0.00643359
Iteration 103, loss = 0.00628720
Iteration 104, loss = 0.00611612
Iteration 105, loss = 0.00595132
Iteration 106, loss = 0.00581445
Iteration 107, loss = 0.00570121
Iteration 108, loss = 0.00550667
Iteration 109, loss = 0.00539441
Iteration 110, loss = 0.00525900
Iteration 111, loss = 0.00514312
Iteration 112, loss = 0.00506382
Iteration 113, loss = 0.00499076
Iteration 114, loss = 0.00482371
Iteration 115, loss = 0.00465838
Iteration 116, loss = 0.00457766
Iteration 117, loss = 0.00447385
Iteration 118, loss = 0.00437399
Iteration 119, loss = 0.00428623
Iteration 120, loss = 0.00419796
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  90
Error rate: 24.64% ( 616/2500)
 - Class 1:  27, Class 2:  21, Class 3: 500, Class 4:  34, Class 5:  34
Iteration 1, loss = 1.24174982
Iteration 2, loss = 0.70873657
Iteration 3, loss = 0.44529355
Iteration 4, loss = 0.31831635
Iteration 5, loss = 0.25022199
Iteration 6, loss = 0.20927210
Iteration 7, loss = 0.18241958
Iteration 8, loss = 0.16243150
Iteration 9, loss = 0.14730434
Iteration 10, loss = 0.13626849
Iteration 11, loss = 0.12597763
Iteration 12, loss = 0.11797168
Iteration 13, loss = 0.11106055
Iteration 14, loss = 0.10505643
Iteration 15, loss = 0.10003211
Iteration 16, loss = 0.09490929
Iteration 17, loss = 0.09051605
Iteration 18, loss = 0.08748011
Iteration 19, loss = 0.08297902
Iteration 20, loss = 0.07952654
Iteration 21, loss = 0.07648029
Iteration 22, loss = 0.07366533
Iteration 23, loss = 0.07010645
Iteration 24, loss = 0.06794794
Iteration 25, loss = 0.06538408
Iteration 26, loss = 0.06283687
Iteration 27, loss = 0.06073315
Iteration 28, loss = 0.05876882
Iteration 29, loss = 0.05713562
Iteration 30, loss = 0.05465311
Iteration 31, loss = 0.05307926
Iteration 32, loss = 0.05118952
Iteration 33, loss = 0.04932718
Iteration 34, loss = 0.04761562
Iteration 35, loss = 0.04633881
Iteration 36, loss = 0.04510047
Iteration 37, loss = 0.04369304
Iteration 38, loss = 0.04209274
Iteration 39, loss = 0.04056855
Iteration 40, loss = 0.03965434
Iteration 41, loss = 0.03824500
Iteration 42, loss = 0.03708879
Iteration 43, loss = 0.03580671
Iteration 44, loss = 0.03477114
Iteration 45, loss = 0.03351626
Iteration 46, loss = 0.03274445
Iteration 47, loss = 0.03181052
Iteration 48, loss = 0.03079572
Iteration 49, loss = 0.02982318
Iteration 50, loss = 0.02877615
Iteration 51, loss = 0.02766739
Iteration 52, loss = 0.02694621
Iteration 53, loss = 0.02619297
Iteration 54, loss = 0.02537508
Iteration 55, loss = 0.02490072
Iteration 56, loss = 0.02422481
Iteration 57, loss = 0.02309294
Iteration 58, loss = 0.02241765
Iteration 59, loss = 0.02188852
Iteration 60, loss = 0.02102718
Iteration 61, loss = 0.02062425
Iteration 62, loss = 0.01983899
Iteration 63, loss = 0.01922866
Iteration 64, loss = 0.01864215
Iteration 65, loss = 0.01815204
Iteration 66, loss = 0.01749160
Iteration 67, loss = 0.01687315
Iteration 68, loss = 0.01629023
Iteration 69, loss = 0.01603175
Iteration 70, loss = 0.01556480
Iteration 71, loss = 0.01502827
Iteration 72, loss = 0.01437981
Iteration 73, loss = 0.01429470
Iteration 74, loss = 0.01383197
Iteration 75, loss = 0.01322475
Iteration 76, loss = 0.01281914
Iteration 77, loss = 0.01238891
Iteration 78, loss = 0.01206337
Iteration 79, loss = 0.01167714
Iteration 80, loss = 0.01134046
Iteration 81, loss = 0.01109695
Iteration 82, loss = 0.01072253
Iteration 83, loss = 0.01041526
Iteration 84, loss = 0.01010324
Iteration 85, loss = 0.00986117
Iteration 86, loss = 0.00957317
Iteration 87, loss = 0.00931340
Iteration 88, loss = 0.00903409
Iteration 89, loss = 0.00880693
Iteration 90, loss = 0.00862095
Iteration 91, loss = 0.00841386
Iteration 92, loss = 0.00815443
Iteration 93, loss = 0.00784312
Iteration 94, loss = 0.00767931
Iteration 95, loss = 0.00747781
Iteration 96, loss = 0.00729401
Iteration 97, loss = 0.00709812
Iteration 98, loss = 0.00689650
Iteration 99, loss = 0.00672093
Iteration 100, loss = 0.00650067
Iteration 101, loss = 0.00634727
Iteration 102, loss = 0.00620278
Iteration 103, loss = 0.00602022
Iteration 104, loss = 0.00592754
Iteration 105, loss = 0.00581191
Iteration 106, loss = 0.00561514
Iteration 107, loss = 0.00545612
Iteration 108, loss = 0.00536746
Iteration 109, loss = 0.00522330
Iteration 110, loss = 0.00510067
Iteration 111, loss = 0.00498999
Iteration 112, loss = 0.00486814
Iteration 113, loss = 0.00476547
Iteration 114, loss = 0.00462198
Iteration 115, loss = 0.00455149
Iteration 116, loss = 0.00441693
Iteration 117, loss = 0.00433851
Iteration 118, loss = 0.00423909
Iteration 119, loss = 0.00414629
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  95
Error rate: 59.96% (1499/2500)
 - Class 1: 472, Class 2: 499, Class 3: 488, Class 4:  30, Class 5:  10
Iteration 1, loss = 1.24594514
Iteration 2, loss = 0.70087184
Iteration 3, loss = 0.43772692
Iteration 4, loss = 0.31098408
Iteration 5, loss = 0.24442802
Iteration 6, loss = 0.20468609
Iteration 7, loss = 0.17851526
Iteration 8, loss = 0.15924902
Iteration 9, loss = 0.14444610
Iteration 10, loss = 0.13314423
Iteration 11, loss = 0.12381885
Iteration 12, loss = 0.11628486
Iteration 13, loss = 0.10969159
Iteration 14, loss = 0.10383361
Iteration 15, loss = 0.09868478
Iteration 16, loss = 0.09374817
Iteration 17, loss = 0.08938480
Iteration 18, loss = 0.08558186
Iteration 19, loss = 0.08184986
Iteration 20, loss = 0.07870442
Iteration 21, loss = 0.07568647
Iteration 22, loss = 0.07218261
Iteration 23, loss = 0.06994338
Iteration 24, loss = 0.06720368
Iteration 25, loss = 0.06439942
Iteration 26, loss = 0.06358455
Iteration 27, loss = 0.06048476
Iteration 28, loss = 0.05767495
Iteration 29, loss = 0.05555126
Iteration 30, loss = 0.05356651
Iteration 31, loss = 0.05197367
Iteration 32, loss = 0.05017462
Iteration 33, loss = 0.04889970
Iteration 34, loss = 0.04718987
Iteration 35, loss = 0.04570117
Iteration 36, loss = 0.04436095
Iteration 37, loss = 0.04311775
Iteration 38, loss = 0.04185327
Iteration 39, loss = 0.04001391
Iteration 40, loss = 0.03891997
Iteration 41, loss = 0.03759548
Iteration 42, loss = 0.03648012
Iteration 43, loss = 0.03514038
Iteration 44, loss = 0.03418080
Iteration 45, loss = 0.03323186
Iteration 46, loss = 0.03220363
Iteration 47, loss = 0.03123956
Iteration 48, loss = 0.03005678
Iteration 49, loss = 0.02918532
Iteration 50, loss = 0.02835366
Iteration 51, loss = 0.02764103
Iteration 52, loss = 0.02672054
Iteration 53, loss = 0.02588990
Iteration 54, loss = 0.02505896
Iteration 55, loss = 0.02431765
Iteration 56, loss = 0.02351365
Iteration 57, loss = 0.02276983
Iteration 58, loss = 0.02191193
Iteration 59, loss = 0.02133482
Iteration 60, loss = 0.02065731
Iteration 61, loss = 0.02016558
Iteration 62, loss = 0.01954588
Iteration 63, loss = 0.01877407
Iteration 64, loss = 0.01820510
Iteration 65, loss = 0.01776065
Iteration 66, loss = 0.01731419
Iteration 67, loss = 0.01647755
Iteration 68, loss = 0.01612570
Iteration 69, loss = 0.01547846
Iteration 70, loss = 0.01517800
Iteration 71, loss = 0.01469376
Iteration 72, loss = 0.01416516
Iteration 73, loss = 0.01368123
Iteration 74, loss = 0.01320999
Iteration 75, loss = 0.01288386
Iteration 76, loss = 0.01253654
Iteration 77, loss = 0.01209709
Iteration 78, loss = 0.01183036
Iteration 79, loss = 0.01142872
Iteration 80, loss = 0.01103513
Iteration 81, loss = 0.01062419
Iteration 82, loss = 0.01042879
Iteration 83, loss = 0.01016090
Iteration 84, loss = 0.00978186
Iteration 85, loss = 0.00951306
Iteration 86, loss = 0.00930182
Iteration 87, loss = 0.00897285
Iteration 88, loss = 0.00868238
Iteration 89, loss = 0.00848926
Iteration 90, loss = 0.00830499
Iteration 91, loss = 0.00801368
Iteration 92, loss = 0.00777720
Iteration 93, loss = 0.00757947
Iteration 94, loss = 0.00730729
Iteration 95, loss = 0.00718918
Iteration 96, loss = 0.00699783
Iteration 97, loss = 0.00678853
Iteration 98, loss = 0.00661528
Iteration 99, loss = 0.00643405
Iteration 100, loss = 0.00626817
Iteration 101, loss = 0.00606946
Iteration 102, loss = 0.00590510
Iteration 103, loss = 0.00581714
Iteration 104, loss = 0.00564819
Iteration 105, loss = 0.00549897
Iteration 106, loss = 0.00536260
Iteration 107, loss = 0.00524893
Iteration 108, loss = 0.00515537
Iteration 109, loss = 0.00502156
Iteration 110, loss = 0.00489725
Iteration 111, loss = 0.00475321
Iteration 112, loss = 0.00464749
Iteration 113, loss = 0.00459276
Iteration 114, loss = 0.00447699
Iteration 115, loss = 0.00432804
Iteration 116, loss = 0.00428165
Iteration 117, loss = 0.00415082
Iteration 118, loss = 0.00409046
Iteration 119, loss = 0.00397816
Iteration 120, loss = 0.00390346
Iteration 121, loss = 0.00381315
Iteration 122, loss = 0.00376304
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted: 100
Error rate: 12.24% ( 306/2500)
 - Class 1:   9, Class 2:  17, Class 3:  89, Class 4:  86, Class 5: 105
Iteration 1, loss = 1.56381470
Iteration 2, loss = 1.44465872
Iteration 3, loss = 1.35798204
Iteration 4, loss = 1.28925537
Iteration 5, loss = 1.23052203
Iteration 6, loss = 1.17714370
Iteration 7, loss = 1.12886558
Iteration 8, loss = 1.08409243
Iteration 9, loss = 1.04274071
Iteration 10, loss = 1.00411362
Iteration 11, loss = 0.96794242
Iteration 12, loss = 0.93398764
Iteration 13, loss = 0.90153847
Iteration 14, loss = 0.87069022
Iteration 15, loss = 0.84102189
Iteration 16, loss = 0.81232682
Iteration 17, loss = 0.78487428
Iteration 18, loss = 0.75823332
Iteration 19, loss = 0.73285180
Iteration 20, loss = 0.70856445
Iteration 21, loss = 0.68555071
Iteration 22, loss = 0.66343568
Iteration 23, loss = 0.64275119
Iteration 24, loss = 0.62287602
Iteration 25, loss = 0.60419031
Iteration 26, loss = 0.58639495
Iteration 27, loss = 0.56949815
Iteration 28, loss = 0.55334513
Iteration 29, loss = 0.53763993
Iteration 30, loss = 0.52278913
Iteration 31, loss = 0.50862608
Iteration 32, loss = 0.49516368
Iteration 33, loss = 0.48226614
Iteration 34, loss = 0.46985339
Iteration 35, loss = 0.45792949
Iteration 36, loss = 0.44655149
Iteration 37, loss = 0.43546032
Iteration 38, loss = 0.42512327
Iteration 39, loss = 0.41470107
Iteration 40, loss = 0.40525926
Iteration 41, loss = 0.39567456
Iteration 42, loss = 0.38637520
Iteration 43, loss = 0.37755792
Iteration 44, loss = 0.36907986
Iteration 45, loss = 0.36055613
Iteration 46, loss = 0.35264817
Iteration 47, loss = 0.34497447
Iteration 48, loss = 0.33730310
Iteration 49, loss = 0.33003863
Iteration 50, loss = 0.32294197
Iteration 51, loss = 0.31596487
Iteration 52, loss = 0.30983618
Iteration 53, loss = 0.30328721
Iteration 54, loss = 0.29710881
Iteration 55, loss = 0.29114233
Iteration 56, loss = 0.28547821
Iteration 57, loss = 0.28017730
Iteration 58, loss = 0.27481429
Iteration 59, loss = 0.26996413
Iteration 60, loss = 0.26489114
Iteration 61, loss = 0.26004302
Iteration 62, loss = 0.25531589
Iteration 63, loss = 0.25107401
Iteration 64, loss = 0.24677856
Iteration 65, loss = 0.24238585
Iteration 66, loss = 0.23850312
Iteration 67, loss = 0.23447110
Iteration 68, loss = 0.23071045
Iteration 69, loss = 0.22712312
Iteration 70, loss = 0.22341821
Iteration 71, loss = 0.22009481
Iteration 72, loss = 0.21679197
Iteration 73, loss = 0.21336581
Iteration 74, loss = 0.21029127
Iteration 75, loss = 0.20722416
Iteration 76, loss = 0.20396957
Iteration 77, loss = 0.20114289
Iteration 78, loss = 0.19829746
Iteration 79, loss = 0.19555139
Iteration 80, loss = 0.19268393
Iteration 81, loss = 0.18994800
Iteration 82, loss = 0.18746838
Iteration 83, loss = 0.18497052
Iteration 84, loss = 0.18283424
Iteration 85, loss = 0.17995202
Iteration 86, loss = 0.17752073
Iteration 87, loss = 0.17550152
Iteration 88, loss = 0.17321294
Iteration 89, loss = 0.17095012
Iteration 90, loss = 0.16876554
Iteration 91, loss = 0.16666115
Iteration 92, loss = 0.16442805
Iteration 93, loss = 0.16258432
Iteration 94, loss = 0.16058084
Iteration 95, loss = 0.15849007
Iteration 96, loss = 0.15658882
Iteration 97, loss = 0.15478481
Iteration 98, loss = 0.15294771
Iteration 99, loss = 0.15106649
Iteration 100, loss = 0.14935461
Iteration 101, loss = 0.14782494
Iteration 102, loss = 0.14615504
Iteration 103, loss = 0.14441618
Iteration 104, loss = 0.14271302
Iteration 105, loss = 0.14124500
Iteration 106, loss = 0.13978570
Iteration 107, loss = 0.13827193
Iteration 108, loss = 0.13683144
Iteration 109, loss = 0.13522631
Iteration 110, loss = 0.13394579
Iteration 111, loss = 0.13241946
Iteration 112, loss = 0.13097749
Iteration 113, loss = 0.12961087
Iteration 114, loss = 0.12840564
Iteration 115, loss = 0.12704202
Iteration 116, loss = 0.12560463
Iteration 117, loss = 0.12436670
Iteration 118, loss = 0.12320055
Iteration 119, loss = 0.12189136
Iteration 120, loss = 0.12063004
Iteration 121, loss = 0.11954407
Iteration 122, loss = 0.11844014
Iteration 123, loss = 0.11716878
Iteration 124, loss = 0.11603082
Iteration 125, loss = 0.11493715
Iteration 126, loss = 0.11380000
Iteration 127, loss = 0.11287465
Iteration 128, loss = 0.11187789
Iteration 129, loss = 0.11066246
Iteration 130, loss = 0.10974078
Iteration 131, loss = 0.10882429
Iteration 132, loss = 0.10774172
Iteration 133, loss = 0.10657670
Iteration 134, loss = 0.10558694
Iteration 135, loss = 0.10467361
Iteration 136, loss = 0.10370522
Iteration 137, loss = 0.10279551
Iteration 138, loss = 0.10193076
Iteration 139, loss = 0.10112200
Iteration 140, loss = 0.10024793
Iteration 141, loss = 0.09915892
Iteration 142, loss = 0.09843464
Iteration 143, loss = 0.09744859
Iteration 144, loss = 0.09668281
Iteration 145, loss = 0.09576528
Iteration 146, loss = 0.09487927
Iteration 147, loss = 0.09404281
Iteration 148, loss = 0.09323335
Iteration 149, loss = 0.09256055
Iteration 150, loss = 0.09177741
Iteration 151, loss = 0.09098870
Iteration 152, loss = 0.09024323
Iteration 153, loss = 0.08940775
Iteration 154, loss = 0.08867123
Iteration 155, loss = 0.08783720
Iteration 156, loss = 0.08721514
Iteration 157, loss = 0.08648916
Iteration 158, loss = 0.08571157
Iteration 159, loss = 0.08501561
Iteration 160, loss = 0.08436138
Iteration 161, loss = 0.08359033
Iteration 162, loss = 0.08291063
Iteration 163, loss = 0.08210909
Iteration 164, loss = 0.08147337
Iteration 165, loss = 0.08078525
Iteration 166, loss = 0.08013421
Iteration 167, loss = 0.07937777
Iteration 168, loss = 0.07876870
Iteration 169, loss = 0.07826021
Iteration 170, loss = 0.07756422
Iteration 171, loss = 0.07690398
Iteration 172, loss = 0.07615457
Iteration 173, loss = 0.07551095
Iteration 174, loss = 0.07496254
Iteration 175, loss = 0.07417555
Iteration 176, loss = 0.07367289
Iteration 177, loss = 0.07301302
Iteration 178, loss = 0.07246199
Iteration 179, loss = 0.07197197
Iteration 180, loss = 0.07135139
Iteration 181, loss = 0.07077775
Iteration 182, loss = 0.07027543
Iteration 183, loss = 0.06975927
Iteration 184, loss = 0.06916141
Iteration 185, loss = 0.06868320
Iteration 186, loss = 0.06817410
Iteration 187, loss = 0.06762435
Iteration 188, loss = 0.06718935
Iteration 189, loss = 0.06661401
Iteration 190, loss = 0.06609715
Iteration 191, loss = 0.06562689
Iteration 192, loss = 0.06506349
Iteration 193, loss = 0.06469004
Iteration 194, loss = 0.06409326
Iteration 195, loss = 0.06366457
Iteration 196, loss = 0.06313460
Iteration 197, loss = 0.06263219
Iteration 198, loss = 0.06224563
Iteration 199, loss = 0.06178252
Iteration 200, loss = 0.06133025
Number of features extracted:   5
Error rate: 8.64% ( 216/2500)
 - Class 1:  24, Class 2:  25, Class 3:  71, Class 4:  57, Class 5:  39
Iteration 1, loss = 1.48902404
Iteration 2, loss = 1.27893528
Iteration 3, loss = 1.12583714
Iteration 4, loss = 1.00472907
Iteration 5, loss = 0.90440232
Iteration 6, loss = 0.81974180
Iteration 7, loss = 0.74714453
Iteration 8, loss = 0.68496456
Iteration 9, loss = 0.63078787
Iteration 10, loss = 0.58346006
Iteration 11, loss = 0.54192127
Iteration 12, loss = 0.50524162
Iteration 13, loss = 0.47211978
Iteration 14, loss = 0.44276196
Iteration 15, loss = 0.41664625
Iteration 16, loss = 0.39294250
Iteration 17, loss = 0.37179625
Iteration 18, loss = 0.35263211
Iteration 19, loss = 0.33500995
Iteration 20, loss = 0.31919242
Iteration 21, loss = 0.30459631
Iteration 22, loss = 0.29137121
Iteration 23, loss = 0.27916212
Iteration 24, loss = 0.26777164
Iteration 25, loss = 0.25734235
Iteration 26, loss = 0.24769434
Iteration 27, loss = 0.23875635
Iteration 28, loss = 0.23031003
Iteration 29, loss = 0.22246621
Iteration 30, loss = 0.21518740
Iteration 31, loss = 0.20817787
Iteration 32, loss = 0.20162324
Iteration 33, loss = 0.19564981
Iteration 34, loss = 0.18961755
Iteration 35, loss = 0.18429620
Iteration 36, loss = 0.17914393
Iteration 37, loss = 0.17431447
Iteration 38, loss = 0.16969869
Iteration 39, loss = 0.16534641
Iteration 40, loss = 0.16099469
Iteration 41, loss = 0.15687503
Iteration 42, loss = 0.15306119
Iteration 43, loss = 0.14933708
Iteration 44, loss = 0.14595808
Iteration 45, loss = 0.14255098
Iteration 46, loss = 0.13952132
Iteration 47, loss = 0.13657176
Iteration 48, loss = 0.13351557
Iteration 49, loss = 0.13096451
Iteration 50, loss = 0.12790991
Iteration 51, loss = 0.12532218
Iteration 52, loss = 0.12282207
Iteration 53, loss = 0.12046738
Iteration 54, loss = 0.11820994
Iteration 55, loss = 0.11585924
Iteration 56, loss = 0.11358227
Iteration 57, loss = 0.11172228
Iteration 58, loss = 0.10949047
Iteration 59, loss = 0.10760436
Iteration 60, loss = 0.10588020
Iteration 61, loss = 0.10386823
Iteration 62, loss = 0.10209045
Iteration 63, loss = 0.10025368
Iteration 64, loss = 0.09851975
Iteration 65, loss = 0.09694568
Iteration 66, loss = 0.09548247
Iteration 67, loss = 0.09365310
Iteration 68, loss = 0.09232624
Iteration 69, loss = 0.09084258
Iteration 70, loss = 0.08955627
Iteration 71, loss = 0.08814509
Iteration 72, loss = 0.08665925
Iteration 73, loss = 0.08536276
Iteration 74, loss = 0.08397381
Iteration 75, loss = 0.08281738
Iteration 76, loss = 0.08141605
Iteration 77, loss = 0.08028735
Iteration 78, loss = 0.07895714
Iteration 79, loss = 0.07800789
Iteration 80, loss = 0.07670415
Iteration 81, loss = 0.07561039
Iteration 82, loss = 0.07453405
Iteration 83, loss = 0.07352600
Iteration 84, loss = 0.07251041
Iteration 85, loss = 0.07144327
Iteration 86, loss = 0.07043722
Iteration 87, loss = 0.06957249
Iteration 88, loss = 0.06853268
Iteration 89, loss = 0.06743567
Iteration 90, loss = 0.06648064
Iteration 91, loss = 0.06567948
Iteration 92, loss = 0.06485008
Iteration 93, loss = 0.06398801
Iteration 94, loss = 0.06299958
Iteration 95, loss = 0.06214881
Iteration 96, loss = 0.06130925
Iteration 97, loss = 0.06064795
Iteration 98, loss = 0.05964354
Iteration 99, loss = 0.05899469
Iteration 100, loss = 0.05823852
Iteration 101, loss = 0.05753484
Iteration 102, loss = 0.05665920
Iteration 103, loss = 0.05606509
Iteration 104, loss = 0.05525847
Iteration 105, loss = 0.05468798
Iteration 106, loss = 0.05387314
Iteration 107, loss = 0.05332105
Iteration 108, loss = 0.05255322
Iteration 109, loss = 0.05192506
Iteration 110, loss = 0.05146209
Iteration 111, loss = 0.05069130
Iteration 112, loss = 0.05015499
Iteration 113, loss = 0.04957289
Iteration 114, loss = 0.04892194
Iteration 115, loss = 0.04823513
Iteration 116, loss = 0.04763548
Iteration 117, loss = 0.04714904
Iteration 118, loss = 0.04656557
Iteration 119, loss = 0.04597007
Iteration 120, loss = 0.04544530
Iteration 121, loss = 0.04485789
Iteration 122, loss = 0.04436281
Iteration 123, loss = 0.04384069
Iteration 124, loss = 0.04345410
Iteration 125, loss = 0.04279785
Iteration 126, loss = 0.04224417
Iteration 127, loss = 0.04186939
Iteration 128, loss = 0.04132970
Iteration 129, loss = 0.04082763
Iteration 130, loss = 0.04026695
Iteration 131, loss = 0.03985707
Iteration 132, loss = 0.03933393
Iteration 133, loss = 0.03890175
Iteration 134, loss = 0.03840125
Iteration 135, loss = 0.03807661
Iteration 136, loss = 0.03762079
Iteration 137, loss = 0.03721658
Iteration 138, loss = 0.03672252
Iteration 139, loss = 0.03620425
Iteration 140, loss = 0.03586894
Iteration 141, loss = 0.03546659
Iteration 142, loss = 0.03501811
Iteration 143, loss = 0.03464640
Iteration 144, loss = 0.03425173
Iteration 145, loss = 0.03391245
Iteration 146, loss = 0.03361407
Iteration 147, loss = 0.03320700
Iteration 148, loss = 0.03283753
Iteration 149, loss = 0.03243866
Iteration 150, loss = 0.03217684
Iteration 151, loss = 0.03181688
Iteration 152, loss = 0.03149011
Iteration 153, loss = 0.03099045
Iteration 154, loss = 0.03073687
Iteration 155, loss = 0.03045651
Iteration 156, loss = 0.03009139
Iteration 157, loss = 0.02971295
Iteration 158, loss = 0.02933881
Iteration 159, loss = 0.02911330
Iteration 160, loss = 0.02874133
Iteration 161, loss = 0.02853915
Iteration 162, loss = 0.02815073
Iteration 163, loss = 0.02781961
Iteration 164, loss = 0.02752322
Iteration 165, loss = 0.02722584
Iteration 166, loss = 0.02698344
Iteration 167, loss = 0.02668163
Iteration 168, loss = 0.02637646
Iteration 169, loss = 0.02609638
Iteration 170, loss = 0.02579784
Iteration 171, loss = 0.02552155
Iteration 172, loss = 0.02527092
Iteration 173, loss = 0.02500308
Iteration 174, loss = 0.02472157
Iteration 175, loss = 0.02443235
Iteration 176, loss = 0.02416524
Iteration 177, loss = 0.02392168
Iteration 178, loss = 0.02371054
Iteration 179, loss = 0.02352635
Iteration 180, loss = 0.02315291
Iteration 181, loss = 0.02296551
Iteration 182, loss = 0.02265858
Iteration 183, loss = 0.02236439
Iteration 184, loss = 0.02212639
Iteration 185, loss = 0.02188761
Iteration 186, loss = 0.02164117
Iteration 187, loss = 0.02140680
Iteration 188, loss = 0.02118517
Iteration 189, loss = 0.02093612
Iteration 190, loss = 0.02064526
Iteration 191, loss = 0.02043842
Iteration 192, loss = 0.02024701
Iteration 193, loss = 0.02005271
Iteration 194, loss = 0.01981969
Iteration 195, loss = 0.01962781
Iteration 196, loss = 0.01941935
Iteration 197, loss = 0.01921700
Iteration 198, loss = 0.01903053
Iteration 199, loss = 0.01884731
Iteration 200, loss = 0.01865389
Number of features extracted:  10
Error rate: 26.04% ( 651/2500)
 - Class 1:  35, Class 2:  23, Class 3:  84, Class 4: 499, Class 5:  10
Iteration 1, loss = 1.50816783
Iteration 2, loss = 1.23982520
Iteration 3, loss = 1.07915588
Iteration 4, loss = 0.96135009
Iteration 5, loss = 0.86433984
Iteration 6, loss = 0.77981615
Iteration 7, loss = 0.70474732
Iteration 8, loss = 0.63722314
Iteration 9, loss = 0.57709303
Iteration 10, loss = 0.52364056
Iteration 11, loss = 0.47701841
Iteration 12, loss = 0.43663350
Iteration 13, loss = 0.40160253
Iteration 14, loss = 0.37107966
Iteration 15, loss = 0.34470134
Iteration 16, loss = 0.32149837
Iteration 17, loss = 0.30111057
Iteration 18, loss = 0.28356993
Iteration 19, loss = 0.26810985
Iteration 20, loss = 0.25399779
Iteration 21, loss = 0.24125475
Iteration 22, loss = 0.22987334
Iteration 23, loss = 0.21947148
Iteration 24, loss = 0.20994109
Iteration 25, loss = 0.20152523
Iteration 26, loss = 0.19348231
Iteration 27, loss = 0.18616427
Iteration 28, loss = 0.17959667
Iteration 29, loss = 0.17337220
Iteration 30, loss = 0.16768777
Iteration 31, loss = 0.16214704
Iteration 32, loss = 0.15694005
Iteration 33, loss = 0.15236820
Iteration 34, loss = 0.14781766
Iteration 35, loss = 0.14335463
Iteration 36, loss = 0.13937870
Iteration 37, loss = 0.13567223
Iteration 38, loss = 0.13191988
Iteration 39, loss = 0.12847303
Iteration 40, loss = 0.12554589
Iteration 41, loss = 0.12220577
Iteration 42, loss = 0.11908750
Iteration 43, loss = 0.11627222
Iteration 44, loss = 0.11350054
Iteration 45, loss = 0.11099213
Iteration 46, loss = 0.10822914
Iteration 47, loss = 0.10597996
Iteration 48, loss = 0.10359370
Iteration 49, loss = 0.10106514
Iteration 50, loss = 0.09900149
Iteration 51, loss = 0.09700243
Iteration 52, loss = 0.09469859
Iteration 53, loss = 0.09291021
Iteration 54, loss = 0.09106763
Iteration 55, loss = 0.08912739
Iteration 56, loss = 0.08723221
Iteration 57, loss = 0.08568771
Iteration 58, loss = 0.08399442
Iteration 59, loss = 0.08247862
Iteration 60, loss = 0.08082138
Iteration 61, loss = 0.07931948
Iteration 62, loss = 0.07780522
Iteration 63, loss = 0.07633310
Iteration 64, loss = 0.07483002
Iteration 65, loss = 0.07351366
Iteration 66, loss = 0.07229198
Iteration 67, loss = 0.07088715
Iteration 68, loss = 0.06957815
Iteration 69, loss = 0.06841643
Iteration 70, loss = 0.06734691
Iteration 71, loss = 0.06608980
Iteration 72, loss = 0.06499076
Iteration 73, loss = 0.06392451
Iteration 74, loss = 0.06283622
Iteration 75, loss = 0.06167925
Iteration 76, loss = 0.06078551
Iteration 77, loss = 0.05972768
Iteration 78, loss = 0.05888423
Iteration 79, loss = 0.05790041
Iteration 80, loss = 0.05704664
Iteration 81, loss = 0.05612541
Iteration 82, loss = 0.05518790
Iteration 83, loss = 0.05445493
Iteration 84, loss = 0.05355170
Iteration 85, loss = 0.05264980
Iteration 86, loss = 0.05187354
Iteration 87, loss = 0.05130764
Iteration 88, loss = 0.05031080
Iteration 89, loss = 0.04960069
Iteration 90, loss = 0.04885748
Iteration 91, loss = 0.04820869
Iteration 92, loss = 0.04747690
Iteration 93, loss = 0.04676319
Iteration 94, loss = 0.04604853
Iteration 95, loss = 0.04550565
Iteration 96, loss = 0.04484809
Iteration 97, loss = 0.04415366
Iteration 98, loss = 0.04347859
Iteration 99, loss = 0.04297093
Iteration 100, loss = 0.04238393
Iteration 101, loss = 0.04175158
Iteration 102, loss = 0.04111054
Iteration 103, loss = 0.04050984
Iteration 104, loss = 0.04010902
Iteration 105, loss = 0.03952010
Iteration 106, loss = 0.03891783
Iteration 107, loss = 0.03839558
Iteration 108, loss = 0.03788736
Iteration 109, loss = 0.03742795
Iteration 110, loss = 0.03682239
Iteration 111, loss = 0.03634211
Iteration 112, loss = 0.03587892
Iteration 113, loss = 0.03537127
Iteration 114, loss = 0.03489309
Iteration 115, loss = 0.03450395
Iteration 116, loss = 0.03397735
Iteration 117, loss = 0.03363451
Iteration 118, loss = 0.03319819
Iteration 119, loss = 0.03287364
Iteration 120, loss = 0.03229468
Iteration 121, loss = 0.03191526
Iteration 122, loss = 0.03161930
Iteration 123, loss = 0.03112332
Iteration 124, loss = 0.03082393
Iteration 125, loss = 0.03038192
Iteration 126, loss = 0.03000817
Iteration 127, loss = 0.02979022
Iteration 128, loss = 0.02925674
Iteration 129, loss = 0.02887931
Iteration 130, loss = 0.02851996
Iteration 131, loss = 0.02820952
Iteration 132, loss = 0.02783867
Iteration 133, loss = 0.02759490
Iteration 134, loss = 0.02722932
Iteration 135, loss = 0.02699416
Iteration 136, loss = 0.02658639
Iteration 137, loss = 0.02633807
Iteration 138, loss = 0.02589394
Iteration 139, loss = 0.02562021
Iteration 140, loss = 0.02530954
Iteration 141, loss = 0.02510447
Iteration 142, loss = 0.02470000
Iteration 143, loss = 0.02440725
Iteration 144, loss = 0.02422563
Iteration 145, loss = 0.02396386
Iteration 146, loss = 0.02366439
Iteration 147, loss = 0.02338487
Iteration 148, loss = 0.02311464
Iteration 149, loss = 0.02278557
Iteration 150, loss = 0.02254664
Iteration 151, loss = 0.02228982
Iteration 152, loss = 0.02205385
Iteration 153, loss = 0.02179194
Iteration 154, loss = 0.02157719
Iteration 155, loss = 0.02132918
Iteration 156, loss = 0.02104601
Iteration 157, loss = 0.02094567
Iteration 158, loss = 0.02065707
Iteration 159, loss = 0.02038933
Iteration 160, loss = 0.02017916
Iteration 161, loss = 0.01996633
Iteration 162, loss = 0.01972229
Iteration 163, loss = 0.01950269
Iteration 164, loss = 0.01932137
Iteration 165, loss = 0.01913857
Iteration 166, loss = 0.01891210
Iteration 167, loss = 0.01874120
Iteration 168, loss = 0.01852174
Iteration 169, loss = 0.01832227
Iteration 170, loss = 0.01815663
Iteration 171, loss = 0.01796580
Iteration 172, loss = 0.01775556
Iteration 173, loss = 0.01759336
Iteration 174, loss = 0.01736671
Iteration 175, loss = 0.01726267
Iteration 176, loss = 0.01704815
Iteration 177, loss = 0.01688895
Iteration 178, loss = 0.01668488
Iteration 179, loss = 0.01648844
Iteration 180, loss = 0.01636127
Iteration 181, loss = 0.01615405
Iteration 182, loss = 0.01597443
Iteration 183, loss = 0.01583531
Iteration 184, loss = 0.01562923
Iteration 185, loss = 0.01549583
Iteration 186, loss = 0.01531636
Iteration 187, loss = 0.01516413
Iteration 188, loss = 0.01496094
Iteration 189, loss = 0.01478828
Iteration 190, loss = 0.01468208
Iteration 191, loss = 0.01455282
Iteration 192, loss = 0.01436104
Iteration 193, loss = 0.01420534
Iteration 194, loss = 0.01406184
Iteration 195, loss = 0.01391380
Iteration 196, loss = 0.01376866
Iteration 197, loss = 0.01362894
Iteration 198, loss = 0.01345987
Iteration 199, loss = 0.01334563
Iteration 200, loss = 0.01315715
Number of features extracted:  15
Error rate: 9.28% ( 232/2500)
 - Class 1:  24, Class 2:  19, Class 3:  88, Class 4:  47, Class 5:  54
Iteration 1, loss = 1.49568111
Iteration 2, loss = 1.19567904
Iteration 3, loss = 0.99850871
Iteration 4, loss = 0.84655243
Iteration 5, loss = 0.72457103
Iteration 6, loss = 0.62587879
Iteration 7, loss = 0.54512106
Iteration 8, loss = 0.47907243
Iteration 9, loss = 0.42453547
Iteration 10, loss = 0.37959109
Iteration 11, loss = 0.34251141
Iteration 12, loss = 0.31149040
Iteration 13, loss = 0.28528929
Iteration 14, loss = 0.26310326
Iteration 15, loss = 0.24424884
Iteration 16, loss = 0.22788559
Iteration 17, loss = 0.21398706
Iteration 18, loss = 0.20170829
Iteration 19, loss = 0.19064136
Iteration 20, loss = 0.18120998
Iteration 21, loss = 0.17272385
Iteration 22, loss = 0.16475787
Iteration 23, loss = 0.15754841
Iteration 24, loss = 0.15125236
Iteration 25, loss = 0.14534973
Iteration 26, loss = 0.13997823
Iteration 27, loss = 0.13508024
Iteration 28, loss = 0.13032080
Iteration 29, loss = 0.12613323
Iteration 30, loss = 0.12199141
Iteration 31, loss = 0.11821682
Iteration 32, loss = 0.11467802
Iteration 33, loss = 0.11144159
Iteration 34, loss = 0.10824102
Iteration 35, loss = 0.10514626
Iteration 36, loss = 0.10235199
Iteration 37, loss = 0.09965480
Iteration 38, loss = 0.09696627
Iteration 39, loss = 0.09448654
Iteration 40, loss = 0.09205025
Iteration 41, loss = 0.08963561
Iteration 42, loss = 0.08741408
Iteration 43, loss = 0.08559241
Iteration 44, loss = 0.08345094
Iteration 45, loss = 0.08139317
Iteration 46, loss = 0.07946204
Iteration 47, loss = 0.07811255
Iteration 48, loss = 0.07582376
Iteration 49, loss = 0.07429784
Iteration 50, loss = 0.07287979
Iteration 51, loss = 0.07108292
Iteration 52, loss = 0.06947915
Iteration 53, loss = 0.06812869
Iteration 54, loss = 0.06667968
Iteration 55, loss = 0.06518766
Iteration 56, loss = 0.06393932
Iteration 57, loss = 0.06266148
Iteration 58, loss = 0.06122815
Iteration 59, loss = 0.06028116
Iteration 60, loss = 0.05883078
Iteration 61, loss = 0.05760659
Iteration 62, loss = 0.05658696
Iteration 63, loss = 0.05535956
Iteration 64, loss = 0.05437459
Iteration 65, loss = 0.05338364
Iteration 66, loss = 0.05224687
Iteration 67, loss = 0.05121210
Iteration 68, loss = 0.05031998
Iteration 69, loss = 0.04948226
Iteration 70, loss = 0.04853294
Iteration 71, loss = 0.04743008
Iteration 72, loss = 0.04665006
Iteration 73, loss = 0.04591233
Iteration 74, loss = 0.04491116
Iteration 75, loss = 0.04416404
Iteration 76, loss = 0.04333960
Iteration 77, loss = 0.04251396
Iteration 78, loss = 0.04162455
Iteration 79, loss = 0.04098471
Iteration 80, loss = 0.04018435
Iteration 81, loss = 0.03950611
Iteration 82, loss = 0.03875045
Iteration 83, loss = 0.03802389
Iteration 84, loss = 0.03758861
Iteration 85, loss = 0.03664491
Iteration 86, loss = 0.03598996
Iteration 87, loss = 0.03533958
Iteration 88, loss = 0.03483047
Iteration 89, loss = 0.03429501
Iteration 90, loss = 0.03363783
Iteration 91, loss = 0.03308559
Iteration 92, loss = 0.03247830
Iteration 93, loss = 0.03189686
Iteration 94, loss = 0.03137709
Iteration 95, loss = 0.03083270
Iteration 96, loss = 0.03034091
Iteration 97, loss = 0.02988605
Iteration 98, loss = 0.02930121
Iteration 99, loss = 0.02877005
Iteration 100, loss = 0.02831116
Iteration 101, loss = 0.02785378
Iteration 102, loss = 0.02747175
Iteration 103, loss = 0.02700485
Iteration 104, loss = 0.02651791
Iteration 105, loss = 0.02612450
Iteration 106, loss = 0.02569079
Iteration 107, loss = 0.02532482
Iteration 108, loss = 0.02483850
Iteration 109, loss = 0.02443444
Iteration 110, loss = 0.02406633
Iteration 111, loss = 0.02367192
Iteration 112, loss = 0.02334219
Iteration 113, loss = 0.02298865
Iteration 114, loss = 0.02257230
Iteration 115, loss = 0.02221417
Iteration 116, loss = 0.02188521
Iteration 117, loss = 0.02156542
Iteration 118, loss = 0.02126487
Iteration 119, loss = 0.02098118
Iteration 120, loss = 0.02061434
Iteration 121, loss = 0.02032417
Iteration 122, loss = 0.01993633
Iteration 123, loss = 0.01959996
Iteration 124, loss = 0.01936717
Iteration 125, loss = 0.01904379
Iteration 126, loss = 0.01877710
Iteration 127, loss = 0.01851054
Iteration 128, loss = 0.01829095
Iteration 129, loss = 0.01789760
Iteration 130, loss = 0.01772211
Iteration 131, loss = 0.01740925
Iteration 132, loss = 0.01713568
Iteration 133, loss = 0.01690859
Iteration 134, loss = 0.01664528
Iteration 135, loss = 0.01642983
Iteration 136, loss = 0.01616570
Iteration 137, loss = 0.01602187
Iteration 138, loss = 0.01573855
Iteration 139, loss = 0.01553725
Iteration 140, loss = 0.01525078
Iteration 141, loss = 0.01505499
Iteration 142, loss = 0.01483730
Iteration 143, loss = 0.01463943
Iteration 144, loss = 0.01440247
Iteration 145, loss = 0.01419270
Iteration 146, loss = 0.01400014
Iteration 147, loss = 0.01383309
Iteration 148, loss = 0.01360216
Iteration 149, loss = 0.01340764
Iteration 150, loss = 0.01321091
Iteration 151, loss = 0.01304450
Iteration 152, loss = 0.01283161
Iteration 153, loss = 0.01265355
Iteration 154, loss = 0.01254137
Iteration 155, loss = 0.01230492
Iteration 156, loss = 0.01215782
Iteration 157, loss = 0.01200037
Iteration 158, loss = 0.01183149
Iteration 159, loss = 0.01164852
Iteration 160, loss = 0.01149925
Iteration 161, loss = 0.01133733
Iteration 162, loss = 0.01122395
Iteration 163, loss = 0.01108703
Iteration 164, loss = 0.01088033
Iteration 165, loss = 0.01074541
Iteration 166, loss = 0.01059612
Iteration 167, loss = 0.01048033
Iteration 168, loss = 0.01037437
Iteration 169, loss = 0.01022413
Iteration 170, loss = 0.01008034
Iteration 171, loss = 0.00997068
Iteration 172, loss = 0.00982547
Iteration 173, loss = 0.00968926
Iteration 174, loss = 0.00956786
Iteration 175, loss = 0.00945707
Iteration 176, loss = 0.00936874
Iteration 177, loss = 0.00923563
Iteration 178, loss = 0.00911467
Iteration 179, loss = 0.00903379
Iteration 180, loss = 0.00891188
Iteration 181, loss = 0.00882916
Iteration 182, loss = 0.00872241
Iteration 183, loss = 0.00859901
Iteration 184, loss = 0.00850015
Iteration 185, loss = 0.00838258
Iteration 186, loss = 0.00829851
Iteration 187, loss = 0.00828842
Iteration 188, loss = 0.00814889
Iteration 189, loss = 0.00802356
Iteration 190, loss = 0.00791574
Iteration 191, loss = 0.00782552
Iteration 192, loss = 0.00776587
Iteration 193, loss = 0.00763745
Iteration 194, loss = 0.00757336
Iteration 195, loss = 0.00747249
Iteration 196, loss = 0.00738313
Iteration 197, loss = 0.00733203
Iteration 198, loss = 0.00721894
Iteration 199, loss = 0.00716442
Iteration 200, loss = 0.00706442
Number of features extracted:  20
Error rate: 11.72% ( 293/2500)
 - Class 1:  48, Class 2:  23, Class 3: 100, Class 4:  91, Class 5:  31
Iteration 1, loss = 1.42385391
Iteration 2, loss = 1.09370956
Iteration 3, loss = 0.86041945
Iteration 4, loss = 0.69000022
Iteration 5, loss = 0.56378704
Iteration 6, loss = 0.47080927
Iteration 7, loss = 0.40175796
Iteration 8, loss = 0.34944802
Iteration 9, loss = 0.30922118
Iteration 10, loss = 0.27785055
Iteration 11, loss = 0.25243964
Iteration 12, loss = 0.23247793
Iteration 13, loss = 0.21501554
Iteration 14, loss = 0.20070461
Iteration 15, loss = 0.18847116
Iteration 16, loss = 0.17783472
Iteration 17, loss = 0.16866227
Iteration 18, loss = 0.16037995
Iteration 19, loss = 0.15294311
Iteration 20, loss = 0.14654154
Iteration 21, loss = 0.14032876
Iteration 22, loss = 0.13487613
Iteration 23, loss = 0.12996634
Iteration 24, loss = 0.12504013
Iteration 25, loss = 0.12076174
Iteration 26, loss = 0.11673688
Iteration 27, loss = 0.11290888
Iteration 28, loss = 0.10923588
Iteration 29, loss = 0.10607181
Iteration 30, loss = 0.10284542
Iteration 31, loss = 0.09980062
Iteration 32, loss = 0.09679421
Iteration 33, loss = 0.09447199
Iteration 34, loss = 0.09174501
Iteration 35, loss = 0.08926325
Iteration 36, loss = 0.08668426
Iteration 37, loss = 0.08455040
Iteration 38, loss = 0.08228005
Iteration 39, loss = 0.08037734
Iteration 40, loss = 0.07835100
Iteration 41, loss = 0.07649054
Iteration 42, loss = 0.07496241
Iteration 43, loss = 0.07305067
Iteration 44, loss = 0.07127502
Iteration 45, loss = 0.06967115
Iteration 46, loss = 0.06813837
Iteration 47, loss = 0.06666168
Iteration 48, loss = 0.06525309
Iteration 49, loss = 0.06368182
Iteration 50, loss = 0.06238197
Iteration 51, loss = 0.06099074
Iteration 52, loss = 0.05968883
Iteration 53, loss = 0.05837812
Iteration 54, loss = 0.05727649
Iteration 55, loss = 0.05600400
Iteration 56, loss = 0.05494233
Iteration 57, loss = 0.05372255
Iteration 58, loss = 0.05284429
Iteration 59, loss = 0.05180097
Iteration 60, loss = 0.05063791
Iteration 61, loss = 0.04980758
Iteration 62, loss = 0.04877200
Iteration 63, loss = 0.04778230
Iteration 64, loss = 0.04681670
Iteration 65, loss = 0.04584764
Iteration 66, loss = 0.04489913
Iteration 67, loss = 0.04409876
Iteration 68, loss = 0.04326486
Iteration 69, loss = 0.04250165
Iteration 70, loss = 0.04170935
Iteration 71, loss = 0.04085817
Iteration 72, loss = 0.04010449
Iteration 73, loss = 0.03923840
Iteration 74, loss = 0.03855710
Iteration 75, loss = 0.03780663
Iteration 76, loss = 0.03719985
Iteration 77, loss = 0.03665307
Iteration 78, loss = 0.03582231
Iteration 79, loss = 0.03527060
Iteration 80, loss = 0.03454237
Iteration 81, loss = 0.03380501
Iteration 82, loss = 0.03324603
Iteration 83, loss = 0.03266214
Iteration 84, loss = 0.03203556
Iteration 85, loss = 0.03149054
Iteration 86, loss = 0.03094473
Iteration 87, loss = 0.03040948
Iteration 88, loss = 0.02986988
Iteration 89, loss = 0.02929350
Iteration 90, loss = 0.02863660
Iteration 91, loss = 0.02822147
Iteration 92, loss = 0.02765569
Iteration 93, loss = 0.02715127
Iteration 94, loss = 0.02665114
Iteration 95, loss = 0.02621125
Iteration 96, loss = 0.02575539
Iteration 97, loss = 0.02533435
Iteration 98, loss = 0.02485958
Iteration 99, loss = 0.02433304
Iteration 100, loss = 0.02383805
Iteration 101, loss = 0.02343328
Iteration 102, loss = 0.02306014
Iteration 103, loss = 0.02260427
Iteration 104, loss = 0.02221017
Iteration 105, loss = 0.02187556
Iteration 106, loss = 0.02152990
Iteration 107, loss = 0.02110122
Iteration 108, loss = 0.02062363
Iteration 109, loss = 0.02031784
Iteration 110, loss = 0.02001024
Iteration 111, loss = 0.01966740
Iteration 112, loss = 0.01932416
Iteration 113, loss = 0.01897928
Iteration 114, loss = 0.01873778
Iteration 115, loss = 0.01828889
Iteration 116, loss = 0.01811453
Iteration 117, loss = 0.01774311
Iteration 118, loss = 0.01734310
Iteration 119, loss = 0.01707547
Iteration 120, loss = 0.01681461
Iteration 121, loss = 0.01642427
Iteration 122, loss = 0.01615305
Iteration 123, loss = 0.01584988
Iteration 124, loss = 0.01559368
Iteration 125, loss = 0.01545072
Iteration 126, loss = 0.01504937
Iteration 127, loss = 0.01480821
Iteration 128, loss = 0.01459993
Iteration 129, loss = 0.01430121
Iteration 130, loss = 0.01405490
Iteration 131, loss = 0.01384940
Iteration 132, loss = 0.01365605
Iteration 133, loss = 0.01335436
Iteration 134, loss = 0.01312273
Iteration 135, loss = 0.01289487
Iteration 136, loss = 0.01265748
Iteration 137, loss = 0.01240838
Iteration 138, loss = 0.01226716
Iteration 139, loss = 0.01201378
Iteration 140, loss = 0.01180693
Iteration 141, loss = 0.01163193
Iteration 142, loss = 0.01143293
Iteration 143, loss = 0.01125503
Iteration 144, loss = 0.01102732
Iteration 145, loss = 0.01085508
Iteration 146, loss = 0.01067128
Iteration 147, loss = 0.01050662
Iteration 148, loss = 0.01034898
Iteration 149, loss = 0.01013628
Iteration 150, loss = 0.01000352
Iteration 151, loss = 0.00978847
Iteration 152, loss = 0.00966664
Iteration 153, loss = 0.00948709
Iteration 154, loss = 0.00934836
Iteration 155, loss = 0.00927101
Iteration 156, loss = 0.00911681
Iteration 157, loss = 0.00894253
Iteration 158, loss = 0.00879804
Iteration 159, loss = 0.00863424
Iteration 160, loss = 0.00851372
Iteration 161, loss = 0.00834365
Iteration 162, loss = 0.00826670
Iteration 163, loss = 0.00811018
Iteration 164, loss = 0.00800276
Iteration 165, loss = 0.00788335
Iteration 166, loss = 0.00773929
Iteration 167, loss = 0.00762425
Iteration 168, loss = 0.00753382
Iteration 169, loss = 0.00738341
Iteration 170, loss = 0.00728302
Iteration 171, loss = 0.00720169
Iteration 172, loss = 0.00708734
Iteration 173, loss = 0.00694028
Iteration 174, loss = 0.00685612
Iteration 175, loss = 0.00674706
Iteration 176, loss = 0.00664314
Iteration 177, loss = 0.00654518
Iteration 178, loss = 0.00643957
Iteration 179, loss = 0.00633953
Iteration 180, loss = 0.00626293
Iteration 181, loss = 0.00617157
Iteration 182, loss = 0.00612643
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  25
Error rate: 13.08% ( 327/2500)
 - Class 1:   7, Class 2:  19, Class 3: 106, Class 4:  77, Class 5: 118
Iteration 1, loss = 1.42171558
Iteration 2, loss = 1.06192756
Iteration 3, loss = 0.82734297
Iteration 4, loss = 0.66091684
Iteration 5, loss = 0.53909475
Iteration 6, loss = 0.44897605
Iteration 7, loss = 0.38166275
Iteration 8, loss = 0.33044775
Iteration 9, loss = 0.29110501
Iteration 10, loss = 0.26015185
Iteration 11, loss = 0.23547944
Iteration 12, loss = 0.21515966
Iteration 13, loss = 0.19869989
Iteration 14, loss = 0.18442847
Iteration 15, loss = 0.17231175
Iteration 16, loss = 0.16212841
Iteration 17, loss = 0.15296374
Iteration 18, loss = 0.14538057
Iteration 19, loss = 0.13801461
Iteration 20, loss = 0.13202965
Iteration 21, loss = 0.12616860
Iteration 22, loss = 0.12097353
Iteration 23, loss = 0.11611644
Iteration 24, loss = 0.11172214
Iteration 25, loss = 0.10785160
Iteration 26, loss = 0.10414436
Iteration 27, loss = 0.10085163
Iteration 28, loss = 0.09712654
Iteration 29, loss = 0.09405269
Iteration 30, loss = 0.09111475
Iteration 31, loss = 0.08840383
Iteration 32, loss = 0.08598270
Iteration 33, loss = 0.08318097
Iteration 34, loss = 0.08101135
Iteration 35, loss = 0.07866704
Iteration 36, loss = 0.07661120
Iteration 37, loss = 0.07451577
Iteration 38, loss = 0.07258047
Iteration 39, loss = 0.07062751
Iteration 40, loss = 0.06875949
Iteration 41, loss = 0.06689279
Iteration 42, loss = 0.06529428
Iteration 43, loss = 0.06375586
Iteration 44, loss = 0.06224092
Iteration 45, loss = 0.06072756
Iteration 46, loss = 0.05913542
Iteration 47, loss = 0.05781193
Iteration 48, loss = 0.05647858
Iteration 49, loss = 0.05542569
Iteration 50, loss = 0.05369026
Iteration 51, loss = 0.05231957
Iteration 52, loss = 0.05134364
Iteration 53, loss = 0.05016008
Iteration 54, loss = 0.04898035
Iteration 55, loss = 0.04783831
Iteration 56, loss = 0.04670666
Iteration 57, loss = 0.04581639
Iteration 58, loss = 0.04475785
Iteration 59, loss = 0.04404949
Iteration 60, loss = 0.04307815
Iteration 61, loss = 0.04199425
Iteration 62, loss = 0.04108176
Iteration 63, loss = 0.04014799
Iteration 64, loss = 0.03934522
Iteration 65, loss = 0.03857503
Iteration 66, loss = 0.03776372
Iteration 67, loss = 0.03693400
Iteration 68, loss = 0.03617773
Iteration 69, loss = 0.03545200
Iteration 70, loss = 0.03464039
Iteration 71, loss = 0.03392121
Iteration 72, loss = 0.03321095
Iteration 73, loss = 0.03261249
Iteration 74, loss = 0.03226885
Iteration 75, loss = 0.03129950
Iteration 76, loss = 0.03056482
Iteration 77, loss = 0.03014830
Iteration 78, loss = 0.02932413
Iteration 79, loss = 0.02893957
Iteration 80, loss = 0.02832933
Iteration 81, loss = 0.02777587
Iteration 82, loss = 0.02721502
Iteration 83, loss = 0.02659778
Iteration 84, loss = 0.02608446
Iteration 85, loss = 0.02562680
Iteration 86, loss = 0.02511041
Iteration 87, loss = 0.02457558
Iteration 88, loss = 0.02414760
Iteration 89, loss = 0.02362311
Iteration 90, loss = 0.02324477
Iteration 91, loss = 0.02290393
Iteration 92, loss = 0.02235138
Iteration 93, loss = 0.02199758
Iteration 94, loss = 0.02152689
Iteration 95, loss = 0.02101273
Iteration 96, loss = 0.02070350
Iteration 97, loss = 0.02033624
Iteration 98, loss = 0.01989061
Iteration 99, loss = 0.01949684
Iteration 100, loss = 0.01914904
Iteration 101, loss = 0.01875341
Iteration 102, loss = 0.01856731
Iteration 103, loss = 0.01809043
Iteration 104, loss = 0.01779831
Iteration 105, loss = 0.01744164
Iteration 106, loss = 0.01704252
Iteration 107, loss = 0.01672978
Iteration 108, loss = 0.01639100
Iteration 109, loss = 0.01611328
Iteration 110, loss = 0.01581367
Iteration 111, loss = 0.01553276
Iteration 112, loss = 0.01520311
Iteration 113, loss = 0.01491149
Iteration 114, loss = 0.01470928
Iteration 115, loss = 0.01441883
Iteration 116, loss = 0.01412381
Iteration 117, loss = 0.01386727
Iteration 118, loss = 0.01369234
Iteration 119, loss = 0.01332342
Iteration 120, loss = 0.01311567
Iteration 121, loss = 0.01287550
Iteration 122, loss = 0.01262376
Iteration 123, loss = 0.01243875
Iteration 124, loss = 0.01221223
Iteration 125, loss = 0.01199286
Iteration 126, loss = 0.01182296
Iteration 127, loss = 0.01153589
Iteration 128, loss = 0.01133936
Iteration 129, loss = 0.01126504
Iteration 130, loss = 0.01100820
Iteration 131, loss = 0.01083507
Iteration 132, loss = 0.01071483
Iteration 133, loss = 0.01043728
Iteration 134, loss = 0.01026554
Iteration 135, loss = 0.01005468
Iteration 136, loss = 0.00994731
Iteration 137, loss = 0.00972162
Iteration 138, loss = 0.00952379
Iteration 139, loss = 0.00940728
Iteration 140, loss = 0.00924301
Iteration 141, loss = 0.00910466
Iteration 142, loss = 0.00894395
Iteration 143, loss = 0.00880540
Iteration 144, loss = 0.00866202
Iteration 145, loss = 0.00855795
Iteration 146, loss = 0.00841868
Iteration 147, loss = 0.00826009
Iteration 148, loss = 0.00816716
Iteration 149, loss = 0.00800262
Iteration 150, loss = 0.00787641
Iteration 151, loss = 0.00772050
Iteration 152, loss = 0.00758830
Iteration 153, loss = 0.00748903
Iteration 154, loss = 0.00737062
Iteration 155, loss = 0.00723783
Iteration 156, loss = 0.00715192
Iteration 157, loss = 0.00704691
Iteration 158, loss = 0.00696740
Iteration 159, loss = 0.00679930
Iteration 160, loss = 0.00673559
Iteration 161, loss = 0.00662386
Iteration 162, loss = 0.00651422
Iteration 163, loss = 0.00639284
Iteration 164, loss = 0.00631074
Iteration 165, loss = 0.00622401
Iteration 166, loss = 0.00613059
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  30
Error rate: 23.40% ( 585/2500)
 - Class 1:  30, Class 2:  16, Class 3: 499, Class 4:  20, Class 5:  20
Iteration 1, loss = 1.36878442
Iteration 2, loss = 0.99274844
Iteration 3, loss = 0.74344225
Iteration 4, loss = 0.57001473
Iteration 5, loss = 0.45116662
Iteration 6, loss = 0.37014867
Iteration 7, loss = 0.31288912
Iteration 8, loss = 0.27195851
Iteration 9, loss = 0.24086082
Iteration 10, loss = 0.21702593
Iteration 11, loss = 0.19821947
Iteration 12, loss = 0.18255383
Iteration 13, loss = 0.16976662
Iteration 14, loss = 0.15921567
Iteration 15, loss = 0.15001093
Iteration 16, loss = 0.14184439
Iteration 17, loss = 0.13500330
Iteration 18, loss = 0.12868720
Iteration 19, loss = 0.12305527
Iteration 20, loss = 0.11811941
Iteration 21, loss = 0.11340912
Iteration 22, loss = 0.10897578
Iteration 23, loss = 0.10500123
Iteration 24, loss = 0.10136828
Iteration 25, loss = 0.09789841
Iteration 26, loss = 0.09489403
Iteration 27, loss = 0.09193629
Iteration 28, loss = 0.08874479
Iteration 29, loss = 0.08616899
Iteration 30, loss = 0.08377640
Iteration 31, loss = 0.08151493
Iteration 32, loss = 0.07880423
Iteration 33, loss = 0.07630540
Iteration 34, loss = 0.07428312
Iteration 35, loss = 0.07243220
Iteration 36, loss = 0.07045696
Iteration 37, loss = 0.06837047
Iteration 38, loss = 0.06646556
Iteration 39, loss = 0.06477826
Iteration 40, loss = 0.06352677
Iteration 41, loss = 0.06163347
Iteration 42, loss = 0.06029673
Iteration 43, loss = 0.05853464
Iteration 44, loss = 0.05716928
Iteration 45, loss = 0.05570938
Iteration 46, loss = 0.05426513
Iteration 47, loss = 0.05302240
Iteration 48, loss = 0.05171786
Iteration 49, loss = 0.05044893
Iteration 50, loss = 0.04934290
Iteration 51, loss = 0.04807888
Iteration 52, loss = 0.04717099
Iteration 53, loss = 0.04611690
Iteration 54, loss = 0.04497167
Iteration 55, loss = 0.04396602
Iteration 56, loss = 0.04321754
Iteration 57, loss = 0.04201512
Iteration 58, loss = 0.04096662
Iteration 59, loss = 0.04017075
Iteration 60, loss = 0.03923181
Iteration 61, loss = 0.03826007
Iteration 62, loss = 0.03737913
Iteration 63, loss = 0.03672046
Iteration 64, loss = 0.03592845
Iteration 65, loss = 0.03508408
Iteration 66, loss = 0.03428565
Iteration 67, loss = 0.03369355
Iteration 68, loss = 0.03280199
Iteration 69, loss = 0.03208820
Iteration 70, loss = 0.03138047
Iteration 71, loss = 0.03070990
Iteration 72, loss = 0.03014713
Iteration 73, loss = 0.02945123
Iteration 74, loss = 0.02874602
Iteration 75, loss = 0.02814469
Iteration 76, loss = 0.02783951
Iteration 77, loss = 0.02701346
Iteration 78, loss = 0.02646748
Iteration 79, loss = 0.02600234
Iteration 80, loss = 0.02533190
Iteration 81, loss = 0.02471301
Iteration 82, loss = 0.02430713
Iteration 83, loss = 0.02377125
Iteration 84, loss = 0.02333521
Iteration 85, loss = 0.02289436
Iteration 86, loss = 0.02248476
Iteration 87, loss = 0.02187339
Iteration 88, loss = 0.02150998
Iteration 89, loss = 0.02121205
Iteration 90, loss = 0.02064087
Iteration 91, loss = 0.02025181
Iteration 92, loss = 0.01977477
Iteration 93, loss = 0.01934847
Iteration 94, loss = 0.01893258
Iteration 95, loss = 0.01853275
Iteration 96, loss = 0.01815481
Iteration 97, loss = 0.01779139
Iteration 98, loss = 0.01743719
Iteration 99, loss = 0.01718588
Iteration 100, loss = 0.01668679
Iteration 101, loss = 0.01642528
Iteration 102, loss = 0.01616023
Iteration 103, loss = 0.01579615
Iteration 104, loss = 0.01551262
Iteration 105, loss = 0.01510015
Iteration 106, loss = 0.01487139
Iteration 107, loss = 0.01450726
Iteration 108, loss = 0.01421324
Iteration 109, loss = 0.01391726
Iteration 110, loss = 0.01365476
Iteration 111, loss = 0.01336006
Iteration 112, loss = 0.01311470
Iteration 113, loss = 0.01288615
Iteration 114, loss = 0.01265524
Iteration 115, loss = 0.01240409
Iteration 116, loss = 0.01210019
Iteration 117, loss = 0.01192040
Iteration 118, loss = 0.01162843
Iteration 119, loss = 0.01146135
Iteration 120, loss = 0.01116558
Iteration 121, loss = 0.01095789
Iteration 122, loss = 0.01077570
Iteration 123, loss = 0.01058183
Iteration 124, loss = 0.01041838
Iteration 125, loss = 0.01012625
Iteration 126, loss = 0.00994392
Iteration 127, loss = 0.00975743
Iteration 128, loss = 0.00961989
Iteration 129, loss = 0.00943783
Iteration 130, loss = 0.00923763
Iteration 131, loss = 0.00902642
Iteration 132, loss = 0.00888004
Iteration 133, loss = 0.00867702
Iteration 134, loss = 0.00850106
Iteration 135, loss = 0.00833285
Iteration 136, loss = 0.00817839
Iteration 137, loss = 0.00802186
Iteration 138, loss = 0.00787637
Iteration 139, loss = 0.00772834
Iteration 140, loss = 0.00757178
Iteration 141, loss = 0.00744903
Iteration 142, loss = 0.00729811
Iteration 143, loss = 0.00715999
Iteration 144, loss = 0.00703758
Iteration 145, loss = 0.00693425
Iteration 146, loss = 0.00681635
Iteration 147, loss = 0.00667538
Iteration 148, loss = 0.00657694
Iteration 149, loss = 0.00643023
Iteration 150, loss = 0.00631398
Iteration 151, loss = 0.00621015
Iteration 152, loss = 0.00612665
Iteration 153, loss = 0.00597630
Iteration 154, loss = 0.00590737
Iteration 155, loss = 0.00579973
Iteration 156, loss = 0.00569308
Iteration 157, loss = 0.00562760
Iteration 158, loss = 0.00550133
Iteration 159, loss = 0.00541037
Iteration 160, loss = 0.00531241
Iteration 161, loss = 0.00522944
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  35
Error rate: 24.72% ( 618/2500)
 - Class 1:  31, Class 2:  20, Class 3: 496, Class 4:  26, Class 5:  45
Iteration 1, loss = 1.45497451
Iteration 2, loss = 1.05258794
Iteration 3, loss = 0.79074045
Iteration 4, loss = 0.60760803
Iteration 5, loss = 0.47995972
Iteration 6, loss = 0.39153556
Iteration 7, loss = 0.32883708
Iteration 8, loss = 0.28357613
Iteration 9, loss = 0.24996323
Iteration 10, loss = 0.22427239
Iteration 11, loss = 0.20403933
Iteration 12, loss = 0.18734235
Iteration 13, loss = 0.17397783
Iteration 14, loss = 0.16256199
Iteration 15, loss = 0.15270477
Iteration 16, loss = 0.14440270
Iteration 17, loss = 0.13732420
Iteration 18, loss = 0.13023857
Iteration 19, loss = 0.12445747
Iteration 20, loss = 0.11906143
Iteration 21, loss = 0.11427222
Iteration 22, loss = 0.10959956
Iteration 23, loss = 0.10563986
Iteration 24, loss = 0.10206584
Iteration 25, loss = 0.09849659
Iteration 26, loss = 0.09499435
Iteration 27, loss = 0.09192464
Iteration 28, loss = 0.08883028
Iteration 29, loss = 0.08620654
Iteration 30, loss = 0.08363522
Iteration 31, loss = 0.08133138
Iteration 32, loss = 0.07884587
Iteration 33, loss = 0.07642374
Iteration 34, loss = 0.07432980
Iteration 35, loss = 0.07202932
Iteration 36, loss = 0.07033702
Iteration 37, loss = 0.06811410
Iteration 38, loss = 0.06614602
Iteration 39, loss = 0.06461439
Iteration 40, loss = 0.06278405
Iteration 41, loss = 0.06126629
Iteration 42, loss = 0.05969878
Iteration 43, loss = 0.05805462
Iteration 44, loss = 0.05644955
Iteration 45, loss = 0.05527144
Iteration 46, loss = 0.05376856
Iteration 47, loss = 0.05236268
Iteration 48, loss = 0.05102808
Iteration 49, loss = 0.05004697
Iteration 50, loss = 0.04865605
Iteration 51, loss = 0.04747486
Iteration 52, loss = 0.04628269
Iteration 53, loss = 0.04521649
Iteration 54, loss = 0.04416857
Iteration 55, loss = 0.04327685
Iteration 56, loss = 0.04220166
Iteration 57, loss = 0.04118313
Iteration 58, loss = 0.04020272
Iteration 59, loss = 0.03925710
Iteration 60, loss = 0.03842701
Iteration 61, loss = 0.03752510
Iteration 62, loss = 0.03680007
Iteration 63, loss = 0.03584705
Iteration 64, loss = 0.03499605
Iteration 65, loss = 0.03414646
Iteration 66, loss = 0.03341152
Iteration 67, loss = 0.03276228
Iteration 68, loss = 0.03193442
Iteration 69, loss = 0.03120513
Iteration 70, loss = 0.03056383
Iteration 71, loss = 0.02992096
Iteration 72, loss = 0.02930411
Iteration 73, loss = 0.02865762
Iteration 74, loss = 0.02798108
Iteration 75, loss = 0.02747470
Iteration 76, loss = 0.02695005
Iteration 77, loss = 0.02638482
Iteration 78, loss = 0.02567014
Iteration 79, loss = 0.02504121
Iteration 80, loss = 0.02459718
Iteration 81, loss = 0.02404442
Iteration 82, loss = 0.02361367
Iteration 83, loss = 0.02292589
Iteration 84, loss = 0.02249185
Iteration 85, loss = 0.02204760
Iteration 86, loss = 0.02152570
Iteration 87, loss = 0.02105743
Iteration 88, loss = 0.02063799
Iteration 89, loss = 0.02025069
Iteration 90, loss = 0.01977405
Iteration 91, loss = 0.01931629
Iteration 92, loss = 0.01890712
Iteration 93, loss = 0.01848723
Iteration 94, loss = 0.01812564
Iteration 95, loss = 0.01771403
Iteration 96, loss = 0.01732624
Iteration 97, loss = 0.01699341
Iteration 98, loss = 0.01662125
Iteration 99, loss = 0.01631204
Iteration 100, loss = 0.01582846
Iteration 101, loss = 0.01559533
Iteration 102, loss = 0.01522889
Iteration 103, loss = 0.01494666
Iteration 104, loss = 0.01461761
Iteration 105, loss = 0.01430242
Iteration 106, loss = 0.01403707
Iteration 107, loss = 0.01373747
Iteration 108, loss = 0.01369799
Iteration 109, loss = 0.01319154
Iteration 110, loss = 0.01293475
Iteration 111, loss = 0.01267396
Iteration 112, loss = 0.01236335
Iteration 113, loss = 0.01215692
Iteration 114, loss = 0.01192283
Iteration 115, loss = 0.01169367
Iteration 116, loss = 0.01143657
Iteration 117, loss = 0.01117857
Iteration 118, loss = 0.01100720
Iteration 119, loss = 0.01078434
Iteration 120, loss = 0.01058150
Iteration 121, loss = 0.01036344
Iteration 122, loss = 0.01014802
Iteration 123, loss = 0.01004130
Iteration 124, loss = 0.00980185
Iteration 125, loss = 0.00963631
Iteration 126, loss = 0.00943517
Iteration 127, loss = 0.00923886
Iteration 128, loss = 0.00907871
Iteration 129, loss = 0.00896990
Iteration 130, loss = 0.00872107
Iteration 131, loss = 0.00854802
Iteration 132, loss = 0.00840327
Iteration 133, loss = 0.00828996
Iteration 134, loss = 0.00807941
Iteration 135, loss = 0.00793271
Iteration 136, loss = 0.00782448
Iteration 137, loss = 0.00764654
Iteration 138, loss = 0.00750850
Iteration 139, loss = 0.00737111
Iteration 140, loss = 0.00724475
Iteration 141, loss = 0.00710492
Iteration 142, loss = 0.00704571
Iteration 143, loss = 0.00686140
Iteration 144, loss = 0.00672051
Iteration 145, loss = 0.00659956
Iteration 146, loss = 0.00648072
Iteration 147, loss = 0.00639037
Iteration 148, loss = 0.00623906
Iteration 149, loss = 0.00617412
Iteration 150, loss = 0.00606616
Iteration 151, loss = 0.00597747
Iteration 152, loss = 0.00586335
Iteration 153, loss = 0.00573895
Iteration 154, loss = 0.00563917
Iteration 155, loss = 0.00556652
Iteration 156, loss = 0.00545584
Iteration 157, loss = 0.00537177
Iteration 158, loss = 0.00526451
Iteration 159, loss = 0.00518542
Iteration 160, loss = 0.00510358
Iteration 161, loss = 0.00501908
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  40
Error rate: 24.60% ( 615/2500)
 - Class 1:  32, Class 2:  18, Class 3: 487, Class 4:  45, Class 5:  33
Iteration 1, loss = 1.35449135
Iteration 2, loss = 0.93911864
Iteration 3, loss = 0.68110869
Iteration 4, loss = 0.51038964
Iteration 5, loss = 0.39941956
Iteration 6, loss = 0.32581674
Iteration 7, loss = 0.27534964
Iteration 8, loss = 0.23955568
Iteration 9, loss = 0.21292041
Iteration 10, loss = 0.19277121
Iteration 11, loss = 0.17622056
Iteration 12, loss = 0.16303257
Iteration 13, loss = 0.15225191
Iteration 14, loss = 0.14298307
Iteration 15, loss = 0.13510966
Iteration 16, loss = 0.12817319
Iteration 17, loss = 0.12177497
Iteration 18, loss = 0.11612919
Iteration 19, loss = 0.11159476
Iteration 20, loss = 0.10678652
Iteration 21, loss = 0.10254198
Iteration 22, loss = 0.09869324
Iteration 23, loss = 0.09494688
Iteration 24, loss = 0.09168940
Iteration 25, loss = 0.08847953
Iteration 26, loss = 0.08579044
Iteration 27, loss = 0.08288608
Iteration 28, loss = 0.08046122
Iteration 29, loss = 0.07793096
Iteration 30, loss = 0.07547905
Iteration 31, loss = 0.07302330
Iteration 32, loss = 0.07082136
Iteration 33, loss = 0.06913984
Iteration 34, loss = 0.06684838
Iteration 35, loss = 0.06468120
Iteration 36, loss = 0.06294134
Iteration 37, loss = 0.06143378
Iteration 38, loss = 0.05976417
Iteration 39, loss = 0.05788219
Iteration 40, loss = 0.05663445
Iteration 41, loss = 0.05488584
Iteration 42, loss = 0.05330719
Iteration 43, loss = 0.05210697
Iteration 44, loss = 0.05077959
Iteration 45, loss = 0.04934446
Iteration 46, loss = 0.04820564
Iteration 47, loss = 0.04698002
Iteration 48, loss = 0.04558545
Iteration 49, loss = 0.04452861
Iteration 50, loss = 0.04340701
Iteration 51, loss = 0.04245586
Iteration 52, loss = 0.04148115
Iteration 53, loss = 0.04046872
Iteration 54, loss = 0.03922575
Iteration 55, loss = 0.03852282
Iteration 56, loss = 0.03744354
Iteration 57, loss = 0.03660381
Iteration 58, loss = 0.03568080
Iteration 59, loss = 0.03476490
Iteration 60, loss = 0.03394716
Iteration 61, loss = 0.03316108
Iteration 62, loss = 0.03225935
Iteration 63, loss = 0.03168239
Iteration 64, loss = 0.03098902
Iteration 65, loss = 0.03021295
Iteration 66, loss = 0.02943061
Iteration 67, loss = 0.02874499
Iteration 68, loss = 0.02808276
Iteration 69, loss = 0.02740385
Iteration 70, loss = 0.02688649
Iteration 71, loss = 0.02616717
Iteration 72, loss = 0.02564585
Iteration 73, loss = 0.02502304
Iteration 74, loss = 0.02445334
Iteration 75, loss = 0.02390457
Iteration 76, loss = 0.02331620
Iteration 77, loss = 0.02279959
Iteration 78, loss = 0.02222010
Iteration 79, loss = 0.02183453
Iteration 80, loss = 0.02120210
Iteration 81, loss = 0.02095083
Iteration 82, loss = 0.02033595
Iteration 83, loss = 0.01983104
Iteration 84, loss = 0.01944634
Iteration 85, loss = 0.01902465
Iteration 86, loss = 0.01857056
Iteration 87, loss = 0.01814537
Iteration 88, loss = 0.01772973
Iteration 89, loss = 0.01734813
Iteration 90, loss = 0.01701028
Iteration 91, loss = 0.01654292
Iteration 92, loss = 0.01620205
Iteration 93, loss = 0.01583813
Iteration 94, loss = 0.01548105
Iteration 95, loss = 0.01516297
Iteration 96, loss = 0.01482972
Iteration 97, loss = 0.01454966
Iteration 98, loss = 0.01416120
Iteration 99, loss = 0.01391692
Iteration 100, loss = 0.01362055
Iteration 101, loss = 0.01331428
Iteration 102, loss = 0.01306493
Iteration 103, loss = 0.01274294
Iteration 104, loss = 0.01251358
Iteration 105, loss = 0.01217810
Iteration 106, loss = 0.01194264
Iteration 107, loss = 0.01165333
Iteration 108, loss = 0.01145512
Iteration 109, loss = 0.01123730
Iteration 110, loss = 0.01095919
Iteration 111, loss = 0.01074618
Iteration 112, loss = 0.01052616
Iteration 113, loss = 0.01028970
Iteration 114, loss = 0.01006455
Iteration 115, loss = 0.00987616
Iteration 116, loss = 0.00967909
Iteration 117, loss = 0.00952072
Iteration 118, loss = 0.00929821
Iteration 119, loss = 0.00918515
Iteration 120, loss = 0.00898257
Iteration 121, loss = 0.00875491
Iteration 122, loss = 0.00861774
Iteration 123, loss = 0.00841527
Iteration 124, loss = 0.00823366
Iteration 125, loss = 0.00808663
Iteration 126, loss = 0.00791242
Iteration 127, loss = 0.00776556
Iteration 128, loss = 0.00763331
Iteration 129, loss = 0.00748483
Iteration 130, loss = 0.00738568
Iteration 131, loss = 0.00721820
Iteration 132, loss = 0.00705298
Iteration 133, loss = 0.00695151
Iteration 134, loss = 0.00679988
Iteration 135, loss = 0.00670322
Iteration 136, loss = 0.00653623
Iteration 137, loss = 0.00644237
Iteration 138, loss = 0.00634919
Iteration 139, loss = 0.00623405
Iteration 140, loss = 0.00611516
Iteration 141, loss = 0.00599681
Iteration 142, loss = 0.00585892
Iteration 143, loss = 0.00575170
Iteration 144, loss = 0.00566555
Iteration 145, loss = 0.00554586
Iteration 146, loss = 0.00543128
Iteration 147, loss = 0.00529347
Iteration 148, loss = 0.00518802
Iteration 149, loss = 0.00510116
Iteration 150, loss = 0.00502421
Iteration 151, loss = 0.00491897
Iteration 152, loss = 0.00482188
Iteration 153, loss = 0.00473075
Iteration 154, loss = 0.00466749
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  45
Error rate: 24.88% ( 622/2500)
 - Class 1:  45, Class 2:  16, Class 3: 500, Class 4:  32, Class 5:  29
Iteration 1, loss = 1.32978796
Iteration 2, loss = 0.89244675
Iteration 3, loss = 0.62389057
Iteration 4, loss = 0.45903342
Iteration 5, loss = 0.35781209
Iteration 6, loss = 0.29358166
Iteration 7, loss = 0.24987544
Iteration 8, loss = 0.21894303
Iteration 9, loss = 0.19568577
Iteration 10, loss = 0.17820334
Iteration 11, loss = 0.16369739
Iteration 12, loss = 0.15201854
Iteration 13, loss = 0.14219155
Iteration 14, loss = 0.13390636
Iteration 15, loss = 0.12649882
Iteration 16, loss = 0.12042443
Iteration 17, loss = 0.11518838
Iteration 18, loss = 0.10994800
Iteration 19, loss = 0.10527149
Iteration 20, loss = 0.10086085
Iteration 21, loss = 0.09711778
Iteration 22, loss = 0.09350364
Iteration 23, loss = 0.09083852
Iteration 24, loss = 0.08699578
Iteration 25, loss = 0.08373966
Iteration 26, loss = 0.08113821
Iteration 27, loss = 0.07835259
Iteration 28, loss = 0.07576553
Iteration 29, loss = 0.07339794
Iteration 30, loss = 0.07120700
Iteration 31, loss = 0.06919272
Iteration 32, loss = 0.06677631
Iteration 33, loss = 0.06486108
Iteration 34, loss = 0.06325773
Iteration 35, loss = 0.06111242
Iteration 36, loss = 0.05932817
Iteration 37, loss = 0.05747163
Iteration 38, loss = 0.05605727
Iteration 39, loss = 0.05435430
Iteration 40, loss = 0.05269048
Iteration 41, loss = 0.05150527
Iteration 42, loss = 0.05026971
Iteration 43, loss = 0.04878581
Iteration 44, loss = 0.04732552
Iteration 45, loss = 0.04604270
Iteration 46, loss = 0.04505333
Iteration 47, loss = 0.04373216
Iteration 48, loss = 0.04229859
Iteration 49, loss = 0.04130565
Iteration 50, loss = 0.04042344
Iteration 51, loss = 0.03909284
Iteration 52, loss = 0.03806492
Iteration 53, loss = 0.03737288
Iteration 54, loss = 0.03627421
Iteration 55, loss = 0.03519697
Iteration 56, loss = 0.03435929
Iteration 57, loss = 0.03362314
Iteration 58, loss = 0.03275886
Iteration 59, loss = 0.03188281
Iteration 60, loss = 0.03111583
Iteration 61, loss = 0.03065644
Iteration 62, loss = 0.02954967
Iteration 63, loss = 0.02891958
Iteration 64, loss = 0.02809368
Iteration 65, loss = 0.02746283
Iteration 66, loss = 0.02669160
Iteration 67, loss = 0.02610455
Iteration 68, loss = 0.02546310
Iteration 69, loss = 0.02490422
Iteration 70, loss = 0.02428697
Iteration 71, loss = 0.02369746
Iteration 72, loss = 0.02325873
Iteration 73, loss = 0.02251780
Iteration 74, loss = 0.02210135
Iteration 75, loss = 0.02148334
Iteration 76, loss = 0.02112338
Iteration 77, loss = 0.02050633
Iteration 78, loss = 0.02006197
Iteration 79, loss = 0.01960262
Iteration 80, loss = 0.01907782
Iteration 81, loss = 0.01858538
Iteration 82, loss = 0.01814374
Iteration 83, loss = 0.01781710
Iteration 84, loss = 0.01738178
Iteration 85, loss = 0.01689817
Iteration 86, loss = 0.01648551
Iteration 87, loss = 0.01605776
Iteration 88, loss = 0.01569186
Iteration 89, loss = 0.01538382
Iteration 90, loss = 0.01510062
Iteration 91, loss = 0.01470343
Iteration 92, loss = 0.01442614
Iteration 93, loss = 0.01402467
Iteration 94, loss = 0.01370202
Iteration 95, loss = 0.01332071
Iteration 96, loss = 0.01298280
Iteration 97, loss = 0.01277694
Iteration 98, loss = 0.01248797
Iteration 99, loss = 0.01213734
Iteration 100, loss = 0.01187836
Iteration 101, loss = 0.01160556
Iteration 102, loss = 0.01133022
Iteration 103, loss = 0.01104783
Iteration 104, loss = 0.01081426
Iteration 105, loss = 0.01050148
Iteration 106, loss = 0.01025342
Iteration 107, loss = 0.01009609
Iteration 108, loss = 0.00982691
Iteration 109, loss = 0.00959788
Iteration 110, loss = 0.00937893
Iteration 111, loss = 0.00916805
Iteration 112, loss = 0.00898983
Iteration 113, loss = 0.00878679
Iteration 114, loss = 0.00857546
Iteration 115, loss = 0.00841770
Iteration 116, loss = 0.00823799
Iteration 117, loss = 0.00804398
Iteration 118, loss = 0.00783509
Iteration 119, loss = 0.00767468
Iteration 120, loss = 0.00751786
Iteration 121, loss = 0.00736138
Iteration 122, loss = 0.00718715
Iteration 123, loss = 0.00705560
Iteration 124, loss = 0.00693563
Iteration 125, loss = 0.00674188
Iteration 126, loss = 0.00663149
Iteration 127, loss = 0.00649561
Iteration 128, loss = 0.00637244
Iteration 129, loss = 0.00620639
Iteration 130, loss = 0.00609638
Iteration 131, loss = 0.00595017
Iteration 132, loss = 0.00583650
Iteration 133, loss = 0.00578046
Iteration 134, loss = 0.00565274
Iteration 135, loss = 0.00550146
Iteration 136, loss = 0.00542844
Iteration 137, loss = 0.00531876
Iteration 138, loss = 0.00521185
Iteration 139, loss = 0.00510787
Iteration 140, loss = 0.00500822
Iteration 141, loss = 0.00493604
Iteration 142, loss = 0.00483780
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  50
Error rate: 25.52% ( 638/2500)
 - Class 1:  44, Class 2:  37, Class 3: 491, Class 4:  41, Class 5:  25
Iteration 1, loss = 1.30465200
Iteration 2, loss = 0.84318319
Iteration 3, loss = 0.57508644
Iteration 4, loss = 0.42025886
Iteration 5, loss = 0.32814349
Iteration 6, loss = 0.27006336
Iteration 7, loss = 0.23086000
Iteration 8, loss = 0.20295294
Iteration 9, loss = 0.18249636
Iteration 10, loss = 0.16628367
Iteration 11, loss = 0.15320162
Iteration 12, loss = 0.14250498
Iteration 13, loss = 0.13378169
Iteration 14, loss = 0.12622353
Iteration 15, loss = 0.11953165
Iteration 16, loss = 0.11362022
Iteration 17, loss = 0.10860131
Iteration 18, loss = 0.10401993
Iteration 19, loss = 0.09982952
Iteration 20, loss = 0.09610094
Iteration 21, loss = 0.09203829
Iteration 22, loss = 0.08907587
Iteration 23, loss = 0.08568717
Iteration 24, loss = 0.08290533
Iteration 25, loss = 0.07967960
Iteration 26, loss = 0.07699088
Iteration 27, loss = 0.07496903
Iteration 28, loss = 0.07218427
Iteration 29, loss = 0.07022538
Iteration 30, loss = 0.06803248
Iteration 31, loss = 0.06588870
Iteration 32, loss = 0.06379122
Iteration 33, loss = 0.06207078
Iteration 34, loss = 0.06024047
Iteration 35, loss = 0.05866856
Iteration 36, loss = 0.05688542
Iteration 37, loss = 0.05539796
Iteration 38, loss = 0.05363301
Iteration 39, loss = 0.05217752
Iteration 40, loss = 0.05059276
Iteration 41, loss = 0.04920357
Iteration 42, loss = 0.04794129
Iteration 43, loss = 0.04666159
Iteration 44, loss = 0.04571654
Iteration 45, loss = 0.04415711
Iteration 46, loss = 0.04355087
Iteration 47, loss = 0.04180558
Iteration 48, loss = 0.04111485
Iteration 49, loss = 0.03977982
Iteration 50, loss = 0.03882685
Iteration 51, loss = 0.03775986
Iteration 52, loss = 0.03725706
Iteration 53, loss = 0.03598260
Iteration 54, loss = 0.03513786
Iteration 55, loss = 0.03409167
Iteration 56, loss = 0.03331892
Iteration 57, loss = 0.03232611
Iteration 58, loss = 0.03159349
Iteration 59, loss = 0.03097079
Iteration 60, loss = 0.03022264
Iteration 61, loss = 0.02931372
Iteration 62, loss = 0.02888769
Iteration 63, loss = 0.02781910
Iteration 64, loss = 0.02709520
Iteration 65, loss = 0.02663558
Iteration 66, loss = 0.02589932
Iteration 67, loss = 0.02531933
Iteration 68, loss = 0.02470813
Iteration 69, loss = 0.02418643
Iteration 70, loss = 0.02352329
Iteration 71, loss = 0.02295007
Iteration 72, loss = 0.02226845
Iteration 73, loss = 0.02180984
Iteration 74, loss = 0.02114921
Iteration 75, loss = 0.02069040
Iteration 76, loss = 0.02020821
Iteration 77, loss = 0.01966641
Iteration 78, loss = 0.01923401
Iteration 79, loss = 0.01872486
Iteration 80, loss = 0.01831888
Iteration 81, loss = 0.01783152
Iteration 82, loss = 0.01733098
Iteration 83, loss = 0.01700401
Iteration 84, loss = 0.01645895
Iteration 85, loss = 0.01614938
Iteration 86, loss = 0.01581987
Iteration 87, loss = 0.01554394
Iteration 88, loss = 0.01503769
Iteration 89, loss = 0.01475559
Iteration 90, loss = 0.01433113
Iteration 91, loss = 0.01397776
Iteration 92, loss = 0.01360847
Iteration 93, loss = 0.01333464
Iteration 94, loss = 0.01295151
Iteration 95, loss = 0.01272264
Iteration 96, loss = 0.01240732
Iteration 97, loss = 0.01210406
Iteration 98, loss = 0.01182068
Iteration 99, loss = 0.01155619
Iteration 100, loss = 0.01122837
Iteration 101, loss = 0.01100755
Iteration 102, loss = 0.01071411
Iteration 103, loss = 0.01055858
Iteration 104, loss = 0.01023264
Iteration 105, loss = 0.01019783
Iteration 106, loss = 0.00984548
Iteration 107, loss = 0.00960662
Iteration 108, loss = 0.00941308
Iteration 109, loss = 0.00919773
Iteration 110, loss = 0.00904313
Iteration 111, loss = 0.00885826
Iteration 112, loss = 0.00861652
Iteration 113, loss = 0.00844626
Iteration 114, loss = 0.00824422
Iteration 115, loss = 0.00809853
Iteration 116, loss = 0.00787934
Iteration 117, loss = 0.00780664
Iteration 118, loss = 0.00756583
Iteration 119, loss = 0.00737988
Iteration 120, loss = 0.00730116
Iteration 121, loss = 0.00710514
Iteration 122, loss = 0.00696151
Iteration 123, loss = 0.00682414
Iteration 124, loss = 0.00673615
Iteration 125, loss = 0.00654703
Iteration 126, loss = 0.00641112
Iteration 127, loss = 0.00629179
Iteration 128, loss = 0.00619703
Iteration 129, loss = 0.00602432
Iteration 130, loss = 0.00593362
Iteration 131, loss = 0.00579717
Iteration 132, loss = 0.00569923
Iteration 133, loss = 0.00559976
Iteration 134, loss = 0.00549273
Iteration 135, loss = 0.00537972
Iteration 136, loss = 0.00526014
Iteration 137, loss = 0.00515861
Iteration 138, loss = 0.00509069
Iteration 139, loss = 0.00495650
Iteration 140, loss = 0.00491209
Iteration 141, loss = 0.00481354
Iteration 142, loss = 0.00471048
Iteration 143, loss = 0.00462919
Iteration 144, loss = 0.00453002
Iteration 145, loss = 0.00444964
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  55
Error rate: 24.48% ( 612/2500)
 - Class 1:  34, Class 2:  18, Class 3: 499, Class 4:  45, Class 5:  16
Iteration 1, loss = 1.34728780
Iteration 2, loss = 0.86149706
Iteration 3, loss = 0.58499144
Iteration 4, loss = 0.42640064
Iteration 5, loss = 0.33173287
Iteration 6, loss = 0.27270236
Iteration 7, loss = 0.23307648
Iteration 8, loss = 0.20438547
Iteration 9, loss = 0.18346478
Iteration 10, loss = 0.16755787
Iteration 11, loss = 0.15437316
Iteration 12, loss = 0.14374801
Iteration 13, loss = 0.13450775
Iteration 14, loss = 0.12663545
Iteration 15, loss = 0.12058271
Iteration 16, loss = 0.11432695
Iteration 17, loss = 0.10953128
Iteration 18, loss = 0.10432393
Iteration 19, loss = 0.10049932
Iteration 20, loss = 0.09587923
Iteration 21, loss = 0.09219490
Iteration 22, loss = 0.08856188
Iteration 23, loss = 0.08567889
Iteration 24, loss = 0.08223872
Iteration 25, loss = 0.07929378
Iteration 26, loss = 0.07712861
Iteration 27, loss = 0.07410039
Iteration 28, loss = 0.07209644
Iteration 29, loss = 0.06962405
Iteration 30, loss = 0.06733087
Iteration 31, loss = 0.06504931
Iteration 32, loss = 0.06313055
Iteration 33, loss = 0.06138734
Iteration 34, loss = 0.05926807
Iteration 35, loss = 0.05734375
Iteration 36, loss = 0.05570151
Iteration 37, loss = 0.05383038
Iteration 38, loss = 0.05229726
Iteration 39, loss = 0.05131121
Iteration 40, loss = 0.04927790
Iteration 41, loss = 0.04784616
Iteration 42, loss = 0.04643619
Iteration 43, loss = 0.04527585
Iteration 44, loss = 0.04397584
Iteration 45, loss = 0.04284178
Iteration 46, loss = 0.04167635
Iteration 47, loss = 0.04040876
Iteration 48, loss = 0.03921920
Iteration 49, loss = 0.03812953
Iteration 50, loss = 0.03712776
Iteration 51, loss = 0.03619566
Iteration 52, loss = 0.03529428
Iteration 53, loss = 0.03410064
Iteration 54, loss = 0.03368929
Iteration 55, loss = 0.03254771
Iteration 56, loss = 0.03160192
Iteration 57, loss = 0.03059979
Iteration 58, loss = 0.02972701
Iteration 59, loss = 0.02895155
Iteration 60, loss = 0.02814468
Iteration 61, loss = 0.02735717
Iteration 62, loss = 0.02668569
Iteration 63, loss = 0.02619341
Iteration 64, loss = 0.02553606
Iteration 65, loss = 0.02469910
Iteration 66, loss = 0.02407415
Iteration 67, loss = 0.02329154
Iteration 68, loss = 0.02270751
Iteration 69, loss = 0.02218492
Iteration 70, loss = 0.02158945
Iteration 71, loss = 0.02107019
Iteration 72, loss = 0.02067529
Iteration 73, loss = 0.01982615
Iteration 74, loss = 0.01948303
Iteration 75, loss = 0.01896337
Iteration 76, loss = 0.01843696
Iteration 77, loss = 0.01797928
Iteration 78, loss = 0.01759378
Iteration 79, loss = 0.01707848
Iteration 80, loss = 0.01668883
Iteration 81, loss = 0.01627244
Iteration 82, loss = 0.01581099
Iteration 83, loss = 0.01535500
Iteration 84, loss = 0.01491001
Iteration 85, loss = 0.01459989
Iteration 86, loss = 0.01420639
Iteration 87, loss = 0.01381251
Iteration 88, loss = 0.01349599
Iteration 89, loss = 0.01313463
Iteration 90, loss = 0.01272970
Iteration 91, loss = 0.01246716
Iteration 92, loss = 0.01224576
Iteration 93, loss = 0.01188709
Iteration 94, loss = 0.01153147
Iteration 95, loss = 0.01124944
Iteration 96, loss = 0.01112739
Iteration 97, loss = 0.01072083
Iteration 98, loss = 0.01050924
Iteration 99, loss = 0.01018498
Iteration 100, loss = 0.00997903
Iteration 101, loss = 0.00970427
Iteration 102, loss = 0.00939943
Iteration 103, loss = 0.00927071
Iteration 104, loss = 0.00907781
Iteration 105, loss = 0.00878936
Iteration 106, loss = 0.00854955
Iteration 107, loss = 0.00843159
Iteration 108, loss = 0.00819075
Iteration 109, loss = 0.00802112
Iteration 110, loss = 0.00787178
Iteration 111, loss = 0.00765439
Iteration 112, loss = 0.00744569
Iteration 113, loss = 0.00730224
Iteration 114, loss = 0.00711267
Iteration 115, loss = 0.00695062
Iteration 116, loss = 0.00677416
Iteration 117, loss = 0.00668573
Iteration 118, loss = 0.00649775
Iteration 119, loss = 0.00637984
Iteration 120, loss = 0.00621936
Iteration 121, loss = 0.00610575
Iteration 122, loss = 0.00598277
Iteration 123, loss = 0.00585692
Iteration 124, loss = 0.00575756
Iteration 125, loss = 0.00561286
Iteration 126, loss = 0.00549433
Iteration 127, loss = 0.00536781
Iteration 128, loss = 0.00528482
Iteration 129, loss = 0.00517094
Iteration 130, loss = 0.00507253
Iteration 131, loss = 0.00499140
Iteration 132, loss = 0.00487100
Iteration 133, loss = 0.00478216
Iteration 134, loss = 0.00467157
Iteration 135, loss = 0.00458908
Iteration 136, loss = 0.00449258
Iteration 137, loss = 0.00443422
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  60
Error rate: 25.56% ( 639/2500)
 - Class 1:  58, Class 2:  18, Class 3: 500, Class 4:  36, Class 5:  27
Iteration 1, loss = 1.32650025
Iteration 2, loss = 0.85095668
Iteration 3, loss = 0.57837476
Iteration 4, loss = 0.41726688
Iteration 5, loss = 0.32301369
Iteration 6, loss = 0.26366779
Iteration 7, loss = 0.22463005
Iteration 8, loss = 0.19696723
Iteration 9, loss = 0.17635686
Iteration 10, loss = 0.16109289
Iteration 11, loss = 0.14811427
Iteration 12, loss = 0.13821991
Iteration 13, loss = 0.12881489
Iteration 14, loss = 0.12145613
Iteration 15, loss = 0.11504418
Iteration 16, loss = 0.10924134
Iteration 17, loss = 0.10468349
Iteration 18, loss = 0.09964014
Iteration 19, loss = 0.09552048
Iteration 20, loss = 0.09164695
Iteration 21, loss = 0.08790956
Iteration 22, loss = 0.08501623
Iteration 23, loss = 0.08181535
Iteration 24, loss = 0.07890658
Iteration 25, loss = 0.07605662
Iteration 26, loss = 0.07374607
Iteration 27, loss = 0.07121714
Iteration 28, loss = 0.06858384
Iteration 29, loss = 0.06667470
Iteration 30, loss = 0.06483862
Iteration 31, loss = 0.06246784
Iteration 32, loss = 0.06056750
Iteration 33, loss = 0.05872274
Iteration 34, loss = 0.05724860
Iteration 35, loss = 0.05552099
Iteration 36, loss = 0.05371122
Iteration 37, loss = 0.05219334
Iteration 38, loss = 0.05065940
Iteration 39, loss = 0.04915705
Iteration 40, loss = 0.04779818
Iteration 41, loss = 0.04631934
Iteration 42, loss = 0.04520263
Iteration 43, loss = 0.04402364
Iteration 44, loss = 0.04263232
Iteration 45, loss = 0.04153581
Iteration 46, loss = 0.04052967
Iteration 47, loss = 0.03948469
Iteration 48, loss = 0.03823065
Iteration 49, loss = 0.03746633
Iteration 50, loss = 0.03629516
Iteration 51, loss = 0.03534412
Iteration 52, loss = 0.03429662
Iteration 53, loss = 0.03360635
Iteration 54, loss = 0.03246795
Iteration 55, loss = 0.03168790
Iteration 56, loss = 0.03078236
Iteration 57, loss = 0.02993762
Iteration 58, loss = 0.02920856
Iteration 59, loss = 0.02844775
Iteration 60, loss = 0.02769297
Iteration 61, loss = 0.02700066
Iteration 62, loss = 0.02631428
Iteration 63, loss = 0.02551551
Iteration 64, loss = 0.02508329
Iteration 65, loss = 0.02445795
Iteration 66, loss = 0.02355967
Iteration 67, loss = 0.02293353
Iteration 68, loss = 0.02223189
Iteration 69, loss = 0.02166725
Iteration 70, loss = 0.02139610
Iteration 71, loss = 0.02065629
Iteration 72, loss = 0.02036765
Iteration 73, loss = 0.01965455
Iteration 74, loss = 0.01907995
Iteration 75, loss = 0.01866377
Iteration 76, loss = 0.01807961
Iteration 77, loss = 0.01770936
Iteration 78, loss = 0.01715376
Iteration 79, loss = 0.01671845
Iteration 80, loss = 0.01638686
Iteration 81, loss = 0.01580366
Iteration 82, loss = 0.01546084
Iteration 83, loss = 0.01505081
Iteration 84, loss = 0.01464708
Iteration 85, loss = 0.01427139
Iteration 86, loss = 0.01392534
Iteration 87, loss = 0.01351003
Iteration 88, loss = 0.01306904
Iteration 89, loss = 0.01277398
Iteration 90, loss = 0.01245522
Iteration 91, loss = 0.01209694
Iteration 92, loss = 0.01190173
Iteration 93, loss = 0.01160175
Iteration 94, loss = 0.01120377
Iteration 95, loss = 0.01093019
Iteration 96, loss = 0.01065983
Iteration 97, loss = 0.01039208
Iteration 98, loss = 0.01012117
Iteration 99, loss = 0.00986469
Iteration 100, loss = 0.00965335
Iteration 101, loss = 0.00945381
Iteration 102, loss = 0.00928863
Iteration 103, loss = 0.00900488
Iteration 104, loss = 0.00881451
Iteration 105, loss = 0.00856978
Iteration 106, loss = 0.00836438
Iteration 107, loss = 0.00815942
Iteration 108, loss = 0.00792335
Iteration 109, loss = 0.00772862
Iteration 110, loss = 0.00755474
Iteration 111, loss = 0.00739336
Iteration 112, loss = 0.00721197
Iteration 113, loss = 0.00704339
Iteration 114, loss = 0.00687142
Iteration 115, loss = 0.00672163
Iteration 116, loss = 0.00654788
Iteration 117, loss = 0.00642774
Iteration 118, loss = 0.00627150
Iteration 119, loss = 0.00612671
Iteration 120, loss = 0.00596914
Iteration 121, loss = 0.00586142
Iteration 122, loss = 0.00579057
Iteration 123, loss = 0.00561292
Iteration 124, loss = 0.00548172
Iteration 125, loss = 0.00537187
Iteration 126, loss = 0.00523407
Iteration 127, loss = 0.00518434
Iteration 128, loss = 0.00502776
Iteration 129, loss = 0.00497008
Iteration 130, loss = 0.00486691
Iteration 131, loss = 0.00473415
Iteration 132, loss = 0.00464621
Iteration 133, loss = 0.00453653
Iteration 134, loss = 0.00444719
Iteration 135, loss = 0.00435173
Iteration 136, loss = 0.00428455
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  65
Error rate: 24.92% ( 623/2500)
 - Class 1:  37, Class 2:  19, Class 3: 499, Class 4:  34, Class 5:  34
Iteration 1, loss = 1.27400940
Iteration 2, loss = 0.77843930
Iteration 3, loss = 0.51190095
Iteration 4, loss = 0.36834149
Iteration 5, loss = 0.28903090
Iteration 6, loss = 0.23823011
Iteration 7, loss = 0.20472560
Iteration 8, loss = 0.18143709
Iteration 9, loss = 0.16350482
Iteration 10, loss = 0.14996935
Iteration 11, loss = 0.13860459
Iteration 12, loss = 0.12928244
Iteration 13, loss = 0.12152543
Iteration 14, loss = 0.11493622
Iteration 15, loss = 0.10888890
Iteration 16, loss = 0.10364500
Iteration 17, loss = 0.09887930
Iteration 18, loss = 0.09501924
Iteration 19, loss = 0.09053977
Iteration 20, loss = 0.08718435
Iteration 21, loss = 0.08364342
Iteration 22, loss = 0.08070388
Iteration 23, loss = 0.07753952
Iteration 24, loss = 0.07491679
Iteration 25, loss = 0.07247293
Iteration 26, loss = 0.06987168
Iteration 27, loss = 0.06725622
Iteration 28, loss = 0.06501493
Iteration 29, loss = 0.06267288
Iteration 30, loss = 0.06099610
Iteration 31, loss = 0.05970292
Iteration 32, loss = 0.05702111
Iteration 33, loss = 0.05496189
Iteration 34, loss = 0.05325708
Iteration 35, loss = 0.05161052
Iteration 36, loss = 0.05064336
Iteration 37, loss = 0.04894264
Iteration 38, loss = 0.04726348
Iteration 39, loss = 0.04602829
Iteration 40, loss = 0.04478814
Iteration 41, loss = 0.04351761
Iteration 42, loss = 0.04203707
Iteration 43, loss = 0.04085781
Iteration 44, loss = 0.03942713
Iteration 45, loss = 0.03835027
Iteration 46, loss = 0.03751097
Iteration 47, loss = 0.03653227
Iteration 48, loss = 0.03536617
Iteration 49, loss = 0.03417185
Iteration 50, loss = 0.03339328
Iteration 51, loss = 0.03225894
Iteration 52, loss = 0.03143722
Iteration 53, loss = 0.03048465
Iteration 54, loss = 0.02982682
Iteration 55, loss = 0.02879842
Iteration 56, loss = 0.02795808
Iteration 57, loss = 0.02722240
Iteration 58, loss = 0.02639754
Iteration 59, loss = 0.02567790
Iteration 60, loss = 0.02500077
Iteration 61, loss = 0.02428754
Iteration 62, loss = 0.02372819
Iteration 63, loss = 0.02302318
Iteration 64, loss = 0.02238222
Iteration 65, loss = 0.02157264
Iteration 66, loss = 0.02091021
Iteration 67, loss = 0.02047960
Iteration 68, loss = 0.01991600
Iteration 69, loss = 0.01946587
Iteration 70, loss = 0.01881449
Iteration 71, loss = 0.01820397
Iteration 72, loss = 0.01772156
Iteration 73, loss = 0.01713828
Iteration 74, loss = 0.01677036
Iteration 75, loss = 0.01633169
Iteration 76, loss = 0.01598258
Iteration 77, loss = 0.01539019
Iteration 78, loss = 0.01507726
Iteration 79, loss = 0.01458662
Iteration 80, loss = 0.01419851
Iteration 81, loss = 0.01372153
Iteration 82, loss = 0.01334197
Iteration 83, loss = 0.01306892
Iteration 84, loss = 0.01268038
Iteration 85, loss = 0.01231056
Iteration 86, loss = 0.01198580
Iteration 87, loss = 0.01167934
Iteration 88, loss = 0.01136048
Iteration 89, loss = 0.01119087
Iteration 90, loss = 0.01090986
Iteration 91, loss = 0.01058637
Iteration 92, loss = 0.01024940
Iteration 93, loss = 0.01010388
Iteration 94, loss = 0.00968339
Iteration 95, loss = 0.00947157
Iteration 96, loss = 0.00929849
Iteration 97, loss = 0.00897353
Iteration 98, loss = 0.00872845
Iteration 99, loss = 0.00856686
Iteration 100, loss = 0.00835799
Iteration 101, loss = 0.00818297
Iteration 102, loss = 0.00798850
Iteration 103, loss = 0.00775648
Iteration 104, loss = 0.00757911
Iteration 105, loss = 0.00739469
Iteration 106, loss = 0.00729204
Iteration 107, loss = 0.00705030
Iteration 108, loss = 0.00687612
Iteration 109, loss = 0.00673352
Iteration 110, loss = 0.00660392
Iteration 111, loss = 0.00643485
Iteration 112, loss = 0.00630551
Iteration 113, loss = 0.00620797
Iteration 114, loss = 0.00601613
Iteration 115, loss = 0.00587650
Iteration 116, loss = 0.00578014
Iteration 117, loss = 0.00567884
Iteration 118, loss = 0.00556923
Iteration 119, loss = 0.00537669
Iteration 120, loss = 0.00524520
Iteration 121, loss = 0.00512415
Iteration 122, loss = 0.00500872
Iteration 123, loss = 0.00490784
Iteration 124, loss = 0.00484132
Iteration 125, loss = 0.00469331
Iteration 126, loss = 0.00462185
Iteration 127, loss = 0.00450909
Iteration 128, loss = 0.00444995
Iteration 129, loss = 0.00433553
Iteration 130, loss = 0.00424121
Iteration 131, loss = 0.00417343
Iteration 132, loss = 0.00408846
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  70
Error rate: 24.84% ( 621/2500)
 - Class 1:  47, Class 2:  20, Class 3: 500, Class 4:  43, Class 5:  11
Iteration 1, loss = 1.29869706
Iteration 2, loss = 0.80246560
Iteration 3, loss = 0.52751771
Iteration 4, loss = 0.37726394
Iteration 5, loss = 0.29262962
Iteration 6, loss = 0.24146178
Iteration 7, loss = 0.20731481
Iteration 8, loss = 0.18321497
Iteration 9, loss = 0.16501895
Iteration 10, loss = 0.15088149
Iteration 11, loss = 0.13990147
Iteration 12, loss = 0.13026435
Iteration 13, loss = 0.12239597
Iteration 14, loss = 0.11598689
Iteration 15, loss = 0.10960384
Iteration 16, loss = 0.10412025
Iteration 17, loss = 0.09935256
Iteration 18, loss = 0.09497002
Iteration 19, loss = 0.09129926
Iteration 20, loss = 0.08766813
Iteration 21, loss = 0.08374528
Iteration 22, loss = 0.08084777
Iteration 23, loss = 0.07742070
Iteration 24, loss = 0.07483141
Iteration 25, loss = 0.07210033
Iteration 26, loss = 0.06970527
Iteration 27, loss = 0.06681788
Iteration 28, loss = 0.06472956
Iteration 29, loss = 0.06277348
Iteration 30, loss = 0.06043947
Iteration 31, loss = 0.05879598
Iteration 32, loss = 0.05642290
Iteration 33, loss = 0.05488701
Iteration 34, loss = 0.05294942
Iteration 35, loss = 0.05160135
Iteration 36, loss = 0.04957960
Iteration 37, loss = 0.04850494
Iteration 38, loss = 0.04686578
Iteration 39, loss = 0.04545661
Iteration 40, loss = 0.04411804
Iteration 41, loss = 0.04300554
Iteration 42, loss = 0.04202297
Iteration 43, loss = 0.04023662
Iteration 44, loss = 0.03918033
Iteration 45, loss = 0.03815101
Iteration 46, loss = 0.03723452
Iteration 47, loss = 0.03594631
Iteration 48, loss = 0.03490224
Iteration 49, loss = 0.03394188
Iteration 50, loss = 0.03289213
Iteration 51, loss = 0.03187166
Iteration 52, loss = 0.03100972
Iteration 53, loss = 0.03041633
Iteration 54, loss = 0.02927548
Iteration 55, loss = 0.02856707
Iteration 56, loss = 0.02782425
Iteration 57, loss = 0.02718911
Iteration 58, loss = 0.02628379
Iteration 59, loss = 0.02547454
Iteration 60, loss = 0.02476550
Iteration 61, loss = 0.02431091
Iteration 62, loss = 0.02353493
Iteration 63, loss = 0.02283738
Iteration 64, loss = 0.02207192
Iteration 65, loss = 0.02148394
Iteration 66, loss = 0.02103349
Iteration 67, loss = 0.02038863
Iteration 68, loss = 0.01976541
Iteration 69, loss = 0.01928662
Iteration 70, loss = 0.01866040
Iteration 71, loss = 0.01810788
Iteration 72, loss = 0.01761132
Iteration 73, loss = 0.01709399
Iteration 74, loss = 0.01666987
Iteration 75, loss = 0.01618707
Iteration 76, loss = 0.01570613
Iteration 77, loss = 0.01526991
Iteration 78, loss = 0.01483161
Iteration 79, loss = 0.01444349
Iteration 80, loss = 0.01396092
Iteration 81, loss = 0.01366305
Iteration 82, loss = 0.01319198
Iteration 83, loss = 0.01278509
Iteration 84, loss = 0.01261955
Iteration 85, loss = 0.01220605
Iteration 86, loss = 0.01185237
Iteration 87, loss = 0.01152298
Iteration 88, loss = 0.01109524
Iteration 89, loss = 0.01089870
Iteration 90, loss = 0.01076811
Iteration 91, loss = 0.01040354
Iteration 92, loss = 0.01002821
Iteration 93, loss = 0.00971873
Iteration 94, loss = 0.00960603
Iteration 95, loss = 0.00931616
Iteration 96, loss = 0.00897456
Iteration 97, loss = 0.00877750
Iteration 98, loss = 0.00857204
Iteration 99, loss = 0.00837213
Iteration 100, loss = 0.00810641
Iteration 101, loss = 0.00790751
Iteration 102, loss = 0.00772216
Iteration 103, loss = 0.00751316
Iteration 104, loss = 0.00732063
Iteration 105, loss = 0.00722487
Iteration 106, loss = 0.00701106
Iteration 107, loss = 0.00681977
Iteration 108, loss = 0.00665393
Iteration 109, loss = 0.00648402
Iteration 110, loss = 0.00635081
Iteration 111, loss = 0.00617699
Iteration 112, loss = 0.00610255
Iteration 113, loss = 0.00591761
Iteration 114, loss = 0.00576026
Iteration 115, loss = 0.00563385
Iteration 116, loss = 0.00550922
Iteration 117, loss = 0.00541177
Iteration 118, loss = 0.00529449
Iteration 119, loss = 0.00519914
Iteration 120, loss = 0.00506072
Iteration 121, loss = 0.00494113
Iteration 122, loss = 0.00483987
Iteration 123, loss = 0.00475063
Iteration 124, loss = 0.00463884
Iteration 125, loss = 0.00453869
Iteration 126, loss = 0.00446260
Iteration 127, loss = 0.00436036
Iteration 128, loss = 0.00427847
Iteration 129, loss = 0.00420643
Iteration 130, loss = 0.00411686
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  75
Error rate: 25.12% ( 628/2500)
 - Class 1:  64, Class 2:  21, Class 3: 500, Class 4:  35, Class 5:   8
Iteration 1, loss = 1.26102997
Iteration 2, loss = 0.74865643
Iteration 3, loss = 0.48705529
Iteration 4, loss = 0.34937545
Iteration 5, loss = 0.27323340
Iteration 6, loss = 0.22683778
Iteration 7, loss = 0.19544510
Iteration 8, loss = 0.17367448
Iteration 9, loss = 0.15715680
Iteration 10, loss = 0.14386071
Iteration 11, loss = 0.13319521
Iteration 12, loss = 0.12457049
Iteration 13, loss = 0.11744986
Iteration 14, loss = 0.11080504
Iteration 15, loss = 0.10519583
Iteration 16, loss = 0.10004364
Iteration 17, loss = 0.09590612
Iteration 18, loss = 0.09160360
Iteration 19, loss = 0.08798118
Iteration 20, loss = 0.08479632
Iteration 21, loss = 0.08094011
Iteration 22, loss = 0.07795433
Iteration 23, loss = 0.07530363
Iteration 24, loss = 0.07259422
Iteration 25, loss = 0.06961787
Iteration 26, loss = 0.06748078
Iteration 27, loss = 0.06475018
Iteration 28, loss = 0.06260946
Iteration 29, loss = 0.06051089
Iteration 30, loss = 0.05862870
Iteration 31, loss = 0.05699122
Iteration 32, loss = 0.05471149
Iteration 33, loss = 0.05298119
Iteration 34, loss = 0.05163742
Iteration 35, loss = 0.05000125
Iteration 36, loss = 0.04821971
Iteration 37, loss = 0.04722363
Iteration 38, loss = 0.04522489
Iteration 39, loss = 0.04387017
Iteration 40, loss = 0.04272736
Iteration 41, loss = 0.04103380
Iteration 42, loss = 0.04003181
Iteration 43, loss = 0.03883773
Iteration 44, loss = 0.03773066
Iteration 45, loss = 0.03639330
Iteration 46, loss = 0.03547258
Iteration 47, loss = 0.03445439
Iteration 48, loss = 0.03353715
Iteration 49, loss = 0.03248742
Iteration 50, loss = 0.03151547
Iteration 51, loss = 0.03048953
Iteration 52, loss = 0.02970281
Iteration 53, loss = 0.02876969
Iteration 54, loss = 0.02800625
Iteration 55, loss = 0.02718231
Iteration 56, loss = 0.02650541
Iteration 57, loss = 0.02572006
Iteration 58, loss = 0.02479198
Iteration 59, loss = 0.02408623
Iteration 60, loss = 0.02336421
Iteration 61, loss = 0.02276639
Iteration 62, loss = 0.02207581
Iteration 63, loss = 0.02151563
Iteration 64, loss = 0.02074023
Iteration 65, loss = 0.02029218
Iteration 66, loss = 0.01968075
Iteration 67, loss = 0.01914038
Iteration 68, loss = 0.01843450
Iteration 69, loss = 0.01803784
Iteration 70, loss = 0.01747722
Iteration 71, loss = 0.01689670
Iteration 72, loss = 0.01645787
Iteration 73, loss = 0.01607454
Iteration 74, loss = 0.01575432
Iteration 75, loss = 0.01515813
Iteration 76, loss = 0.01473521
Iteration 77, loss = 0.01425784
Iteration 78, loss = 0.01394891
Iteration 79, loss = 0.01348971
Iteration 80, loss = 0.01324417
Iteration 81, loss = 0.01271419
Iteration 82, loss = 0.01235603
Iteration 83, loss = 0.01206042
Iteration 84, loss = 0.01177763
Iteration 85, loss = 0.01146030
Iteration 86, loss = 0.01113739
Iteration 87, loss = 0.01082882
Iteration 88, loss = 0.01059563
Iteration 89, loss = 0.01021476
Iteration 90, loss = 0.00997007
Iteration 91, loss = 0.00974781
Iteration 92, loss = 0.00948419
Iteration 93, loss = 0.00919360
Iteration 94, loss = 0.00900878
Iteration 95, loss = 0.00875867
Iteration 96, loss = 0.00856647
Iteration 97, loss = 0.00827224
Iteration 98, loss = 0.00804845
Iteration 99, loss = 0.00787080
Iteration 100, loss = 0.00762953
Iteration 101, loss = 0.00753182
Iteration 102, loss = 0.00732363
Iteration 103, loss = 0.00708496
Iteration 104, loss = 0.00695992
Iteration 105, loss = 0.00678691
Iteration 106, loss = 0.00662952
Iteration 107, loss = 0.00659041
Iteration 108, loss = 0.00632332
Iteration 109, loss = 0.00614294
Iteration 110, loss = 0.00603588
Iteration 111, loss = 0.00586262
Iteration 112, loss = 0.00569209
Iteration 113, loss = 0.00558851
Iteration 114, loss = 0.00548190
Iteration 115, loss = 0.00536034
Iteration 116, loss = 0.00525698
Iteration 117, loss = 0.00510876
Iteration 118, loss = 0.00500936
Iteration 119, loss = 0.00490917
Iteration 120, loss = 0.00477468
Iteration 121, loss = 0.00470369
Iteration 122, loss = 0.00466674
Iteration 123, loss = 0.00451892
Iteration 124, loss = 0.00442597
Iteration 125, loss = 0.00435632
Iteration 126, loss = 0.00423242
Iteration 127, loss = 0.00413637
Iteration 128, loss = 0.00406049
Iteration 129, loss = 0.00399293
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  80
Error rate: 24.76% ( 619/2500)
 - Class 1:  26, Class 2:  18, Class 3: 500, Class 4:  38, Class 5:  37
Iteration 1, loss = 1.25420875
Iteration 2, loss = 0.71285900
Iteration 3, loss = 0.45209964
Iteration 4, loss = 0.32452029
Iteration 5, loss = 0.25600835
Iteration 6, loss = 0.21398411
Iteration 7, loss = 0.18619355
Iteration 8, loss = 0.16574830
Iteration 9, loss = 0.15067086
Iteration 10, loss = 0.13874304
Iteration 11, loss = 0.12935036
Iteration 12, loss = 0.12064082
Iteration 13, loss = 0.11378879
Iteration 14, loss = 0.10855283
Iteration 15, loss = 0.10358731
Iteration 16, loss = 0.09788248
Iteration 17, loss = 0.09329273
Iteration 18, loss = 0.08946739
Iteration 19, loss = 0.08539705
Iteration 20, loss = 0.08203714
Iteration 21, loss = 0.07867503
Iteration 22, loss = 0.07600662
Iteration 23, loss = 0.07347000
Iteration 24, loss = 0.07067753
Iteration 25, loss = 0.06804333
Iteration 26, loss = 0.06538082
Iteration 27, loss = 0.06339237
Iteration 28, loss = 0.06115701
Iteration 29, loss = 0.05901207
Iteration 30, loss = 0.05726053
Iteration 31, loss = 0.05524759
Iteration 32, loss = 0.05369069
Iteration 33, loss = 0.05175239
Iteration 34, loss = 0.05020352
Iteration 35, loss = 0.04856252
Iteration 36, loss = 0.04688266
Iteration 37, loss = 0.04614502
Iteration 38, loss = 0.04424990
Iteration 39, loss = 0.04292881
Iteration 40, loss = 0.04178662
Iteration 41, loss = 0.04058508
Iteration 42, loss = 0.03924839
Iteration 43, loss = 0.03832177
Iteration 44, loss = 0.03694241
Iteration 45, loss = 0.03586096
Iteration 46, loss = 0.03495909
Iteration 47, loss = 0.03402862
Iteration 48, loss = 0.03292602
Iteration 49, loss = 0.03245945
Iteration 50, loss = 0.03082180
Iteration 51, loss = 0.03017343
Iteration 52, loss = 0.02927760
Iteration 53, loss = 0.02824293
Iteration 54, loss = 0.02749899
Iteration 55, loss = 0.02669009
Iteration 56, loss = 0.02571767
Iteration 57, loss = 0.02509013
Iteration 58, loss = 0.02434307
Iteration 59, loss = 0.02362739
Iteration 60, loss = 0.02289409
Iteration 61, loss = 0.02228269
Iteration 62, loss = 0.02155264
Iteration 63, loss = 0.02116097
Iteration 64, loss = 0.02048211
Iteration 65, loss = 0.01978023
Iteration 66, loss = 0.01929769
Iteration 67, loss = 0.01883031
Iteration 68, loss = 0.01808793
Iteration 69, loss = 0.01753024
Iteration 70, loss = 0.01693210
Iteration 71, loss = 0.01649957
Iteration 72, loss = 0.01613926
Iteration 73, loss = 0.01546255
Iteration 74, loss = 0.01523511
Iteration 75, loss = 0.01469955
Iteration 76, loss = 0.01425602
Iteration 77, loss = 0.01375925
Iteration 78, loss = 0.01340075
Iteration 79, loss = 0.01297196
Iteration 80, loss = 0.01270115
Iteration 81, loss = 0.01232389
Iteration 82, loss = 0.01195086
Iteration 83, loss = 0.01167665
Iteration 84, loss = 0.01135469
Iteration 85, loss = 0.01100903
Iteration 86, loss = 0.01060860
Iteration 87, loss = 0.01036352
Iteration 88, loss = 0.01008603
Iteration 89, loss = 0.00984270
Iteration 90, loss = 0.00947298
Iteration 91, loss = 0.00918964
Iteration 92, loss = 0.00901956
Iteration 93, loss = 0.00874093
Iteration 94, loss = 0.00850901
Iteration 95, loss = 0.00830319
Iteration 96, loss = 0.00807197
Iteration 97, loss = 0.00792812
Iteration 98, loss = 0.00764052
Iteration 99, loss = 0.00743920
Iteration 100, loss = 0.00720831
Iteration 101, loss = 0.00705817
Iteration 102, loss = 0.00687790
Iteration 103, loss = 0.00672255
Iteration 104, loss = 0.00661146
Iteration 105, loss = 0.00637021
Iteration 106, loss = 0.00623026
Iteration 107, loss = 0.00604410
Iteration 108, loss = 0.00594447
Iteration 109, loss = 0.00576608
Iteration 110, loss = 0.00566128
Iteration 111, loss = 0.00550053
Iteration 112, loss = 0.00536446
Iteration 113, loss = 0.00527869
Iteration 114, loss = 0.00512485
Iteration 115, loss = 0.00504290
Iteration 116, loss = 0.00491437
Iteration 117, loss = 0.00476259
Iteration 118, loss = 0.00468052
Iteration 119, loss = 0.00456917
Iteration 120, loss = 0.00446414
Iteration 121, loss = 0.00438677
Iteration 122, loss = 0.00427968
Iteration 123, loss = 0.00419702
Iteration 124, loss = 0.00411499
Iteration 125, loss = 0.00402529
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  85
Error rate: 25.20% ( 630/2500)
 - Class 1:  52, Class 2:  21, Class 3: 496, Class 4:  28, Class 5:  33
Iteration 1, loss = 1.23264111
Iteration 2, loss = 0.71168005
Iteration 3, loss = 0.45534568
Iteration 4, loss = 0.32512518
Iteration 5, loss = 0.25513153
Iteration 6, loss = 0.21282880
Iteration 7, loss = 0.18436724
Iteration 8, loss = 0.16472608
Iteration 9, loss = 0.14927332
Iteration 10, loss = 0.13746685
Iteration 11, loss = 0.12754086
Iteration 12, loss = 0.11972386
Iteration 13, loss = 0.11253944
Iteration 14, loss = 0.10660246
Iteration 15, loss = 0.10106448
Iteration 16, loss = 0.09665539
Iteration 17, loss = 0.09214256
Iteration 18, loss = 0.08825616
Iteration 19, loss = 0.08459795
Iteration 20, loss = 0.08137446
Iteration 21, loss = 0.07833662
Iteration 22, loss = 0.07463700
Iteration 23, loss = 0.07215143
Iteration 24, loss = 0.06950764
Iteration 25, loss = 0.06673249
Iteration 26, loss = 0.06468050
Iteration 27, loss = 0.06209770
Iteration 28, loss = 0.06022713
Iteration 29, loss = 0.05814866
Iteration 30, loss = 0.05656482
Iteration 31, loss = 0.05425620
Iteration 32, loss = 0.05264518
Iteration 33, loss = 0.05079130
Iteration 34, loss = 0.04962542
Iteration 35, loss = 0.04788429
Iteration 36, loss = 0.04612884
Iteration 37, loss = 0.04466225
Iteration 38, loss = 0.04334399
Iteration 39, loss = 0.04202698
Iteration 40, loss = 0.04084882
Iteration 41, loss = 0.03957626
Iteration 42, loss = 0.03851394
Iteration 43, loss = 0.03710772
Iteration 44, loss = 0.03618171
Iteration 45, loss = 0.03509802
Iteration 46, loss = 0.03419892
Iteration 47, loss = 0.03298172
Iteration 48, loss = 0.03202591
Iteration 49, loss = 0.03108045
Iteration 50, loss = 0.03030984
Iteration 51, loss = 0.02919447
Iteration 52, loss = 0.02852693
Iteration 53, loss = 0.02763909
Iteration 54, loss = 0.02675714
Iteration 55, loss = 0.02619446
Iteration 56, loss = 0.02514025
Iteration 57, loss = 0.02452960
Iteration 58, loss = 0.02400418
Iteration 59, loss = 0.02302438
Iteration 60, loss = 0.02241934
Iteration 61, loss = 0.02148806
Iteration 62, loss = 0.02121073
Iteration 63, loss = 0.02037130
Iteration 64, loss = 0.01976346
Iteration 65, loss = 0.01925370
Iteration 66, loss = 0.01859727
Iteration 67, loss = 0.01826599
Iteration 68, loss = 0.01749529
Iteration 69, loss = 0.01736022
Iteration 70, loss = 0.01647345
Iteration 71, loss = 0.01594392
Iteration 72, loss = 0.01538471
Iteration 73, loss = 0.01494218
Iteration 74, loss = 0.01458557
Iteration 75, loss = 0.01410961
Iteration 76, loss = 0.01363849
Iteration 77, loss = 0.01329949
Iteration 78, loss = 0.01280770
Iteration 79, loss = 0.01258716
Iteration 80, loss = 0.01212419
Iteration 81, loss = 0.01171907
Iteration 82, loss = 0.01141447
Iteration 83, loss = 0.01104058
Iteration 84, loss = 0.01070966
Iteration 85, loss = 0.01043902
Iteration 86, loss = 0.01012212
Iteration 87, loss = 0.00981273
Iteration 88, loss = 0.00961453
Iteration 89, loss = 0.00932727
Iteration 90, loss = 0.00914280
Iteration 91, loss = 0.00884293
Iteration 92, loss = 0.00853454
Iteration 93, loss = 0.00831722
Iteration 94, loss = 0.00811510
Iteration 95, loss = 0.00795768
Iteration 96, loss = 0.00761533
Iteration 97, loss = 0.00743195
Iteration 98, loss = 0.00727906
Iteration 99, loss = 0.00706357
Iteration 100, loss = 0.00686770
Iteration 101, loss = 0.00669532
Iteration 102, loss = 0.00649617
Iteration 103, loss = 0.00634633
Iteration 104, loss = 0.00626463
Iteration 105, loss = 0.00610313
Iteration 106, loss = 0.00593730
Iteration 107, loss = 0.00574835
Iteration 108, loss = 0.00559328
Iteration 109, loss = 0.00553079
Iteration 110, loss = 0.00532373
Iteration 111, loss = 0.00518893
Iteration 112, loss = 0.00508261
Iteration 113, loss = 0.00498246
Iteration 114, loss = 0.00485510
Iteration 115, loss = 0.00475732
Iteration 116, loss = 0.00460036
Iteration 117, loss = 0.00449942
Iteration 118, loss = 0.00440465
Iteration 119, loss = 0.00434034
Iteration 120, loss = 0.00421072
Iteration 121, loss = 0.00413541
Iteration 122, loss = 0.00405188
Iteration 123, loss = 0.00398557
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  90
Error rate: 23.84% ( 596/2500)
 - Class 1:  24, Class 2:  23, Class 3: 449, Class 4:  40, Class 5:  60
Iteration 1, loss = 1.22806842
Iteration 2, loss = 0.69244835
Iteration 3, loss = 0.43612349
Iteration 4, loss = 0.31198556
Iteration 5, loss = 0.24621120
Iteration 6, loss = 0.20632934
Iteration 7, loss = 0.17986619
Iteration 8, loss = 0.16070564
Iteration 9, loss = 0.14585998
Iteration 10, loss = 0.13446963
Iteration 11, loss = 0.12572480
Iteration 12, loss = 0.11800610
Iteration 13, loss = 0.11091970
Iteration 14, loss = 0.10525560
Iteration 15, loss = 0.10012384
Iteration 16, loss = 0.09531116
Iteration 17, loss = 0.09121350
Iteration 18, loss = 0.08675996
Iteration 19, loss = 0.08326834
Iteration 20, loss = 0.08018487
Iteration 21, loss = 0.07714892
Iteration 22, loss = 0.07378453
Iteration 23, loss = 0.07108007
Iteration 24, loss = 0.06820568
Iteration 25, loss = 0.06650209
Iteration 26, loss = 0.06376788
Iteration 27, loss = 0.06131640
Iteration 28, loss = 0.05907381
Iteration 29, loss = 0.05727989
Iteration 30, loss = 0.05567203
Iteration 31, loss = 0.05362853
Iteration 32, loss = 0.05209240
Iteration 33, loss = 0.05015105
Iteration 34, loss = 0.04885523
Iteration 35, loss = 0.04713339
Iteration 36, loss = 0.04550036
Iteration 37, loss = 0.04401485
Iteration 38, loss = 0.04275020
Iteration 39, loss = 0.04137782
Iteration 40, loss = 0.04020757
Iteration 41, loss = 0.03888847
Iteration 42, loss = 0.03778212
Iteration 43, loss = 0.03672359
Iteration 44, loss = 0.03576307
Iteration 45, loss = 0.03443453
Iteration 46, loss = 0.03364517
Iteration 47, loss = 0.03278065
Iteration 48, loss = 0.03139125
Iteration 49, loss = 0.03058045
Iteration 50, loss = 0.02978028
Iteration 51, loss = 0.02889784
Iteration 52, loss = 0.02833559
Iteration 53, loss = 0.02731546
Iteration 54, loss = 0.02622897
Iteration 55, loss = 0.02558323
Iteration 56, loss = 0.02467444
Iteration 57, loss = 0.02378356
Iteration 58, loss = 0.02356085
Iteration 59, loss = 0.02262392
Iteration 60, loss = 0.02186904
Iteration 61, loss = 0.02149447
Iteration 62, loss = 0.02062434
Iteration 63, loss = 0.01998785
Iteration 64, loss = 0.01949122
Iteration 65, loss = 0.01877690
Iteration 66, loss = 0.01849923
Iteration 67, loss = 0.01818622
Iteration 68, loss = 0.01715606
Iteration 69, loss = 0.01664993
Iteration 70, loss = 0.01603832
Iteration 71, loss = 0.01559420
Iteration 72, loss = 0.01514908
Iteration 73, loss = 0.01473529
Iteration 74, loss = 0.01422244
Iteration 75, loss = 0.01400234
Iteration 76, loss = 0.01343398
Iteration 77, loss = 0.01300089
Iteration 78, loss = 0.01264228
Iteration 79, loss = 0.01226737
Iteration 80, loss = 0.01193914
Iteration 81, loss = 0.01162582
Iteration 82, loss = 0.01131840
Iteration 83, loss = 0.01094308
Iteration 84, loss = 0.01051784
Iteration 85, loss = 0.01030106
Iteration 86, loss = 0.01014953
Iteration 87, loss = 0.00973538
Iteration 88, loss = 0.00957630
Iteration 89, loss = 0.00915634
Iteration 90, loss = 0.00893105
Iteration 91, loss = 0.00870049
Iteration 92, loss = 0.00846713
Iteration 93, loss = 0.00824317
Iteration 94, loss = 0.00804721
Iteration 95, loss = 0.00785859
Iteration 96, loss = 0.00768570
Iteration 97, loss = 0.00744626
Iteration 98, loss = 0.00721368
Iteration 99, loss = 0.00701361
Iteration 100, loss = 0.00684048
Iteration 101, loss = 0.00666256
Iteration 102, loss = 0.00653283
Iteration 103, loss = 0.00630329
Iteration 104, loss = 0.00615461
Iteration 105, loss = 0.00602915
Iteration 106, loss = 0.00587160
Iteration 107, loss = 0.00571833
Iteration 108, loss = 0.00557933
Iteration 109, loss = 0.00542729
Iteration 110, loss = 0.00531876
Iteration 111, loss = 0.00519882
Iteration 112, loss = 0.00507950
Iteration 113, loss = 0.00493841
Iteration 114, loss = 0.00484361
Iteration 115, loss = 0.00472130
Iteration 116, loss = 0.00460688
Iteration 117, loss = 0.00450496
Iteration 118, loss = 0.00441112
Iteration 119, loss = 0.00431566
Iteration 120, loss = 0.00421391
Iteration 121, loss = 0.00412025
Iteration 122, loss = 0.00407853
Iteration 123, loss = 0.00394640
Iteration 124, loss = 0.00390898
Iteration 125, loss = 0.00380694
Iteration 126, loss = 0.00373251
Iteration 127, loss = 0.00365828
Iteration 128, loss = 0.00357357
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted:  95
Error rate: 24.40% ( 610/2500)
 - Class 1:  35, Class 2:  17, Class 3: 500, Class 4:  27, Class 5:  31
Iteration 1, loss = 1.23220760
Iteration 2, loss = 0.68801923
Iteration 3, loss = 0.42939169
Iteration 4, loss = 0.30562079
Iteration 5, loss = 0.24119414
Iteration 6, loss = 0.20217268
Iteration 7, loss = 0.17642421
Iteration 8, loss = 0.15749706
Iteration 9, loss = 0.14377342
Iteration 10, loss = 0.13236318
Iteration 11, loss = 0.12333357
Iteration 12, loss = 0.11559470
Iteration 13, loss = 0.10980809
Iteration 14, loss = 0.10353901
Iteration 15, loss = 0.09758634
Iteration 16, loss = 0.09330977
Iteration 17, loss = 0.08906265
Iteration 18, loss = 0.08526092
Iteration 19, loss = 0.08169475
Iteration 20, loss = 0.07839698
Iteration 21, loss = 0.07522176
Iteration 22, loss = 0.07207463
Iteration 23, loss = 0.06930091
Iteration 24, loss = 0.06685010
Iteration 25, loss = 0.06439508
Iteration 26, loss = 0.06202604
Iteration 27, loss = 0.05978297
Iteration 28, loss = 0.05822074
Iteration 29, loss = 0.05650620
Iteration 30, loss = 0.05398227
Iteration 31, loss = 0.05221831
Iteration 32, loss = 0.05057286
Iteration 33, loss = 0.04895893
Iteration 34, loss = 0.04718625
Iteration 35, loss = 0.04560592
Iteration 36, loss = 0.04411313
Iteration 37, loss = 0.04254188
Iteration 38, loss = 0.04130769
Iteration 39, loss = 0.04003784
Iteration 40, loss = 0.03893589
Iteration 41, loss = 0.03757750
Iteration 42, loss = 0.03630727
Iteration 43, loss = 0.03559518
Iteration 44, loss = 0.03438521
Iteration 45, loss = 0.03310913
Iteration 46, loss = 0.03207027
Iteration 47, loss = 0.03125651
Iteration 48, loss = 0.03013914
Iteration 49, loss = 0.02924984
Iteration 50, loss = 0.02829228
Iteration 51, loss = 0.02736869
Iteration 52, loss = 0.02660331
Iteration 53, loss = 0.02584145
Iteration 54, loss = 0.02493422
Iteration 55, loss = 0.02435110
Iteration 56, loss = 0.02353003
Iteration 57, loss = 0.02283099
Iteration 58, loss = 0.02229740
Iteration 59, loss = 0.02135734
Iteration 60, loss = 0.02090263
Iteration 61, loss = 0.02015299
Iteration 62, loss = 0.01950329
Iteration 63, loss = 0.01875930
Iteration 64, loss = 0.01826182
Iteration 65, loss = 0.01758968
Iteration 66, loss = 0.01706475
Iteration 67, loss = 0.01652870
Iteration 68, loss = 0.01624335
Iteration 69, loss = 0.01553263
Iteration 70, loss = 0.01491263
Iteration 71, loss = 0.01451161
Iteration 72, loss = 0.01414270
Iteration 73, loss = 0.01370493
Iteration 74, loss = 0.01320602
Iteration 75, loss = 0.01300265
Iteration 76, loss = 0.01261717
Iteration 77, loss = 0.01208078
Iteration 78, loss = 0.01178768
Iteration 79, loss = 0.01158334
Iteration 80, loss = 0.01102555
Iteration 81, loss = 0.01070607
Iteration 82, loss = 0.01036353
Iteration 83, loss = 0.01008298
Iteration 84, loss = 0.00980702
Iteration 85, loss = 0.00954465
Iteration 86, loss = 0.00928831
Iteration 87, loss = 0.00899309
Iteration 88, loss = 0.00871477
Iteration 89, loss = 0.00845782
Iteration 90, loss = 0.00828240
Iteration 91, loss = 0.00814306
Iteration 92, loss = 0.00787299
Iteration 93, loss = 0.00757024
Iteration 94, loss = 0.00738527
Iteration 95, loss = 0.00726452
Iteration 96, loss = 0.00698684
Iteration 97, loss = 0.00683099
Iteration 98, loss = 0.00665079
Iteration 99, loss = 0.00643825
Iteration 100, loss = 0.00630592
Iteration 101, loss = 0.00610274
Iteration 102, loss = 0.00595626
Iteration 103, loss = 0.00581914
Iteration 104, loss = 0.00568715
Iteration 105, loss = 0.00553682
Iteration 106, loss = 0.00536932
Iteration 107, loss = 0.00525349
Iteration 108, loss = 0.00515972
Iteration 109, loss = 0.00505592
Iteration 110, loss = 0.00488120
Iteration 111, loss = 0.00476966
Iteration 112, loss = 0.00468985
Iteration 113, loss = 0.00455622
Iteration 114, loss = 0.00447516
Iteration 115, loss = 0.00437153
Iteration 116, loss = 0.00426223
Iteration 117, loss = 0.00415686
Iteration 118, loss = 0.00407413
Iteration 119, loss = 0.00398809
Iteration 120, loss = 0.00390779
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Number of features extracted: 100
Error rate: 24.48% ( 612/2500)
 - Class 1:  12, Class 2:  29, Class 3: 500, Class 4:  24, Class 5:  47